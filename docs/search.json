[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "",
    "text": "Preface\nThis text has been modified from material by Dr. Susan VanderPlas – see Statistical Computing using R and Python for her course book – with integration of content and videos from Dr. Allison Theobold and Dr. Kelly Bodwin.\nThis text is designed to demonstrate statistical programming concepts and techniques in R. It is intended as a substitute for hours and hours of video lectures - watching someone code and talk about code is not usually the best way to learn how to code. It’s far better to learn how to code by … coding.\nI hope that you will work through this text week by week over the quarter. I have included comics, snark, gifs, YouTube videos, extra resources, and more: my goal is to make this a collection of the best information I can find on statistical programming.\nIn most cases, this text includes way more information than you need. Everyone comes into this class with a different level of computing experience, so I’ve attempted to make this text comprehensive. Unfortunately, that means some people will be bored and some will be overwhelmed. Use this text in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-text",
    "href": "index.html#how-to-use-this-text",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "How to Use This Text",
    "text": "How to Use This Text\nI’ve made an effort to use some specific formatting and enable certain features that make this text a useful tool for this class.\n\nSpecial Sections\n\n\nCheck-in\nCheck-in sections contain quizzes or preview activities you must complete and submit to Canvas for credit.\n\n\n\nTry It Out\nTry it out sections contain the required weekly practice activities (and sometimes additional/optional activities) you should do to reinforce the things you’ve just read.\n\n\n\nWatch Out\nWatch out sections contain things you may want to look out for - common errors, etc.\n\n\n\nExamples\nExample sections contain code and other information. Don’t skip them!\n\n\n\nGo Read\nSometimes, there are better resources out there than something I could write myself. When you see this section, go read the enclosed link as if it were part of the book.\n\n\n\nLearn More\nLearn More sections contain other references that may be useful on a specific topic. Suggestions are welcome (email me to suggest a new reference that I should add), as there’s no way for one person to catalog all of the helpful programming resources on the internet!\n\n\n\nMy Opinion\nThese sections contain things you should definitely not consider as fact and should just take with a grain of salt.\n\n\n\nNote\nNote sections contain clarification points (anywhere I would normally say “note that ….)\n\n\n\nExpandable Sections\n\n\nThese are expandable sections, with additional information when you click on the line\n\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\nMany times, examples will be in expandable sections\n\nThis keeps the code and output from obscuring the actual information in the textbook that I want you to retain. You can always look up the syntax, but you do need to absorb the details I’ve written out.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "Additional Resources",
    "text": "Additional Resources\nReferences or additional readings may come from the following texts:\n\nR for Data Science (2e)\nAdvanced R\nModern Dive\nStat 545 (Data wrangling, exploration, and analysis with R) by Jenny Bryan\n\nYou can find additional help for Coding in R from the following resources:\n\nRStudio Education Page\nPosit Primers\n“R Bootcamp” Practice\n\nPart of this course will be dedicated to conducting familiar statistical analyses using R code. You can find a refresher on statistical concepts from the following resources:\n\nIntroduction to Modern Statistics\nModern Dive",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-prereading.html",
    "href": "00-prereading.html",
    "title": "Pre-reading",
    "section": "",
    "text": "Objectives\nReading: 25 minute(s) at 200 WPM.\nVideos: 28 minutes\nA prerequisite for this course is an introductory programming course. Therefore, there is some assumed knowledge. Maybe you have not seen these concepts in R, but you have already developed a base for logically thinking through a computing problem in another language.\nThis chapter is meant to provide a resource for the basics of programming (in R) and gives me a place to refer back to (as need be) in future chapters.\nIn this chapter you will:",
    "crumbs": [
      "Pre-reading"
    ]
  },
  {
    "objectID": "00-prereading.html#ch0-objectives",
    "href": "00-prereading.html#ch0-objectives",
    "title": "Pre-reading",
    "section": "",
    "text": "Learn the basics of a computer system.\nRefresh your mathematical logic and apply it to variables, vectors, and matrices.\nKnow the different types of variables and how to assign them to objects in R.\nUnderstand how to create and index vectors and matrices in R.\nExtend your logic to control structures with if-then statements and loops.",
    "crumbs": [
      "Pre-reading"
    ]
  },
  {
    "objectID": "00-prereading.html#computer-basics",
    "href": "00-prereading.html#computer-basics",
    "title": "Pre-reading",
    "section": "Computer Basics",
    "text": "Computer Basics\nIt is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\n\nHardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\nOperating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\nFile Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\nStop watching at 4:16.\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\n\nRecommend watching - helpful for understanding file paths!\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home.\nIn Chapter 1, we will discuss Directories, Paths, and Projects as they relate to R and getting setup to be successful in this course.",
    "crumbs": [
      "Pre-reading"
    ]
  },
  {
    "objectID": "00-prereading.html#vectors-matrices-and-arrays",
    "href": "00-prereading.html#vectors-matrices-and-arrays",
    "title": "Pre-reading",
    "section": "Vectors, Matrices, and Arrays",
    "text": "Vectors, Matrices, and Arrays\n\n\nThis section introduces some of the most important tools for working with data: vectors, matrices, loops, and if statements. It would be nice to gradually introduce each one of these topics separately, but they tend to go together, especially when you’re talking about programming in the context of data processing.\n\nMathematical Logic\nBefore we start talking about data structures and control structures, though, we’re going to take a minute to review some concepts from mathematical logic. This will be useful for both data structures and control structures, so stick with me for a few minutes.\n\nAnd, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT.\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\nx &lt;- c(TRUE, FALSE, TRUE, FALSE)\ny &lt;- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n\n[1]  TRUE FALSE FALSE FALSE\n\nx | y # OR\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n!x & y # NOT X AND Y\n\n[1] FALSE  TRUE FALSE FALSE\n\nx & !y # X AND NOT Y\n\n[1] FALSE FALSE  TRUE FALSE\n\n\n\n\nDe Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\nDefinitionsDeMorgan’s First LawDeMorgan’s Second Law\n\n\n Suppose that we set the convention that .\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)\n\n\n\n\n\n\n\n\nBasic Data Types\nWhile we will discuss data types more in depth during class, it is important to have a base grasp on the types of data you might see in a programming language.\n\nValues and Types\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn R, there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\ndouble or float or numeric- decimal numbers.\n\nfloat is short for floating-point value.\ndouble is a floating-point value with more precision (“double precision”).1\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\ncharacter or string - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, R has a function to help you with that.\n\nclass(FALSE)\nclass(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nclass(2)\nclass(\"Hello, programmer!\")\n\n[1] \"logical\"\n[1] \"integer\"\n[1] \"numeric\"\n[1] \"character\"\n\n\n\nIn R, boolean values are TRUE and FALSE. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000. Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.\n\n\n\nVariables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nIn R, we assign variables values using the syntax object_name &lt;- value You can read this as “object name gets value” in your head.\n\nmessage &lt;- \"So long and thanks for all the fish\"\nyear &lt;- 2025\nthe_answer &lt;- 42L\nearth_demolished &lt;- FALSE\n\n\nNote that in R, we assign variables values using the &lt;- operator. Technically, = will work for assignment, but &lt;- is more common than = in R by convention.\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\n\nValid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R.\nWhat happens if we try to create a variable name that isn’t valid?\nStarting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol”.\n\n1st_thing &lt;- \"check your variable names!\"\n\nError in parse(text = input): &lt;text&gt;:1:2: unexpected symbol\n1: 1st_thing\n     ^\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\nThere are a few different conventions for naming things that may be useful:\n\nsome_people_use_snake_case, where words are separated by underscores\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nsome.people.use.periods\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated.\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.\n\n\nType Conversions\nWe talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. R is dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\nmode(2L + 3.14159) # add integer 2 and pi\n\n[1] \"numeric\"\n\nmode(2L + TRUE) # add integer 2 and TRUE\n\n[1] \"numeric\"\n\nmode(TRUE + FALSE) # add TRUE and FALSE\n\n[1] \"numeric\"\n\n\nAll of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how R handles type conversions.\n\nTest it out!\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “*” or “.” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell R that we want our string to be treated as a number, or vice-versa.\nIn R, we can explicitly convert a variable’s type using as.XXX() functions, where XXX is the type you want to convert to (as.numeric, as.integer, as.logical, as.character, etc.).\n\nx &lt;- 3\ny &lt;- \"3.14159\"\n\nx + y\n\nError in x + y: non-numeric argument to binary operator\n\nx + as.numeric(y)\n\n[1] 6.14159\n\n\n\n\nOperators and Functions\nIn addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\n\nOperation\nR symbol\n\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nInteger Division\n%/%\n\n\nModular Division\n%%\n\n\nExponentiation\n^\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nSo 14 %/% 3 would be 4, and 14 %% 3 would be 2.\n\n14 %/% 3\n\n[1] 4\n\n14 %% 3\n\n[1] 2\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated.\n\n\nOrder of Operations\nR operates under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n(1+1)^(5-2) # 2 ^ 3 = 8\n\n[1] 8\n\n1 + 2^3 * 4 # 1 + (8 * 4)\n\n[1] 33\n\n3*1^3 # 3 * 1\n\n[1] 3\n\n\n\n\nString Operations\nYou will have to use functions to perform operations on strings, as R does not have string operators. In R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n\n[1] \"first second\"\n\npaste(\"first\", \"second\", collapse = \" \")\n\n[1] \"first second\"\n\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works on separate parameters\n\n[1] \"first\"  \"second\"\n\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n\n[1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n\n[1] \"a-first b-second c-third d-fourth\"\n\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n\n[1] \"a\" \"b\" \"c\"\n\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n\n[1] \"abc\"\n\n\nYou don’t need to understand the details of this at this point in the class, but it is useful to know how to combine strings.\n\n\nFunctions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. Instead, I’m going to show you how to use functions.\nIt may be helpful at this point to print out the R reference card3. This cheat sheet contains useful functions for a variety of tasks.\nMethods are a special type of function that operate on a specific variable type. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\n\n\n\nData Structures\nIn the previous section, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\n\nLists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nx &lt;- list(\"a\", 3, FALSE)\nx\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] FALSE\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\nIndexing\nEvery element in a list has an index (a location, indicated by an integer position)4.\nIn R, we count from 1.\n\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\nx &lt;- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n\n[[1]]\n[1] \"a\"\n\nx[1:2] # This returns multiple elements in the list\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\nx[[1]] # This returns the item\n\n[1] \"a\"\n\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n\nError in x[[1:2]]: subscript out of bounds\n\n\nList indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\n\n\n\nVectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \n\n\nIndexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is. I’m going to demonstrate indices with the string vector\n\n\n\n\n\n\n\nR\n\n\n\n\n\n1-indexed language\n\n\n\nCount elements as 1, 2, 3, 4, …, N\n\n\n\n\n\n\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi &lt;- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n\n[1] 3\n\ndigits_pi[2]\n\n[1] 1\n\ndigits_pi[3]\n\n[1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n\nnumeric(0)\n\ndigits_pi[11]\n\n[1] 5\n\n# Print out the vector\ndigits_pi\n\n [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\nfavorite_cats &lt;- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"    \n\nfavorite_cats[2] &lt;- \"Nyan Cat\"\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"    \n\n\nIf you’re curious about any of these cats, see the footnotes5.\n\n\nIndexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n# Define a character vector\nweekdays &lt;- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend &lt;- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days &lt;- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days &lt;- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n\n[1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days &lt;- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n\n\n\n\nReviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n\n[1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n\n[1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n\n[1] 2 0\n\n\nAs a reminder, this is an example of implicit type conversion - R decides what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\n\n\n\nMatrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\nMatrix (Lego)\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nMost of the problems we’re going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\n\n\n\nIndexing in Matrices\nR uses [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\nmy_mat &lt;- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] &lt;- 500\n\nmy_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]  500   10   11   12\n\n\n\n\nMatrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\ntranspose - flip the matrix across the left top -&gt; right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\nmatrix multiplication (dot product) - you will learn more about this in linear algebra, but here’s a preview. Here is a better explanation of the cross product \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\nx &lt;- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny &lt;- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n3 * x\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n# Transpose\nt(x)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nt(y)\n\n     [,1] [,2]\n[1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n\n     [,1]\n[1,]   17\n[2,]   39\n\n\n\n\n\nArrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\narray(1:8, dim = c(2,2,2))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement.",
    "crumbs": [
      "Pre-reading"
    ]
  },
  {
    "objectID": "00-prereading.html#control-structures",
    "href": "00-prereading.html#control-structures",
    "title": "Pre-reading",
    "section": "Control Structures",
    "text": "Control Structures\nThe focus of this course is more on working with data - however in prior programming courses you have likely developed the logical thinking to work with Control structures. Control structures are statements in a program that determine when code is evaluated (and how many times it might be evaluated). There are two main types of control structures: if-statements and loops.\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\n\n\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\nHere’s another example:\n\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’\n\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time.\n\nConditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\n\n\nLet’s try this out:\n\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) { \n  y &lt;- 8\n} else {\n  y &lt;- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nThe logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x &gt; 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) y &lt;- 8 else y &lt;- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\nRepresenting Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n\n\nUS Tax brackets\n\n\nProblemSolutionProgram Flow Chart\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\n\nrate\nIncome\n\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n# Start with total income\nincome &lt;- 200000\n\n# x will hold income that hasn't been taxed yet\nx &lt;- income\n# y will hold taxes paid\ny &lt;- 0\n\nif (x &lt;= 10275) {\n  y &lt;- x*.1 # tax paid\n  x &lt;- 0 # All money has been taxed\n} else {\n  y &lt;- y + 10275 * .1\n  x &lt;- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x &lt;= (41775 - 10275)) {\n  y &lt;- y + x * .12\n  x &lt;- 0\n} else {\n  y &lt;- y + (41775 - 10275) * .12\n  x &lt;- x - (41775 - 10275) \n}\n\nif (x &lt;= (89075 - 41775)) {\n  y &lt;- y + x * .22\n  x &lt;- 0\n} else {\n  y &lt;- y + (89075 - 41775) * .22\n  x &lt;- x - (89075 - 41775)\n}\n\nif (x &lt;= (170050 - 89075)) {\n  y &lt;- y + x * .24\n  x &lt;- 0\n} else {\n  y &lt;- y + (170050 - 89075) * .24\n  x &lt;- x - (170050 - 89075)\n}\n\nif (x &lt;= (215950 - 170050)) {\n  y &lt;- y + x * .32\n  x &lt;- 0\n} else {\n  y &lt;- y + (215950 - 170050) * .32\n  x &lt;- x - (215950 - 170050)\n}\n\nif (x &lt;= (539900 - 215950)) {\n  y &lt;- y + x * .35\n  x &lt;- 0\n} else {\n  y &lt;- y + (539900 - 215950) * .35\n  x &lt;- x - (539900 - 215950)\n}\n\nif (x &gt; 0) {\n  y &lt;- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n\n[1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\nLet’s explore using program flow maps for a slightly more complicated problem: The tax bracket example that we used to demonstrate if statement syntax.\n\n\n\nThe control flow diagram for the code in the previous example\n\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\n\n\n\nChaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\n\nAge brackets\n\nFor instance, suppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\nProgram Flow Map\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as &lt;18, 18-25, 26-40, 41-55, 56-65, and &gt;65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\n\nThe important thing to realize when examining this program flow map is that if age &lt;= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age &lt;= 18), (age &lt;= 25), and (age &lt;= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - &lt;40 is evaluated first, and so &lt;= 25 and &lt;= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\n\nIn code, we would write this statement using else-if (or elif) statements.\n\nage &lt;- 40 # change this as you will to see how the code works\n\nif (age &lt; 18) {\n  bracket &lt;- \"&lt;18\"\n} else if (age &lt;= 25) {\n  bracket &lt;- \"18-25\"\n} else if (age &lt;= 40) {\n  bracket &lt;- \"26-40\"\n} else if (age &lt;= 55) {\n  bracket &lt;- \"41-55\" \n} else if (age &lt;= 65) {\n  bracket &lt;- \"56-65\"\n} else {\n  bracket &lt;- \"&gt;65\"\n}\n\nbracket\n\n[1] \"26-40\"\n\n\n\n\n\n\n\n\n\nLoops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\nWhile Loops\nWe just discussed conditional statements, where a block of code is only executed if a logical statement is true.\nThe simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\n\nFlow map showing while-loop pseudocode (while x &lt;= N) { # code that changes x in some way} and the program flow map expansion where we check if x &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\nx &lt;- 0\n\nwhile (x &lt; 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x &lt;- x + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n\n\n\nWhile loops\n\n\nProblemMath NotationSolution\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nYou will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk &lt;- 1\nprod &lt;- 1\nans &lt;- (exp(pi) - exp(-pi))/(2*pi)\ndelta &lt;- 0.0001\n\nwhile (abs(prod - ans) &gt;= 0.0001) {\n  prod &lt;- prod * (1 + 1/k^2)\n  k &lt;- k + 1\n}\n\nk\n\n[1] 36761\n\nprod\n\n[1] 3.675978\n\nans\n\n[1] 3.676078\n\n\n\n\n\n\n\n\nFor Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\nFlow MapR\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nFor loops are often run from 1 to N but in essence, a for loop is run for every value of a vector (which is why loops are included in the same chapter as vectors).\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n\n[1] \"January\"\n[1] \"February\"\n[1] \"March\"\n[1] \"April\"\n[1] \"May\"\n[1] \"June\"\n[1] \"July\"\n[1] \"August\"\n[1] \"September\"\n[1] \"October\"\n[1] \"November\"\n[1] \"December\"\n\n\n\n\n\nAvoiding Infinite Loops\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\nThis while loop runs until either x &lt; 10 or n &gt; 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n&gt;50 check to the loop so that we don’t tie up the computer for eternity.\n\nx &lt;- 0\nn &lt;- 0 # count the number of times the loop runs\n\nwhile (x &lt; 10) { \n  print(x)\n  x &lt;- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n &lt;- n + 1\n  if (n &gt; 50) \n    break # this stops the loop if n &gt; 50\n}\n\n[1] 0\n[1] 0.4729593\n[1] 0.9858417\n[1] 0.1372321\n[1] 1.323804\n[1] 2.677874\n[1] 1.916362\n[1] 1.878708\n[1] 1.223298\n[1] 0.1178043\n[1] 0.3587898\n[1] 0.9780306\n[1] 1.110049\n[1] -0.2334552\n[1] 0.2966643\n[1] 1.393706\n[1] 2.804279\n[1] 4.249758\n[1] 3.706629\n[1] 3.969112\n[1] 3.162744\n[1] 4.398826\n[1] 4.230561\n[1] 3.885983\n[1] 5.007536\n[1] 4.916564\n[1] 5.068941\n[1] 4.406242\n[1] 4.795512\n[1] 5.025923\n[1] 4.186623\n[1] 5.322228\n[1] 6.578634\n[1] 6.510968\n[1] 5.428602\n[1] 3.97464\n[1] 3.704266\n[1] 3.180291\n[1] 2.775947\n[1] 3.158462\n[1] 4.895029\n[1] 3.761331\n[1] 2.419419\n[1] 2.681526\n[1] 2.030446\n[1] 2.06169\n[1] 0.08610835\n[1] 1.172137\n[1] 2.084754\n[1] 1.408874\n[1] 2.110559\n\n\nIn the example above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\n\n\nControlling Loops\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\nBreak StatementNext/Continue Statement\n\n\n\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n\n\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n\n[1] 1\n[1] 2\n[1] \"Divisible by 3\"\n[1] 4\n[1] \"Divisible by 5\"\n[1] \"Divisible by 3\"\n[1] 7\n[1] 8\n[1] \"Divisible by 3\"\n[1] \"Divisible by 5\"\n[1] 11\n[1] \"Divisible by 3\"\n[1] 13\n[1] 14\n[1] \"Exiting now\"\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use.",
    "crumbs": [
      "Pre-reading"
    ]
  },
  {
    "objectID": "00-prereading.html#footnotes",
    "href": "00-prereading.html#footnotes",
    "title": "Pre-reading",
    "section": "",
    "text": "This means that doubles take up more memory but can store more decimal places. You don’t need to worry about this much in R.↩︎\nIn some ways, this is like the difference between an automatic and a manual transmission - you have fewer things to worry about, but you also don’t know what’s going on under the hood nearly as well↩︎\nFrom https://cran.r-project.org/doc/contrib/Short-refcard.pdf↩︎\nThroughout this section (and other sections), lego pictures are rendered using https://www.mecabricks.com/en/workshop. It’s a pretty nice tool for building stuff online!↩︎\nGrumpy cat, Garfield, Nyan cat. Jorts and Jean: The initial post and the update (both are worth a read because the story is hilarious). The cats also have a Twitter account where they promote workers rights.↩︎",
    "crumbs": [
      "Pre-reading"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Objectives\nReading: 26 minute(s) at 200 WPM.\nVideos: 47 minutes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#ch1-objectives",
    "href": "01-introduction.html#ch1-objectives",
    "title": "1  Introduction",
    "section": "",
    "text": "Set up necessary software for this class on personal machines.\nDetect and resolve problems related to file systems, working directories, and system paths when troubleshooting software installation.\nCreate Quarto documents with good reproducible principles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#getting-started",
    "href": "01-introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.1 Getting Started",
    "text": "1.1 Getting Started\n\nSetting up your computer\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class discord or contact me for help.\nHere’s a very broad overview of what each of these programs or services does and why I’m asking you to install them.\n\n\n\n\n\n\n\n\nProgram\nLogo\nPurpose\n\n\n\n\nR\n\nA statistical programming language built around working with data\n\n\nRStudio IDE\n\nAn integrated desktop environment created to make it easy to work with R and other data-science programming tools.\n\n\nQuarto\n\nA document creation system based on pandoc. Quarto allows you to include code, results, and pictures generated from data within a document so that they automatically update when the document is recompiled.\n\n\n\nIf you already have R downloaded, please follow these steps anyways, to make sure you have the most recent version of R. Do not ignore these instructions. If you neglect to update your version of R, you may find that updating a package will make it so your code will not run.\n\n\n\nInstalling R, RStudio, and Quarto\n\nDownload and run the R installer for your operating system from CRAN:\n\nWindows: https://cran.rstudio.com/bin/windows/base/\nMac: https://cran.rstudio.com/bin/macosx/ (double check your macOS version)\nLinux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\n(Required) If you are on Windows, you should also install the Rtools4 package; this will ensure you get fewer warnings later when installing packages. Make sure you double check the version compatability.\nMore detailed instructions for Windows are available here\nDownload and install the latest version of RStudio for your operating system (see Step 2 Installers). RStudio is a integrated development environment (IDE) for R - it contains a set of tools designed to make writing R code easier.\nEnsure Quarto is downloaded and installed. Quarto is a command-line tool released by RStudio that allows Rstudio to work with R specific tools in a unified way. We will talk more about Quarto in a later section, but for now just know this is the “notebook” you will be completing and writing all your assignments in.\nThe newest versions of RStudio have started automatically installing Quarto when RStudio is downloaded\nTo check – Open a new Rstudio window &gt; click + in the top left under File &gt; Quarto Document\n\n\n\n\n\n\n\n\n\n\nIf you do not see Quarto, download and install the latest version of Quarto for your operating system.\n Make sure you remember to create your RProject and upload a screenshot to canvas.\n\n\n\nIf you would like a video tutorial on downloading RStudio, here is one:\n\n\n\n\n\n\n\nRStudio organization has recently re-branded to posit. Rstudio is still the name of the IDE, but these two names may be used interchangeably. See a video of Hadley Wickham talking about the re-branding.\n\n\n\n\n\n\nIntroduction to R and RStudio\nIn this section, we will learn more about some of the tools you just installed. You may have worked with R and RStudio in previous classes, but never had a course dedicated to learning about their functionality.\n\nIntroduction to R\nR is a statistical programming language. Unlike more general-purpose languages, R is optimized for working with data and doing statistics. R was created by Ross Ihaka and Robert Gentleman in 1993 (hence “R”) and was formally released by the R Core Group in 1997 (a group of 20ish volunteers who are the only people who can change the base - built in- functionality of R). If you want to build an independent, standalone graphical interface, or run a web server, R is probably not the ideal language to use (you might want C/python or PHP/python, respectively). If you want to vacuum up a bunch of data, fit several regression models, and then compare the models, R is a great option and will be faster than working in a more general-purpose language like C or base python.\n\n\n\nR is\n\nvector-based\n1 indexed (start counting 1, 2, 3, …)\na scripting language (R code does not have to be compiled before it is run)\n\nOne thing to know about R is that it is open-source. This means that no company owns R (like there is for SAS or Matlab) and that developers cannot charge for the use of their R software. This does not mean that all of your code needs to be public (you can keep your code private), but it is important to be a good open-source citizen by sharing your code publicly when possible (later we will learn about GitHub), contributing to public projects and packages, creating your own packages, and using R for ethical and respectful projects.\n\nNote that RStudio is NOT R, but a platform to help you use R through and that it is a way to make money around the culture of R.\n\n\n\nThe History of R\nThe History of R\n\n\n\nRStudio: the IDE\nAn IDE is an integrated development environment - a fancy, souped up text editor that is built to make programming easier. Back in the dark ages, people wrote programs in text editors and then used the command line to compile those programs and run them.\n\n\n\n\n\n\nRStudio provides a cheat-sheet for the IDE if you are so inclined.\n\nRStudio is not R - it’s just a layer on top of R. So if you have a question about the user interface, you have an RStudio question. If you have a question about the code, you have an R question.\n\n\n\nNavigating RStudio\n\n\n\n\n\nThe RStudio window will look something like this.\n\n\n\n\n\nTop-leftTop-rightBottom-leftBottom-right\n\n\nIncludes the text editor. This is where you’ll do most of your work.\n\n\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When an R file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\nIn the top right, you’ll find the environment, history, and connections tabs. The environment tab shows you the objects available in R (variables, data files, etc.), the history tab shows you what code you’ve run recently, and the connections tab is useful for setting up database connections.\n\n\nOn the bottom left is the console. There are also other tabs to give you a terminal (command line) prompt, and a jobs tab to monitor progress of long-running jobs. In this class we’ll primarily use the console tab.\n\nknitr::include_graphics(\"https://srvanderplas.github.io/stat-computing-r-python/images/tools/Rstudio-terminal-R.png\")\n\n\n\n\n\n\n\n\n\n\nOn the bottom right, there are a set of tabs:\n\nfiles (to give you an idea of where you are working, and what files are present),\n\n\n\n\n\n\n\n\n\n\n\nplots (which will be self-explanatory),\npackages (which extensions to R are installed and loaded),\n\n\nknitr::include_graphics(\"https://srvanderplas.github.io/stat-computing-r-python/images/tools/Rstudio-packages-tab.png\")\n\n\n\n\n\n\n\n\n\nthe help tab (where documentation will show up), and\n\n\n\n\n\n\n\n\n\n\n\nthe viewer window, which is used for interactive graphics or previewing HTML documents.\n\n\n\n\n\n\nInstalling Packages\nOne of R’s strengths is the package repository, CRAN (Comprehensive R Archive Network), that allows anyone (yes, even you!) to write an R package. Packages contain “extra” functionality (outside the base functionality of R). This means that R generally has the latest statistical methods available, and one of the best ways to ensure someone uses your work is to write an R package to make that work accessible to the general population of statisticians/biologists/geneticists.\nTo install the tibble package in R, we would use the following code:\n\ninstall.packages(\"tibble\")\n\nThen, to use the functions within that package, we need to load the package:\n\nlibrary(\"tibble\")\n\nWhen you load a package, all of the functions in that package are added to your R Namespace (this is a technical term) - basically the list of all of the things R knows about. This may be problematic if you have two packages with the same function name.\n\nYou only need to install a package once and your computer will be able to find the package on your computer when it needs to. However, you need to load the package every time R is restarted or you switch to a new project.\n\nIf you want to use a function from a package without loading the package into your namespace, you can do that by using pkgname::function syntax.\nFor instance, this code creates a sample data frame using the tribble() function in the tibble package.\n\ntibble::tribble(~col1, ~col2, 1, 'a', 2, 'b', 3, 'c')\n\n# A tibble: 3 × 2\n   col1 col2 \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 c    \n\n\n\n\nTest your setup\nWe will find our way in R and learn more about Quarto in the following sections, but for now open RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console:\n\n\ninstall.packages(\n\n  c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\")\n\n)\n\nCan you find the text editor?\n\nCreate a new quarto document (File &gt; New File &gt; Quarto Document).\nCompile the document using the Render button and use the Viewer pane to see the result.\nIf this all worked, you have RStudio, Quarto, and R set up correctly on your machine.\n\n\n\n\n\nAdditional Resources: Basics of R and RStudio\nBasics of R Programming\n Basics of R\n RStudio Primer, Basics of Programming in R\nIntroduction to RStudio\n A tour of RStudio, BasicsBasics1\n Quick tour of RStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#directories-and-projects",
    "href": "01-introduction.html#directories-and-projects",
    "title": "1  Introduction",
    "section": "1.2 Directories, Paths, and Projects",
    "text": "1.2 Directories, Paths, and Projects\nIn the pre-reading section, File Systems, we learned how to organize our personal files and locate them using absolute and relative file paths. The idea of a “base folder” or starting place was introduced as a working directory. In R, there are two ways to set up your file path and file system organization:\n\n\n\n\nSet your working directory in R (do not reccommend)\nUse RProjects (preferred!)\n\n\nWorking Directories in R\nTo find where your working directory is in R, you can either look at the top of your console or type getwd() into your console.\n\n\n\n\n\n\n\n\n\n\ngetwd()\n\n[1] \"/Users/czmann/Documents/teaching/stat331/stat331-calpoly-text\"\n\n\nAlthough it is not recommended, you can set your working directory in R with setwd().\n\nsetwd(\"/path/to/my/assignment/folder\")\n\n\n\nRprojects\nSince there are often many files necessary for a project (e.g. data sources, images, etc.), R has a nice built in system for setting up your project organization with RProjects. You can either create a new folder on your computer containing an Rproject (e.g., you have not yet created a folder for this class) or you can add an Rproject to an existing folder on your computer (e.g., you have already created a folder for this class).\n\nCreate with a new folderAdd to an existing folder\n\n\nTo create a Rproject, first open RStudio on your computer and click File &gt; New Project, then:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGive your folder a name (it doesn’t have to be my-stat331). However, it is good practice for this file folder name to not contain spaces.\nThen, browse on your computer for a location to save this folder to. For example, mine is saved in my Documents. Make sure you know how to find this; it should NOT be saved in your Downloads!\n\n\n\n\n\n\n\n\n\nThis new folder, my-stat331 should now live in your Documents folder (or wherever you save it to) and contain a my-stat331.Rproj file. This is your new “home” base for this class - whenever you refer to a file with a relative path it will begin to look for it here.\n\n\nTo add a Rproject to an existing folder on your computer (e.g., you already created a folder for this class), first open RStudio on your computer and click File &gt; New Project, then:\n\n\n\n\n\n\n\n\n\nThen, browse on your computer to select the existing folder you wish to add your Rproject to. For example, mine is saved in my Documents and called my-stat331.\n\n\n\n\n\n\n\n\n\nYour existing folder, my-stat331 should now contain a my-stat331.Rproj file. This is your new “home” base for this class - whenever you refer to a file with a relative path it will begin to look for it here.\n\n\n\n\n\n\n\n\n\n\nIf you plan to use the Studio computers during class, I recommend having a way to access and save your material between the Studio computers and your personal computer (e.g. put your class directory and Rproject on OneDrive). The idea with Rprojects is then your relative file paths will work in either computer without any changes!\n\n\n\n\nFile Paths in R\nA quick warning on file paths is that Mac/Linux and Windows differ in the direction of their backslash to separate folder locations. Mac/Linux use / (e.g. practice-activities/PA1.pdf) while Window’s uses \\ (e.g. practice-activities\\PA1.pdf).\nR can work with both, however, a backslash \\ means something different to R so if you copy a file path from your file filder in Windows, you will need to replace all backslashes with a double backslash \\\\ (e.g. practice-activities\\\\PA1.pdf)\n\n\n\nWorkflow\n Workflow and Projects\n Software Carpentry – Project Managment with RStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#scripts-and-notebooks",
    "href": "01-introduction.html#scripts-and-notebooks",
    "title": "1  Introduction",
    "section": "1.3 Scripts and Notebooks",
    "text": "1.3 Scripts and Notebooks\nIn this class, we’ll be using markdown notebooks to keep our code and notes in the same place. One of the advantages of both R is that it is a scripting language, but it can be used within notebooks as well. This means that you can have an R script file, and you can run that file, but you can also create a document (like the one you’re reading now) that has code AND text together in one place. This is called literate programming and it is a very useful workflow both when you are learning programming and when you are working as an analyst and presenting results.\n\nScripts\nBefore I show you how to use literate programming, let’s look at what it replaces: scripts. Scripts are files of code that are meant to be run on their own. They may produce results, or format data and save it somewhere, or scrape data from the web – scripts can do just about anything.\nScripts can even have documentation within the file, using # characters (at least, in R) at the beginning of a line. # indicates a comment – that is, that the line does not contain code and should be ignored by the computer when the program is run. Comments are incredibly useful to help humans to understand what the code does and why it does it.\n\n\nPlotting a logarithmic spiral\n\nThis code will use concepts we have not yet introduced - feel free to tinker with it if you want, but know that you’re not responsible for being able to write this code yet. You just need to read it and get a sense for what it does. I have heavily commented it to help with this process.\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta &lt;- seq(0, 4*pi, .01)\n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta\nr &lt;- seq(0, 5, length.out = length(theta))\n\n# Now define x and y in cartesian coordinates\nx &lt;- r * cos(theta)\ny &lt;- r * sin(theta)\n\nplot(x, y, type = \"l\")\n\n\n\n\n\n\n\nFigure 1.1: A Cartesian Spiral in R\n\n\n\n\n\nTo create your first script, click File &gt; New File &gt; R Script and copy paste the code from above. You can save this script on your computer just as you would any other file such as a word document, pdf, or image.\n\nScripts can be run in Rstudio by clicking the Run button  at the top of the editor window when the script is open.\n\n\n\n\n\n\nMost of the time, you will run scripts interactively - that is, you’ll be sitting there watching the script run and seeing what the results are as you are modifying the script. However, one advantage to scripts over notebooks is that it is easy to write a script and schedule it to run without supervision to complete tasks which may be repetitive.\n\n\nNotebooks\nNotebooks are an implementation of literate programming. R has native notebooks that allow you to code in R. This book is written using Quarto markdown, which is an extension of Rmarkdown.\nIn this class, we’re going to use Quarto/R markdown. This matters because the goal is that you learn something useful for your own coding and then you can easily apply it when you go to work as an analyst somewhere to produce impressive documents.\nTo create a quarto document click File &gt; New File &gt; Quarto Document. This will open a notebook template using quarto. You can then Render the document to a pdf or html file.\n\nIntroduction to Quarto\n(Required) Read through the following resources to introduce Quarto:\n\nR4DS: Quarto\nIntro to Quarto\n\nWhile in this class we will be using Quarto, before Quarto there was RMarkdown (and it is still widely used). If you wish, you can read about Rmarkdown here.\n\nDownload and save the Markdown syntax Cheat Sheet.\n\nLearn more about Notebooks and Quarto\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\nYihui Xie is the person responsible for knitr and Rmarkdown.\n\nYou can learn more about the functionality of Quarto at the following links:\n\nComputations in Quarto\nAuthoring & Formatting Quarto Documents\n\nYou can find the entire list of options you can use to format your HTML file with Quarto here. Poke around the gallery of cool HTML documents rendered with Quarto:\n\nInteractive document published to the web\nAdvanced Layout with HTML\nPimp my RMarkdown\nQuarto Tip A Day",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#getting-help",
    "href": "01-introduction.html#getting-help",
    "title": "1  Introduction",
    "section": "1.4 Getting help",
    "text": "1.4 Getting help\nIn R, you can access help with a ?. Suppose we want to get help on a for loop. In the R console, we can run this line of code to get help on for loops.\n\n?`for`\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name.\n(You will have to run this in interactive mode for it to work)\nw3schools has an excellent R help on basic functions that may be useful as well - usually, these pages will have examples.\n\nGoogle is your friend.\nThe R community has an enormous aresenal of online learning resources. I will linked a few throughout the “read more” sections in this text, but you can always find more!\n\nLearn to:\n\nGoogle for tutorials and examples\nUse Stack Overflow\nAsk questions on Twitter\nMake good use of the vast and welcoming R network on the internet",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html",
    "href": "02-tidy-data-and-basics-of-graphics.html",
    "title": "2  Tidy Data & Basics of Graphics",
    "section": "",
    "text": "Objectives\nReading: 31 minute(s) at 200 WPM.\nVideos: 14 minutes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy Data & Basics of Graphics</span>"
    ]
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#ch2-objectives",
    "href": "02-tidy-data-and-basics-of-graphics.html#ch2-objectives",
    "title": "2  Tidy Data & Basics of Graphics",
    "section": "",
    "text": "Recognize tidy data formats and identify the variables and columns in data sets\nRead in data from common formats into R\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data\nEvaluate existing charts and develop new versions that improve accessibility and readability",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy Data & Basics of Graphics</span>"
    ]
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#tidy-data",
    "href": "02-tidy-data-and-basics-of-graphics.html#tidy-data",
    "title": "2  Tidy Data & Basics of Graphics",
    "section": "2.1 Tidy Data",
    "text": "2.1 Tidy Data\nIn the pre-reading, we learned about Basic Data Types (strings/characters, numeric/double/floats, integers, and logical/booleans) and Data Structures (1D - vectors and lists; 2D - matrices and data frames). This class will mainly focus on working with data frames. It is important to know the type and format of the data you’re working with. It is much easier to create data visualizations or conduct statistical analyses if the data you use is in the right shape.\nData can be formatted in what are often referred to as wide format or long format. Wide format data has variables spread across columns and typically uses less space to display. This format is how you would typically choose to present your data as there is far less repetition of labels and row elements. Long format data is going to have each variable in a column and each observation in a row; this is likely not the most compact form of the data.\nLong formatted data is often what we call tidy data - a specific format of data in which each variable is a column, each observation is a row and each type of observational unit forms a table (or in R, a data frame).\n\n\n\n\n\nWhat is tidy data? Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\n\n\nSame data, different layouts\nCan you determine which of the following data sets follows the tidy data format?\n\nOption 1:Option 2:Option 3:Solution\n\n\n\n\n\nName\nTreatment A\nTreatment B\n\n\n\n\nBrian Boatwright\nNA\n18\n\n\nTrenna Porras\n4\n1\n\n\nHannaa Kumar\n6\n7\n\n\n\n\n\n\n\n\nTreatment\nBrian Boatwright\nTrenna Porras\nHannaa Kumar\n\n\n\n\nA\nNA\n4\n6\n\n\nB\n18\n1\n7\n\n\n\n\n\n\n\n\nName\nTreatment\nMeasurement\n\n\n\n\nBrian Boatwright\nA\nNA\n\n\nTrenna Porras\nA\n4\n\n\nHannaa Kumar\nA\n6\n\n\nBrian Boatwright\nB\n18\n\n\nTrenna Porras\nB\n1\n\n\nHannaa Kumar\nB\n7\n\n\n\n\n\nOption 3 follows the tidy data format since each variable (Name, Treatment, and Measurement) belong to their own columns and each observation taken is identified by a single row.\n\n\n\n\nData frames are a specific object type in R, data.frame(), and can be indexed the same as matrices. It may be useful to actually look at your data before beginning to work with it to see the format of the data. The following functions in R help us learn information about our data sets:\n\nclass(): outputs the object type\nnames(): outputs the variable (column) names\nhead(): outputs the first 6 rows of a dataframe\nglimpse() or str(): output a transpose of dataframe or matrix; shows the data types\nsummary() outputs 6-number summaries or frequencies for all variables in the data set depending on the variables data type\ndata$variable: extracts a specific variable (column) from the data set\n\nYou may also choose to click on the data set name in your Environment window pane in R and the data set will pop up in a new tab in the script pane.\n\n\nWorking with data sets in R\n\nThe cars data set is a default data set that lives in R (for examples like this!).\n\n# outputs the class of the cars data set (data.frame)\nclass(cars)\n\n[1] \"data.frame\"\n\n# outputs the names of the variables included in the cars data set (speed, dist)\nnames(cars)\n\n[1] \"speed\" \"dist\" \n\n# outputs the first six rows of the cars data set\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# outputs\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n# outputs the second row of the cars data set\ncars[2,]\n\n  speed dist\n2     4   10\n\n# outputs the first column of the cars data set as a vector\ncars[,1]\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n# outputs the speed variable of the cars data set as a vector (notice this is the same as above)\ncars$speed\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n\n:::\n\nIn a perfect world, all data would come in the right format for our needs, but this is often not the case. We will spend the next few weeks learning about how to use R to reformat our data to follow the tidy data framework and see why this is so important. For now, we will work with nice clean data sets but you should be able to identify when data follows a tidy data format and when it does not.\n\n2.1.0.1 Wait. So is it Tidy format?\nThe concept of tidy data is useful for mapping variables from the data set to elements in a graph, specifications of a model, or aggregating to create summaries. However, what is considered to be “tidy data” format for one task, might not be in the correct “tidy data” format for a different task. It is important for you to consider the end goal when restructuring your data.\nPart of this course is building the skills for you to be able to map your data operation steps from an original data set to the correct format (and output).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy Data & Basics of Graphics</span>"
    ]
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#loading-external-data",
    "href": "02-tidy-data-and-basics-of-graphics.html#loading-external-data",
    "title": "2  Tidy Data & Basics of Graphics",
    "section": "2.2 Loading External Data",
    "text": "2.2 Loading External Data\n\n2.2.1 An Overview of External Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables (like tidy data we learned about above!). This type of data can be stored on the computer in multiple ways:\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\n.csv “Comma-separated”\n.txt plain text - could be just text, comma-separated, tab-separated, etc.\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n.xls\n.xlsx\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying threads here.\nAlso, there’s this amazing comic:\n\n\nTo be minimally functional in R, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files.\n\nLearn more about external data\n\n\nAdditional external data formats\n\nWe will not cover binary files and databases, but you can consult one or more online references if you are interested in learning about these.\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs. We aren’t going to do anything with binary formats, just know they exist.\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which may be more common:\n\nWeb related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis.\nSpatial files: Shapefiles are by far the most common version of spatial files1. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information.\n\n\n\n\n\n2.2.2 Where does my data live?\n\n\n\nThe most common way to read data into R is with the read_csv() function. You may provide the file input as either a url to the data set or a path for the file input.\n\nThe read_csv() function belongs to the readr package (side note: readr is installed and loaded as part of the tidyverse package). To install, either click on your Packages tab or use:\n\ninstall.packages(\"readr\")\n\n\n\n\nread_csv()\n\n# load the readr package (or alternatively the tidyverse package)\nlibrary(readr)\n\n# this will work for everyone!\nsurveys &lt;- read_csv(file = \"https://raw.githubusercontent.com/earobinson95/stat331-calpoly/master/lab-assignments/Lab2-graphics/surveys.csv\")\n\n# this will work only on my computer\n# notice this is an absolute file path on my computer\nsurveys &lt;- read_csv(file = \"C:/Users/erobin17/OneDrive - Cal Poly/stat331/labs/lab2/surveys.csv\")\n\n\nRecall our discussion about Directories, Paths, and Projects. Often you will specify a relative file path to your data set rather than an absolute file path. This works if the file is in the same directory as the code or within a sub-folder of the same directory as the code.\n\nRelative file paths\n\n# this will work if the file is in the same directory as the code\n# (i.e., the Quarto document and the data are in the same folder)\nsurveys &lt;- read_csv(file = \"surveys.csv\")\n\n# this will work if a sub-folder called \"data\" is in the same directory as the code\n# (i.e., you first have to enter the data sub folder and then you can access the surveys.csv data set)\nsurveys &lt;- read_csv(file = \"data/surveys.csv\")\n\n\n\n\n\nI often organize my workflow with sub-folders so that I would have a sub-folder called data to reference from my base directory location.\nYou can either choose to create an overall data sub-folder right within your stat331 folder (e.g., stat331 &gt; data) and store all of your data for your class there… or you may choose to store the data associated with each individual assignment within that folder (e.g., stat331 &gt; labs &gt; lab2 &gt; data).\n\n\n\nBut wait! I thought we created our RProjects (my-stat331.Rproj) to indicate our “home base” directory? When working with a Quarto document, this changes your base directory (for any code running within the .qmd file) to the same folder the .qmd file lives in.\nOpen your class directory my-stat331.Rproj and type getwd() into the Console. This should lead you to the folder directory your RProject was created in.\nNow open your lab1.qmd assignment and type getwd() into a code chunk. This should lead you to the folder directory your lab1.qmd file is saved in.\nUgh. Well that is confusing.\n\nA great solution to consistency in file paths is the here package:\n\ninstall.packages(\"here\")\n\n\n\n\n\n\n\n\nWorkflows that shred…, by Allison Horst\n\n\n\n\nThis package thinks “here” is the your directory folder the RProject (my-331.Rproj) lives in (e.g., your stat331 folder). This makes a global “home base” that overrides any other directory path (e.g., from your .qmd files).\n\n# shows you the file path the function here() will start at.\nhere::dr_here()\n\n\nThe here package\nIf my stat331 file structure looks like this:\n\n\n\n\n\n\n\n\n\nthen my RProject sets the working directory to this stat331 folder. However, if my lab1.qmd file lives inside my labs folder – e.g., C:/Users/erobin17/OneDrive - Cal Poly/stat331/labs/lab1.qmd, then when I try to load data (surveys.csv) in within the lab1.qmd file, it will be looking for that relative file path from within the labs folder (where the lab1.qmd lives) and not from within the stat331 (“home base”) folder.\n\nsurveys &lt;- read_csv(\"surveys.csv\")\n\nHowever, if my data is stored within a data sub-folder as shown above, then within my lab1.qmd file, my relative file path must “backtrack” (..) out of the labs folder one move before entering into the data folder to access the data set.\n\nsurveys &lt;- read_csv(\"../data/surveys.csv\")\n\nEnter the here package! With the here package, within my lab1.qmd file I can ALWAYS assume I am in my stat331 (“home base”) folder and enter directly into the data sub-folder.\n\nsurveys &lt;- read_csv(here::here(\"data\", \"surveys.csv\"))\n\nNote the folder and file names are in quotations because they are names of files and not objects in R.\n\n\n\n\nUsing “::” before the function (e.g., PackageName::FunctionName) is a way of telling R which package the function lives in without having to load that entire package (e.g., library(PackageName). There are some common functions the R community does this for mainly just out of practice. here is one of them and you will often see here::here() telling you to go into the here package and access/use the here() function. Essentially it saves memory from not having to load unnecessary functions if you only need one function from that package.\n\n\n\nI have a complicated relationship with the here package. I see the strong uses for it, but I don’t always set up my workflow in this way. You will form your own opinions, but the important thing for you to know is how to find the correct file path to read your data into your .qmd files.\n\n\n\n2.2.3 How do I load in my data?\n\nIt may be helpful to save this data import with the tidyverse cheat sheet.\n\n(required) Read the following to learn more about importing data into R\n\nR4DS: Data Import\n\n\nPreviously in this chapter, we learned about common types of data files and briefly introduced the read_csv() and function. There are many functions in R that read in specific formats of data:\n\nBase Rreadr & readxl\n\n\nBase R contains the read.table() and read.csv() functions. In read.table() you must specify the delimiter with the sep = argument. read.csv() is just a specific subset function of read.table() that automatically assumes a comma delimiter.\n\n# comma delimiter\nread.table(file = \"ages.txt\", sep = \",\")\n\n# tab delimiter\nread.table(file = \"ages.txt\", sep = \"\\t\")\n\n\n\nThe tidyverse has some cleaned-up versions in the readr and readxl packages:\n\nread_csv() works like read.csv, with some extra stuff\nread_txv() is for tab-separated data\nread_table() is for any data with “columns” (white space separating)\nread_delim() is for special “delimiters” separating data\nread_xlsx() is specifically for dealing with Excel files\n\n\n\n\n\n\nLearn more\n\nRSQLite vignette\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy Data & Basics of Graphics</span>"
    ]
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#basics-of-graphics",
    "href": "02-tidy-data-and-basics-of-graphics.html#basics-of-graphics",
    "title": "2  Tidy Data & Basics of Graphics",
    "section": "2.3 Basics of Graphics",
    "text": "2.3 Basics of Graphics\nNow that we understand the format of tidy data and how to load external data into R, we want to be able to do something with that data! We are going to start with creating data visualizations (arguably my favorite!).\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts.\n\n\nThis is one of the less serious schemes I’ve seen\n\nBut, in my opinion, Randall missed the opportunity to put a pie chart as Neutral Evil.\n\nHopefully by the end of this, you will be able to at least make the charts which are most commonly used to show data and statistical concepts.\n\n\n2.3.1 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John Tukey, Data Based Graphics: Visual Display in the Decades to Come\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\nConsider this thought experiment: You have a simple data set - 2 numeric variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\n\n\nhead(simple_data)\n\n# A tibble: 6 × 2\n   var1  var2\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.975  16.9\n2 6.10  178. \n3 4.61  103. \n4 6.49  173. \n5 6.60  190. \n6 6.45  170. \n\n\n\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\n\n\nsummary(simple_data)\n\n      var1              var2       \n Min.   :0.03114   Min.   :-35.87  \n 1st Qu.:2.42756   1st Qu.: 31.48  \n Median :5.10491   Median :103.93  \n Mean   :4.98367   Mean   :132.78  \n 3rd Qu.:7.29845   3rd Qu.:219.89  \n Max.   :9.97662   Max.   :432.62  \n\ncov(simple_data$var1, simple_data$var2)\n\n[1] 324.7271\n\n\n\nDraw a scatter plot of the two variables\n\n\nggplot(data = simple_data,\n       mapping = aes(x = var1,\n                     y = var2\n                     )\n       ) +\n  geom_point()\n\n\n\n\n\n\n\n\nWhich one would you rather use? Why?\nOur brains are very good at processing large amounts of visual information quickly. Evolutionarily, it’s important to be able to e.g. survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIt’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.\n\nYou’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while ggplot has a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it.\n\n\n\n\n2.3.2 ggplot2\nIn this class, we’re going to use the ggplot2 package to create graphics in R. This package is already installed as part of the tidyverse, but can be installed:\n\ninstall.packages(\"ggplot2\")\n\nand/or loaded:\n\nlibrary(\"ggplot2\")\n\n# alternatively\nlibrary(\"tidyverse\") # (my preference!)\n\n\n\n\n\n\nBuilding a masterpiece, by Allison Horst\n\n\nWe will learn about all of these different pieces and the process of creating graphics by working through examples, but there is a general “template” for creating graphics in ggplot:\n\nggplot(data = &lt;DATA&gt;,\n       mapping = aes(&lt;MAPPINGS&gt;)\n       ) +\n  &lt;GEOM FUNCTION&gt;() +\n  any other arugments ...\n\nwhere\n\n is the name of the data set\n is the name of the geom you want for the plot (e.g., geom_histogram(), geom_line(), geom_point())\n is where variables from the data are mapped to parts of the plot (e.g., x = speed, y = distance, color = carmodel)\nany other arguments could include the theme, labels, faceting, etc.\n\nYou may want to download and save cheat sheets and reference guides for ggplot.\n\n\n2.3.3 The Grammar of Graphics\nThe grammar of graphics is an approach first introduced in Leland Wilkinson’s book (Wilkinson 2005). Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the data set itself relates to the components of the chart.\nThis has a few advantages:\n\nIt’s relatively easy to represent the same data set with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: (Sarkar 2018)\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\n\nExploratory Data Analysis with the grammar of graphics\n\nSketchggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing) # this data set is housed internally to R\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median\n           )\n       ) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions/aspects of your data set you want to visualize.\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms (geometric objects) that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom (e.g., points on a scatter plot WITH a line of best fit overlaid).\n\nDecide the geometric objects (geoms) you want to represent your variables with on the chart. For example, a scatterplot displays the information as points by mapping the \\(x\\) and \\(y\\) aesthetics to points on the graph.\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from Sarkar (2018)).\n\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of ggplot2’s features.\nBefore we start exploring, let’s add a title and label our axes, so that we’re creating good, informative charts:\nxlab(), ylab(), and ggtitle().\nAlternatively, you could use labs(title = , x = , y = ) to add a title and label the axes.\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median)\n       ) + \n  geom_point() + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\") \n\n\n\n\n\n\n\n\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\ngeom_smooth(method = \"&lt;LINE TYPE&gt;\") where method = lm adds a linear regression model and method = loess adds a loess regression smoother.\n\nSketchggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median)\n       ) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median)\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n...aes(...color = &lt;VARIABLE&gt;)\n\nSketchggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median, \n           color = city \n           )\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, \n       aes(x = date, \n           y = median, \n           color = city\n           )\n       ) + \n  # geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities). Don’t worry too much about the subsetting code yet, we will get to that next week!\n\ncitylist &lt;- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub &lt;- txhousing |&gt;\n  filter(city %in% citylist)\n\nggplot(data = housingsub, #&lt;&lt; \n       aes(x = date, \n           y = median, \n           color = city\n           )\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\n\n\n\n\nInstead of using color, another way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\nfacet_wrap(), facet_grid()\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, \n       aes(x = date, \n           y = median\n           )\n       ) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  facet_wrap(~ city) + \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\nggplot(data = housingsub, \n       aes(x = date, \n           y = median, \n           size = sales)\n       ) + \n  # Make points transparent\n  geom_point(alpha = 0.15) +  \n  geom_smooth(method = \"loess\") + \n  facet_wrap(~ city) +\n  # Remove extra information from the legend - \n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales',  \n                             override.aes = list(linetype = NA,  \n                                                 fill = 'transparent' \n                                                 ) \n                             ) \n         ) + #&lt;&lt; \n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), #&lt;&lt; \n        legend.justification = c(1, 0) \n        ) + #&lt;&lt; \n  xlab(\"Date\") + \n  ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\n\n\n\n\nUp to this point, we’ve used the same position information - date for the \\(x\\) axis, median sale price for the \\(y\\) axis. Let’s switch that up a bit so that we can play with some transformations on the \\(x\\) and \\(y\\) axis and add variable mappings to a continuous variable.\n\nggplot(data = housingsub, \n       aes(x = listings, \n           y = sales, \n           color = city)\n       ) + \n  geom_point(alpha = 0.15) +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + \n  ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices (subset of cities)\")\n\n\n\n\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation2. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\nscale_x_log10(), scale_y_log10()\n\nggplot(data = housingsub, \n       aes(x = listings, #&lt;&lt; \n           y = sales, #&lt;&lt; \n           color = city\n           )\n       ) + \n  geom_point(alpha = 0.15) +\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() + #&lt;&lt; \n  scale_y_log10() + #&lt;&lt; \n  xlab(\"Number of Listings\") + \n  ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in \\(x\\) and \\(y\\), and mapping the size or color of the point to number of sales.\n\nhouston &lt;- txhousing |&gt; \n  filter(city == \"Houston\")\n\nggplot(data = houston,\n       aes(x = date, \n           y = inventory, \n           size = sales \n           )\n       ) + \n  geom_point(shape = 1) + \n  xlab(\"Date\") + \n  ylab(\"Months of Inventory\") + \n  guides(size = guide_legend(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\n\n\nggplot(data = houston, \n       aes(x = date, \n           y = inventory, \n           color = sales \n           )\n       ) + \n  geom_point() + \n  xlab(\"Date\") + \n  ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, \n       aes(x = sales, #&lt;&lt; \n           y = inventory, \n           color = date \n           )\n       ) + \n  geom_point() + \n  xlab(\"Number of Sales\") + \n  ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Date\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\n\nIs that easier or harder to read?\n\n\n\n\nSpecial properties of aesthetics\n\nGlobal vs Local aesthetics\nAny aesthetics assigned to the mapping within the first line, ggplot(), will be inherited by the rest of the geometric, geom_xxx(), lines. This is called a global aesthetic.\n\nggplot(data = housingsub, \n       mapping = aes(x = date, \n                     y = median, \n                     color = city \n                     )\n       ) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAny aesthetics assigned to the mapping within a geometric object is only applied to that specific geom. Notice how color is no longer mapped to the regression line and there is only one overall regression line for all cities.\n\nggplot(data = housingsub, \n       mapping = aes(x = date, \n                     y = median\n                     )\n       ) +\n  geom_point(mapping = aes(color = city) \n             ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAssigning your aesthetics vs Setting your aesthetics\nWhen you assign a variable from your data set to your aesthetics, you put it inside aes() without quotation marks around the variable name:\n\nggplot(data = housingsub) +\n  geom_point(mapping = aes(x = date, \n                           y = median, \n                           color = city \n                           )\n             )\n\n\n\n\n\n\n\n\nIf you want to set a specific color, you could put this outside the aes() mapping in quotation marks because blue is not an object in R. Notice this color is assigned specifically in that geometric object.\n\nggplot(data = housingsub) +\n  geom_point(mapping = aes(x = date, \n                           y = median\n                           ), \n             color = \"blue\" \n             )\n\n\n\n\n\n\n\n\nYou should NOT put your color inside the aes() parentheses.\n\nggplot(data = txhousing) +\n  geom_point(mapping = aes(x = date, \n                           y = median, \n                           color = \"blue\" \n                           )\n             )\n\n\n\n\n\n\n\n\n\n\n\n2.3.3.1 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type. Consider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through cataloges like the R Graph Gallery to see what visualizations match your data and use-case. ### Creating Good Charts\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.\n\n\n2.3.3.2 Perceptual and Cognitive Factors\n\nColor\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, &lt;1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\n\nYou can take a test designed to screen for colorblindness here\n\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n \n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\n\n\nIt is possible to simulate the effect of color blindness and color deficiency on an image.\n\n\n\n\n\n\n\nOriginal image using a rainbow color scale\n\n\n\n\n\n\n\nRed cone deficient\n\n\n\n\n\n\n\nGreen cone deficient\n\n\n\n\n\n\n\nBlue cone deficient\n\n\n\n\n\n\n\n\n\nRed cone absent\n\n\n\n\n\n\n\nGreen cone absent\n\n\n\n\n\n\n\nBlue cone absent\n\n\n\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\nImplications and Guidelines\n\nDo not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -&gt; blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.3\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women4.\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\n\n\n\nShort Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\n\nTry it out!\n\n\nClick here, read the information, and then click to hide it.\n\n1 4 2 2 3 9 8 0 7 8\n\n\n\nWait a few seconds, then expand this section\n\nWhat was the third number?\n\n\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n\n\nGrouping and Sense-making\nImposing order on visual chaos.\n\nAmbiguous ImagesIllusory ContoursFigure/Ground\n\n\nWhat does the figure below look like to you?\n\n\n\nIs it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\n\nConsider this image - what do you see?\n\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\n\n\n\n\n\n\n\n\nOr, I could use a line chart\n\n\n\n\n\n\n\n\n\nOr, I could use a box plot\n\n\n\n\n\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.^[See this paper for more details.\n\n\n\nGeneral guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.5\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n\n\n\n\n\n\n\n\n\n\n.\n\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\n\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration).\n\n\n\nR graphics\n\nggplot2 cheat sheet\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nggplot tricks\nR graph cookbook\nData Visualization in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nSarkar, Dipanjan (DJ). 2018. “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-Dimensional….” Medium. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Statistics and Computing. New York: Springer Science.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy Data & Basics of Graphics</span>"
    ]
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#footnotes",
    "href": "02-tidy-data-and-basics-of-graphics.html#footnotes",
    "title": "2  Tidy Data & Basics of Graphics",
    "section": "",
    "text": "though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times↩︎\nThis isn’t necessarily a good thing, but you should know how to do it. The jury is still very much out on whether log transformations make data easier to read and understand↩︎\nWhen the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information.↩︎\nLisa Charlotte Rost. What to consider when choosing colors for data visualization.↩︎\nSee this paper for the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy Data & Basics of Graphics</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html",
    "href": "03-data-cleaning-and-manipulation.html",
    "title": "3  Data Cleaning and Manipulation",
    "section": "",
    "text": "Objectives\nReading: 18 minute(s) at 200 WPM.\nVideos: 60 minutes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#ch3-objectives",
    "href": "03-data-cleaning-and-manipulation.html#ch3-objectives",
    "title": "3  Data Cleaning and Manipulation",
    "section": "",
    "text": "Apply data manipulation verbs (filter, select, group by, summarize, mutate) to prepare data for analysis\nIdentify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately and understand the consequences of data cleaning steps\nCreate summaries of data appropriate for analysis or display using data manipulation techniques",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#a-quick-note-about-tibble",
    "href": "03-data-cleaning-and-manipulation.html#a-quick-note-about-tibble",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.1 A quick note about tibble",
    "text": "3.1 A quick note about tibble\nWe have been talking about our data in terms of data.frame objects in R. This is meant to inform you there is another object type in R called tibbles. Essentially, Tibbles are data frames, but they have certain features that make them easier to work with and provide additional cool features that can be useful (e.g., see nest()).\n\ntibble(\n  team   = c(\"A\", \"B\", \"C\", \"D\"), \n  points = c(22, 30, 18, 54)\n)\n\n# A tibble: 4 × 2\n  team  points\n  &lt;chr&gt;  &lt;dbl&gt;\n1 A         22\n2 B         30\n3 C         18\n4 D         54\n\n\nYou can use as_tibble() to convert data.frame objects in R to a tibble object.\n\nLearn more about tibbles\nYou can read more about Tibbles in R for Data Science: Tibbles",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#introduction-to-dplyr",
    "href": "03-data-cleaning-and-manipulation.html#introduction-to-dplyr",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.2 Introduction to dplyr",
    "text": "3.2 Introduction to dplyr\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for data visualization or statistical analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\n\n\n\n\n\nData wrangling (by Allison Horst)\n\n\nWe will be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations.\n\n\n\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\n\nMain dplyr verbs\n\nfilter()\narrange()\nselect()\nmutate()\nsummarize()\nUse group_by() to perform group wise operations\nUse the pipe operator (|&gt; or %&gt;%) to chain together data wrangling operations\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#motivation-example-dataset",
    "href": "03-data-cleaning-and-manipulation.html#motivation-example-dataset",
    "title": "3  Data Cleaning and Manipulation",
    "section": "Motivation & Example Dataset",
    "text": "Motivation & Example Dataset\nLast week we learned all about creating graphics in ggplot2. I am hoping to use data visualization as motivation going forward in this class – how do we get our data look like what we need in order to create the graph we want?\n\nLet’s explore how the dplyr verbs work, using the starwars data set, which contains a comprehensive list of the characters in the Star Wars movies and information about their height, mass, hair_color, skin_color, eye_color, birth_year, sex, gender, homeworld, species, films, vehicles, and starships.\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load data set into memory. The loading isn’t complete until we actually use the data set though… so let’s look at our variables and types and print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstr(starwars)\nstarwars\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\nWe could create a scatterplot of the character’s height by mass, color by species, and facet by homeworld.\n\nlibrary(ggplot2)\nggplot(data = starwars, aes(x = height, \n                            y = mass, \n                            color = species)\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\n\n\n\n\nThere is way too much going on in these plots to see anything of importance. Let’s break it down into the parts we are interested in.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#filter-pick-cases-rows-based-on-their-values",
    "href": "03-data-cleaning-and-manipulation.html#filter-pick-cases-rows-based-on-their-values",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.3 filter(): Pick cases (rows) based on their values",
    "text": "3.3 filter(): Pick cases (rows) based on their values\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\n\n\n\n\ndplyr filter() by Allison Horst\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Indexing Matrices, but here we’ll focus on using specific functions to perform the same operation.\nThe dplyr verb for selecting rows is filter(). filter() takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\nLet’s say we were interested in only the people, we could create a new data set starwars_people and filter on the species variable.\n\n# Get only the people\nstarwars_people &lt;- filter(.data = starwars, \n                          species == \"Human\"\n                          )\nstarwars_people\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\n\n\n\n\n\nWe can create the same plot with our new subset of data (starwars_people).\n\n\nCode\nggplot(data = starwars_people, \n       mapping = aes(x = height, \n                     y = mass, \n                     color = species\n                     )\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\n\n\n\n\n\nThis looks better, but what if we only care about the people who come from Tatooine? Starting with our original starwars data set, we can combine logical AND statements with a comma to define a data subset called starwars_tatoonie_people.\n\n# Get only the people who come from Tatooine\nstarwars_tatooine_people &lt;- filter(.data = starwars, \n                                   species == \"Human\", \n                                   homeworld == \"Tatooine\"\n                                   )\nstarwars_tatooine_people\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = starwars_tatooine_people, \n       mapping = aes(x = height, \n                     y = mass, \n                     color = species\n                     )\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\n\n\n\n\n\n\n\n\nUseful comparison operations in R\nWe might not always want to only filter on a variable set equal to a certain category or value, the following operations can help you combine logical operations in filter().\n\n&gt; greater than\n&lt; less than\n== equal to\n%in% identifies if an element belongs to a vector\n| or\n\n\n\n3.3.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements.\n\n\n\n\nFiltering by row number\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\nNotice how we now have C-3PO, Darth Vader, Beru Whites, Anakin Skywalker, etc. (rows 2, 4, 6, …) from the original starwars data set output above.\n\nfilter(.data = starwars, \n       row_number() %% 2 == 0\n       ) \n\n\nEven RowsOriginal starwars\n\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\n\n\n\n\n\n\narrange() Sorting rows by variable values\nAnother common operation is to sort your data frame by the values of one or more variables.\narrange() is a dplyr verb for sorting rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\nThe code below arranges the starwars characters tallest to shortest.\n\narrange(.data = starwars, \n        desc(height)\n        )\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nYarael Poof\n264\nNA\nnone\nwhite\nyellow\nNA\nmale\nmasculine\nQuermia\nQuermian\n\n\nTarfful\n234\n136\nbrown\nbrown\nblue\nNA\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nLama Su\n229\n88\nnone\ngrey\nblack\nNA\nmale\nmasculine\nKamino\nKaminoan\n\n\nChewbacca\n228\n112\nbrown\nunknown\nblue\n200.0\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nRoos Tarpals\n224\n82\nnone\ngrey\norange\nNA\nmale\nmasculine\nNaboo\nGungan\n\n\nGrievous\n216\n159\nnone\nbrown, white\ngreen, yellow\nNA\nmale\nmasculine\nKalee\nKaleesh\n\n\nTaun We\n213\nNA\nnone\ngrey\nblack\nNA\nfemale\nfeminine\nKamino\nKaminoan\n\n\nRugor Nass\n206\nNA\nnone\ngreen\norange\nNA\nmale\nmasculine\nNaboo\nGungan\n\n\nTion Medon\n206\n80\nnone\ngrey\nblack\nNA\nmale\nmasculine\nUtapau\nPau'an\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\n\n\n\n\n\n\n\n\nKeep the top \\(n\\) values of a variable\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nThe code below outputs the 5 tallest characters in star wars.\n\nslice_max(.data = starwars, \n          order_by = height, \n          n = 5\n          )\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nYarael Poof\n264\nNA\nnone\nwhite\nyellow\nNA\nmale\nmasculine\nQuermia\nQuermian\n\n\nTarfful\n234\n136\nbrown\nbrown\nblue\nNA\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nLama Su\n229\n88\nnone\ngrey\nblack\nNA\nmale\nmasculine\nKamino\nKaminoan\n\n\nChewbacca\n228\n112\nbrown\nunknown\nblue\n200\nmale\nmasculine\nKashyyyk\nWookiee\n\n\nRoos Tarpals\n224\n82\nnone\ngrey\norange\nNA\nmale\nmasculine\nNaboo\nGungan\n\n\n\n\n\n\n\n\nOf course, there is a similar slice_min() function as well:\n\nThe code below outputs the 5 shortest characters in star wars.\n\nslice_min(.data = starwars, \n          order_by = height, \n          n = 5\n          )\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nYoda\n66\n17\nwhite\ngreen\nbrown\n896\nmale\nmasculine\nNA\nYoda's species\n\n\nRatts Tyerel\n79\n15\nnone\ngrey, blue\nunknown\nNA\nmale\nmasculine\nAleen Minor\nAleena\n\n\nWicket Systri Warrick\n88\n20\nbrown\nbrown\nbrown\n8\nmale\nmasculine\nEndor\nEwok\n\n\nDud Bolt\n94\n45\nnone\nblue, grey\nyellow\nNA\nmale\nmasculine\nVulpter\nVulptereen\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33\nnone\nmasculine\nNaboo\nDroid\n\n\nR4-P17\n96\nNA\nnone\nsilver, red\nred, blue\nNA\nnone\nfeminine\nNA\nDroid\n\n\n\n\n\n\n\nBy default, slice_max() and slice_min() return values tied with the nth value as well, which is why our result above has 6 rows.\n\n\n\nUse with_ties = FALSE.\n\nslice_min(.data = starwars, \n          order_by = height, \n          n = 5, \n          with_ties = FALSE\n          )\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nYoda\n66\n17\nwhite\ngreen\nbrown\n896\nmale\nmasculine\nNA\nYoda's species\n\n\nRatts Tyerel\n79\n15\nnone\ngrey, blue\nunknown\nNA\nmale\nmasculine\nAleen Minor\nAleena\n\n\nWicket Systri Warrick\n88\n20\nbrown\nbrown\nbrown\n8\nmale\nmasculine\nEndor\nEwok\n\n\nDud Bolt\n94\n45\nnone\nblue, grey\nyellow\nNA\nmale\nmasculine\nVulpter\nVulptereen\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33\nnone\nmasculine\nNaboo\nDroid\n\n\n\n\n\n\n\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nThe code below outputs the shortest 2% of characters in star wars.\n\nslice_min(.data = starwars, \n          order_by = height, \n          prop = 0.02\n          )\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nYoda\n66\n17\nwhite\ngreen\nbrown\n896\nmale\nmasculine\nNA\nYoda's species",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#select-pick-columns",
    "href": "03-data-cleaning-and-manipulation.html#select-pick-columns",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.4 select(): Pick columns",
    "text": "3.4 select(): Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data (.data =). After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\n\nAn exhaustive(?) list of ways to select variables in dplyr\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n!(&lt;set of variables&gt;) will give you any columns that aren’t in the set of variables in parentheses\n\n(&lt;set of vars 1&gt;) & (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2.\nc() combines sets of variables.\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\neverything() matches all variables\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n\nLet’s try these selector functions out and see what we can accomplish!\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting simple, let’s only subset and keep only the following variables from the starwars data set: name, height, mass, birth_year, species, and homeworld.\n\nselect(.data = starwars, name, height, mass, birth_year, species, homeworld)\n\n\n\n\n\n\nname\nheight\nmass\nbirth_year\nspecies\nhomeworld\n\n\n\n\nLuke Skywalker\n172\n77\n19.0\nHuman\nTatooine\n\n\nC-3PO\n167\n75\n112.0\nDroid\nTatooine\n\n\nR2-D2\n96\n32\n33.0\nDroid\nNaboo\n\n\nDarth Vader\n202\n136\n41.9\nHuman\nTatooine\n\n\nLeia Organa\n150\n49\n19.0\nHuman\nAlderaan\n\n\nOwen Lars\n178\n120\n52.0\nHuman\nTatooine\n\n\nBeru Whitesun Lars\n165\n75\n47.0\nHuman\nTatooine\n\n\nR5-D4\n97\n32\nNA\nDroid\nTatooine\n\n\nBiggs Darklighter\n183\n84\n24.0\nHuman\nTatooine\n\n\nObi-Wan Kenobi\n182\n77\n57.0\nHuman\nStewjon\n\n\n\n\n\n\n\nSince name, height, and mass are next to each other, we could have specified name:mass to tell us to select all of the columns between and including name to mass.\n\nselect(.data = starwars, name:mass, birth_year, species, homeworld)\n\n\n\n\n\n\nname\nheight\nmass\nbirth_year\nspecies\nhomeworld\n\n\n\n\nLuke Skywalker\n172\n77\n19.0\nHuman\nTatooine\n\n\nC-3PO\n167\n75\n112.0\nDroid\nTatooine\n\n\nR2-D2\n96\n32\n33.0\nDroid\nNaboo\n\n\nDarth Vader\n202\n136\n41.9\nHuman\nTatooine\n\n\nLeia Organa\n150\n49\n19.0\nHuman\nAlderaan\n\n\nOwen Lars\n178\n120\n52.0\nHuman\nTatooine\n\n\nBeru Whitesun Lars\n165\n75\n47.0\nHuman\nTatooine\n\n\nR5-D4\n97\n32\nNA\nDroid\nTatooine\n\n\nBiggs Darklighter\n183\n84\n24.0\nHuman\nTatooine\n\n\nObi-Wan Kenobi\n182\n77\n57.0\nHuman\nStewjon\n\n\n\n\n\n\n\nThe select column is also useful for reordering the variables in your data set.\nPerhaps we want the birth_year, sex, gender, homeworld, and species to follow the name of the star wars character. We can use the everything() function to specify we want all the other variables to follow.\n\nselect(.data = starwars, name, birth_year:species, everything())\n\n\n\n\n\n\nname\nbirth_year\nsex\ngender\nhomeworld\nspecies\nheight\nmass\nhair_color\nskin_color\neye_color\n\n\n\n\nLuke Skywalker\n19.0\nmale\nmasculine\nTatooine\nHuman\n172\n77\nblond\nfair\nblue\n\n\nC-3PO\n112.0\nnone\nmasculine\nTatooine\nDroid\n167\n75\nNA\ngold\nyellow\n\n\nR2-D2\n33.0\nnone\nmasculine\nNaboo\nDroid\n96\n32\nNA\nwhite, blue\nred\n\n\nDarth Vader\n41.9\nmale\nmasculine\nTatooine\nHuman\n202\n136\nnone\nwhite\nyellow\n\n\nLeia Organa\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n150\n49\nbrown\nlight\nbrown\n\n\nOwen Lars\n52.0\nmale\nmasculine\nTatooine\nHuman\n178\n120\nbrown, grey\nlight\nblue\n\n\nBeru Whitesun Lars\n47.0\nfemale\nfeminine\nTatooine\nHuman\n165\n75\nbrown\nlight\nblue\n\n\nR5-D4\nNA\nnone\nmasculine\nTatooine\nDroid\n97\n32\nNA\nwhite, red\nred\n\n\nBiggs Darklighter\n24.0\nmale\nmasculine\nTatooine\nHuman\n183\n84\nblack\nlight\nbrown\n\n\nObi-Wan Kenobi\n57.0\nmale\nmasculine\nStewjon\nHuman\n182\n77\nauburn, white\nfair\nblue-gray\n\n\n\n\n\n\n\nNote that everything() won’t duplicate columns you’ve already added.\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\n\ndplyr::relocate\n\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n\n# move numeric variables to the front\nrelocate(.data = starwars, \n         where(is.numeric)\n         )\n\n# A tibble: 87 × 11\n   height  mass birth_year name     hair_color skin_color eye_color sex   gender\n    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; \n 1    172    77       19   Luke Sk… blond      fair       blue      male  mascu…\n 2    167    75      112   C-3PO    &lt;NA&gt;       gold       yellow    none  mascu…\n 3     96    32       33   R2-D2    &lt;NA&gt;       white, bl… red       none  mascu…\n 4    202   136       41.9 Darth V… none       white      yellow    male  mascu…\n 5    150    49       19   Leia Or… brown      light      brown     fema… femin…\n 6    178   120       52   Owen La… brown, gr… light      blue      male  mascu…\n 7    165    75       47   Beru Wh… brown      light      blue      fema… femin…\n 8     97    32       NA   R5-D4    &lt;NA&gt;       white, red red       none  mascu…\n 9    183    84       24   Biggs D… black      light      brown     male  mascu…\n10    182    77       57   Obi-Wan… auburn, w… fair       blue-gray male  mascu…\n# ℹ 77 more rows\n# ℹ 2 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#mutate-add-and-transform-variables",
    "href": "03-data-cleaning-and-manipulation.html#mutate-add-and-transform-variables",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.5 mutate(): Add and transform variables",
    "text": "3.5 mutate(): Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our data set in various ways (i.e., remove rows or columns from the original data set). But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate().\n\n\n\n\n\n\nMutate (by Allison Horst)\n\n\n\nLet’s create a new variable, BMI calculated from existing columns – mass/height\\(^2\\)\n\nmutate(.data = starwars, \n       BMI = mass/height^2, \n       .after = mass\n       )\n\n\n\n\n\n\nname\nheight\nmass\nBMI\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nLuke Skywalker\n172\n77\n0.0026028\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\n0.0026892\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\nR2-D2\n96\n32\n0.0034722\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\nDarth Vader\n202\n136\n0.0033330\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\n0.0021778\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\n0.0037874\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\n0.0027548\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\n0.0034010\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nDroid\n\n\nBiggs Darklighter\n183\n84\n0.0025083\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\n0.0023246\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\nBy default, the new variable will be tacked on to the end of the data set as the last column. Using .after or .before arguments allows you to place the new variable in the middle of the data set.\n\nWe can combine the mutate function with other variables such as ifelse().\n\nLet’s replace the species variable to indicate Human or Not Human.\n\nmutate(.data = starwars, \n       species = if_else(species == \"Human\", \n                        species, \n                        \"Not Human\"\n                        )\n       )\n\n\n\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\n\nLuke Skywalker\n172\n77\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\nC-3PO\n167\n75\nNA\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nNot Human\n\n\nR2-D2\n96\n32\nNA\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nNot Human\n\n\nDarth Vader\n202\n136\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\nLeia Organa\n150\n49\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\nOwen Lars\n178\n120\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\nBeru Whitesun Lars\n165\n75\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\nR5-D4\n97\n32\nNA\nwhite, red\nred\nNA\nnone\nmasculine\nTatooine\nNot Human\n\n\nBiggs Darklighter\n183\n84\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\nObi-Wan Kenobi\n182\n77\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n\n\n\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. We will cover special data types like these in a few weeks!\n\n\nMutate and new challenges\n\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation and Google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of Googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\n\nSource\n\n\n\n\n\nSource\n\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\n\nApologies for the noninclusive language, but the sentiment is real. Source",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#summarize",
    "href": "03-data-cleaning-and-manipulation.html#summarize",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.6 summarize()",
    "text": "3.6 summarize()\nThe next verb is one that we’ve already implicitly seen in action: summarize() takes a data frame with potentially many rows of data and reduces it down to one row of data using some function.\n\n\n\n\nHere (in a trivial example), I compute the overall average height of a star war’s character.\n\nsummarize(.data = starwars,\n          avg_height = mean(height, na.rm = T)\n          )\n\n# A tibble: 1 × 1\n  avg_height\n       &lt;dbl&gt;\n1       175.\n\n\nThe na.rm = T argument says to ignore/remove the missing (NA) values in calculating the average.\n\nThe real power of summarize, though, is in combination with group_by. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#group_by-group-by-power",
    "href": "03-data-cleaning-and-manipulation.html#group_by-group-by-power",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.7 group_by() Group By + (?) = Power!",
    "text": "3.7 group_by() Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\n\n\n\nsplit my data set into smaller data sets - one for each day\ncompute summary values for each smaller data set\nput my summarized data back together into a single data set\n\nThis is known as the split-apply-combine “Group by: Split-Apply-Combine” (2022) or sometimes, map-reduce (Dean and Ghemawat 2008) strategy (though map-reduce is usually on specifically large data sets and performed in parallel).\nIn tidy parlance, group_by() is the verb that accomplishes the first task. summarize() accomplishes the second task and implicitly accomplishes the third as well.\n\nLet’s see how things change when we calculate the average height of star wars characters by their species.\n\nstarwars |&gt; \n  group_by(species) |&gt; \n  summarize(height = mean(height, na.rm = T))\n\n# A tibble: 38 × 2\n   species   height\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Aleena       79 \n 2 Besalisk    198 \n 3 Cerean      198 \n 4 Chagrian    196 \n 5 Clawdite    168 \n 6 Droid       131.\n 7 Dug         112 \n 8 Ewok         88 \n 9 Geonosian   183 \n10 Gungan      209.\n# ℹ 28 more rows\n\n\nThe next section Pipe Operator will introduce and talk about what the |&gt; symbol is, this example is just hard to show without it!\n\n\n\n\n\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\nWhen you group_by() a variable, your result carries this grouping with it. summarize() will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#pipe",
    "href": "03-data-cleaning-and-manipulation.html#pipe",
    "title": "3  Data Cleaning and Manipulation",
    "section": "3.8 Pipe Operator",
    "text": "3.8 Pipe Operator\nThe powerhouse of the tidyverse package comes from the pipe operator. This specifies a sequence of operations (kind of like how we layered our graphics in ggplot2). The output from the previous line (often a subset) is automatically passed into the first argument of the next line (remember, data first! .data =).\nThe native pipe operator is |&gt;, but the magrittr pipe operator %&gt;% was used up until recently (and still is often used!).\n\n\n\n\n\n\nThe keyboard shortcut for adding a pipe operator to your code is Ctrl/Cmd + Shift + M.\nHowever, if you want to use this shortcut for the native pipe, you need to change your global R settings:\nTools &gt; Global Options &gt; Code &gt; checkbox Use native pipe operator, |&gt;\n\n\n\n(required) Read more about the pipe operators at Workflow Pipes and Formatting Pipes.\n\n\n\nLet’s combine all of our new skills with the pipe operator!\n\n\nUse filter() to subset our data to only Human’s and Droid’s\nUse mutate() to create the new variable, BMI,\nUse group_by() to create groups by species,\nUse summarize() to calculate the mean and standard deviation of BMI\nUse mutate() to calculate the average plus/minus one standard deviation.\n\nWe could either assign this new data set that has summary values of BMI by species or we could pipe the data set directly into a plot – recall the first argument for ggplot() is data =.\n\nstarwars |&gt; \n  filter(species %in% c(\"Human\", \"Droid\")) |&gt; \n  mutate(BMI = mass/height^2) |&gt; \n  group_by(species) |&gt; \n  summarize(avg_BMI = mean(BMI, na.rm = TRUE),\n            sd_BMI = sd(BMI, na.rm = TRUE)\n            ) |&gt; \n  mutate(BMI_1sd_below = avg_BMI - sd_BMI,\n         BMI_1sd_above = avg_BMI + sd_BMI\n         ) |&gt; \n  ggplot(aes(x = species, \n             y = avg_BMI)\n         ) +\n  geom_point() +\n  geom_errorbar(aes(ymin = BMI_1sd_below,\n                    ymax = BMI_1sd_above),\n                width = 0.2\n                ) +\n  labs(x = \"Species\", \n       subtitle = \"Average BMI\") +\n  theme(axis.title.y = element_blank())\n\n\n\n\n\n\n\n\nAs with ggplot, formatting your dplyr code pipelines so it is readable will help both you and me!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#additional-resources",
    "href": "03-data-cleaning-and-manipulation.html#additional-resources",
    "title": "3  Data Cleaning and Manipulation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nModern Dive: Data Wrangling\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#references",
    "href": "03-data-cleaning-and-manipulation.html#references",
    "title": "3  Data Cleaning and Manipulation",
    "section": "References",
    "text": "References\n\n\n\n\nDean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” Communications of the ACM 51 (1): 107–13. https://doi.org/10.1145/1327452.1327492.\n\n\n“Group by: Split-Apply-Combine.” 2022. In Pandas 1.4.3 Documentation. Python. https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40: 1–29.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#footnotes",
    "href": "03-data-cleaning-and-manipulation.html#footnotes",
    "title": "3  Data Cleaning and Manipulation",
    "section": "",
    "text": "See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.↩︎\nThe philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.↩︎\nIt accomplishes this through the magic of quasiquotation, which we will not cover in this course because it’s basically witchcraft.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Cleaning and Manipulation</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html",
    "href": "04-data-joins-and-transformations.html",
    "title": "4  Data Joins and Transformations",
    "section": "",
    "text": "Objectives\nReading: 17 minute(s) at 200 WPM.\nVideos: 26 minutes.\nThe first section of this chapter is heavily outsourced to r4ds as they do a much better job at providing examples and covering the extensive functionality of factor data types than I myself would ever be able to.\nBroadly, your objective while reading the second section of this chapter is to be able to identify data sets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should master the following concepts:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html#ch4-objectives",
    "href": "04-data-joins-and-transformations.html#ch4-objectives",
    "title": "4  Data Joins and Transformations",
    "section": "",
    "text": "Use forcats to reorder and relabel factor variables in data cleaning steps and data visualizations.\n\n\n\nDetermine what data format is necessary to generate a desired plot or statistical model.\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures.\nUnderstand relational data formats and how to use data joins to assemble data from multiple tables into a single table.\n\n\nFunctions covered this week:\nlibrary(forcats) (not an exhaustive list)\n\nfactor(), levels()\nfct_relevel()\nfct_reorder() fct_reorder2()\nfct_collapse()\n\nDownload the forcats cheatsheet.\nlibrary(tidyr)\n\npivot_longer(), pivot_wider()\nseparate(), unite()\n\nlibrary(dplyr)\n\nleft_join(), right_join(), full_join()\nsemi_join(), anti_join()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html#factors-with-forcats",
    "href": "04-data-joins-and-transformations.html#factors-with-forcats",
    "title": "4  Data Joins and Transformations",
    "section": "4.1 Factors with forcats",
    "text": "4.1 Factors with forcats\nWe have been floating around the idea of factor data types. In this section, we will formally define factors and why they are needed for data visualization and analysis. We will then learn useful functions for working with factors in our data cleaning steps.\n\n\n\nIn short, factors are categorical variables with a fixed number of values (think a set number of groups). One of the main features that set factors apart from groups is that you can reorder the groups to be non-alphabetical. In this section we will be using the forcats package (part of the tidyverse!) to create and manipulate factor variables.\n\n(Required) Go read about factors in r4ds.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html#identifying-the-problem-messy-data",
    "href": "04-data-joins-and-transformations.html#identifying-the-problem-messy-data",
    "title": "4  Data Joins and Transformations",
    "section": "4.2 Identifying the problem: Messy data",
    "text": "4.2 Identifying the problem: Messy data\nThe illustrations below are lifted from an excellent blog post (Lowndes and Horst 2020) about tidy data; they’re reproduced here because\n\n\n\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing or plotting.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\n\n\n\n\nFigure 4.1: Tidy data format, illustrated.\n\n\n\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the dataset, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own dataset.\n\n\n\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\n\n\n\nFigure 4.2: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\n\n\n\nExamples: Messy Data\n\nThese datasets all display the same data: TB (Tuberculosis) cases documented by the WHO (World Health Organization) in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\nFor each of the data set, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\nAll of these data sets are “built-in” to the tidyverse package\n\nTable 12345\n\n\n\n\n\nTable 1\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\n\ncountry\nyear\ntype\ncount\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\n\ncountry\nyear\nrate\n\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\n\ncountry\ncentury\nyear\nrate\n\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n:::\n\n\nBy the end of this chapter, you will have the skills needed to wrangle and transform the most common “messy” data sets into “tidy” form.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html#pivot-operations",
    "href": "04-data-joins-and-transformations.html#pivot-operations",
    "title": "4  Data Joins and Transformations",
    "section": "4.3 Pivot Operations",
    "text": "4.3 Pivot Operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\nThe two operations we’ll learn here are wide -&gt; long and long -&gt; wide.\n\nThis animation uses the functions pivot_wider() and pivot_longer() from the tidyr package in R – Animation source.\n\n4.3.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\n\n\nPicture the Operationpivot_longer()\n\n\nTables 4a and 4b (from above) are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntable4a |&gt; \n  pivot_longer(cols = `1999`:`2000`, \n               names_to = \"year\", \n               values_to = \"cases\"\n               )\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\ntable4b |&gt; \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"population\"\n               )\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables (columns) we want to pivot (see table4a pivot)\nselect variables (columns) we don’t want to pivot, using - to remove them (see table4b pivot)\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\n\n\n\n\n\n4.3.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\n\n\nPicture the Operationpivot_wider()\n\n\nTable 2 (from above) is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\n\ntable2 |&gt;\n  pivot_wider(names_from  = type, \n              values_from = count\n              )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\n\n\nLearn More in R4DS\nRead more about pivoting in r4ds.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html#separating-and-uniting-variables",
    "href": "04-data-joins-and-transformations.html#separating-and-uniting-variables",
    "title": "4  Data Joins and Transformations",
    "section": "4.4 Separating and Uniting Variables",
    "text": "4.4 Separating and Uniting Variables\nWe will talk about strings and regular expressions next week, but there’s a task that is fairly commonly encountered with functions that belong to the tidyr package: separating variables into two different columns separate() and it’s complement, unite(), which is useful for combining two variables into one column.\n\nseparate_wider_delim() and separate_wider_position()unite()\n\n\n\n\n\n\n\nA visual representation of what separating variables means for data set operations.\n\n\n\n\n\ntable3 |&gt;\n  separate_wider_delim(cols        = rate,\n                       names       = c(\"cases\", \"population\"),\n                       delim       = \"/\",\n                       cols_remove = FALSE\n           )\n\n# A tibble: 6 × 5\n  country      year cases  population rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            \n1 Afghanistan  1999 745    19987071   745/19987071     \n2 Afghanistan  2000 2666   20595360   2666/20595360    \n3 Brazil       1999 37737  172006362  37737/172006362  \n4 Brazil       2000 80488  174504898  80488/174504898  \n5 China        1999 212258 1272915272 212258/1272915272\n6 China        2000 213766 1280428583 213766/1280428583\n\n\nI’ve left the rate column in the original data frame (cols_remove = F) just to make it easy to compare and verify that yes, it worked.\n\n\nAnd, of course, there is a complementary operation, which is when it’s necessary to join two columns to get a useable data value.\n\n\n\n\n\nA visual representation of what uniting variables means for data set operations.\n\n\n\n\n\ntable5 |&gt;\n  unite(col = \"year\",\n        c(century, year),\n        sep = ''\n        )\n\n# A tibble: 6 × 3\n  country     year  rate             \n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan 1999  745/19987071     \n2 Afghanistan 2000  2666/20595360    \n3 Brazil      1999  37737/172006362  \n4 Brazil      2000  80488/174504898  \n5 China       1999  212258/1272915272\n6 China       2000  213766/1280428583\n\n\n\n\n\n\n\nLearn More in R4DS\nThe separate_xxx() is actually a family of experimental functions stemming from the superseeded separate() function. You can read more about separate_xxx() and unite() in r4ds and r4ds.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "04-data-joins-and-transformations.html#merging-tables",
    "href": "04-data-joins-and-transformations.html#merging-tables",
    "title": "4  Data Joins and Transformations",
    "section": "4.5 Merging Tables",
    "text": "4.5 Merging Tables\nThe final essential data tidying and transformation skill you need to acquire is joining tables. It is common for data to be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.\n\n\nExamples: Relational Data Example: Primary School Records\n\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n \nWe could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment\nIt would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren’t well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.\n\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nkeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs\n\n\n4.5.1 Animating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\n\nMutating Joins\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins.\n\nInner JoinLeft JoinRight JoinFull Join\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\n\nDemonstration: Mutating Joins\n\n\nt1 &lt;- tibble(x = c(\"A\", \"B\", \"D\"), \n             y = c(1, 2, 3)\n             )\nt1\n\n# A tibble: 3 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A         1\n2 B         2\n3 D         3\n\nt2 &lt;- tibble(x = c(\"B\", \"C\", \"D\"), \n             z = c(2, 4, 5)\n             )\nt2\n\n# A tibble: 3 × 2\n  x         z\n  &lt;chr&gt; &lt;dbl&gt;\n1 B         2\n2 C         4\n3 D         5\n\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n\n# A tibble: 2 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 D         3     5\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\nleft_join(t1, t2)\n\n# A tibble: 3 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    NA\n2 B         2     2\n3 D         3     5\n\nleft_join(t2, t1)\n\n# A tibble: 3 × 3\n  x         z     y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 C         4    NA\n3 D         5     3\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n\n# A tibble: 3 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 D         3     5\n3 C        NA     4\n\nright_join(t2, t1)\n\n# A tibble: 3 × 3\n  x         z     y\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B         2     2\n2 D         5     3\n3 A        NA     1\n\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join.\n\nfull_join(t1, t2)\n\n# A tibble: 4 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    NA\n2 B         2     2\n3 D         3     5\n4 C        NA     4\n\n\n\n\n\n\nFiltering Joins\n\nSemi JoinAnti Join\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\n\n\n\n\nLearn More in R4DS\nRead more about joins in r4ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nLowndes, Julie, and Allison Horst. 2020. “Tidy Data for Efficiency, Reproducibility, and Collaboration.” Blog. Openscapes. https://www.openscapes.org/blog/2020/10/12/tidy-data//.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Joins and Transformations</span>"
    ]
  },
  {
    "objectID": "05-special-data-types.html",
    "href": "05-special-data-types.html",
    "title": "5  Special Data Types",
    "section": "",
    "text": "Objectives\nReading: 16 minute(s) at 200 WPM + r4ds required readings\nVideos: 32 minutes\nThis chapter is heavily outsourced to r4ds as they do a much better job at providing examples and covering the extensive functionality of each of the packages than I myself would ever be able to.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special Data Types</span>"
    ]
  },
  {
    "objectID": "05-special-data-types.html#ch5-objectives",
    "href": "05-special-data-types.html#ch5-objectives",
    "title": "5  Special Data Types",
    "section": "",
    "text": "Clean and extract information from character strings using stringr\nWork with date and time variables using lubridate",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special Data Types</span>"
    ]
  },
  {
    "objectID": "05-special-data-types.html#string-operations-with-stringr",
    "href": "05-special-data-types.html#string-operations-with-stringr",
    "title": "5  Special Data Types",
    "section": "5.1 String Operations with stringr",
    "text": "5.1 String Operations with stringr\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\n\n\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\n\nAlternately, the xkcd version of the above quote\n\n\n\n(required) Go read about strings in r4ds. \n\n\n\nstringr\nDownload the stringr cheatsheet.\n\nTable of string functions in the R stringr package. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\n\nTask\nstringr\n\n\n\nReplace pattern with replacement\nstr_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\n\nConvert case\nstr_to_lower(x), str_to_upper(x) , str_to_title(x)\n\n\nStrip whitespace from start/end\nstr_trim(x) , str_squish(x)\n\n\nPad strings to a specific length\nstr_pad(x, …)\n\n\nTest if the string contains a pattern\nstr_detect(x, pattern)\n\n\nCount how many times a pattern appears in the string\nstr_count(x, pattern)\n\n\nFind the first appearance of the pattern within the string\nstr_locate(x, pattern)\n\n\nFind all appearances of the pattern within the string\nstr_locate_all(x, pattern)\n\n\nDetect a match at the start/end of the string\nstr_starts(x, pattern) ,str_ends(x, pattern)\n\n\nSubset a string from index a to b\nstr_sub(x, a, b)\n\n\nConvert string encoding\nstr_conv(x, encoding)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.1 Regular Expressions\nMatching exact strings is easy - it’s just like using find and replace.\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears &lt;- str_extract(human_talk, \"walk\")\ndog_hears\n\n[1] \"walk\"\n\n\nBut, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler.\n\n\nShort Regular Expressions Primer\n\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions. The subset of regular expression syntax we’re going to cover here is fairly limited, but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn &lt;- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n\n[1] \"123-45-6789\"\n\n\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n* means repeat between 0 and inf times\n+ means 1 or more times\n? means 0 or 1 times – most useful when you’re looking for something optional\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times”\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n\n[1] \"banana\"\n\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n\n[1] \"ba\"\n\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n\n[[1]]\n[1] \"ba\" \"na\" \"na\"\n\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n\n[1] \"anan\"\n\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn &lt;- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n\n[1] \"123-45-6789\"\n\nphone &lt;- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n\n[1] \"123-456-7890\"\n\nnuid &lt;- str_extract(num_string, \"[0-9]{8}\")\nnuid\n\n[1] \"12345678\"\n\nbank_balance &lt;- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n\n[1] \"$50,000,000.23\"\n\n\nThere are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end)\n\n^ has multiple meanings:\n\nif it’s the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c”.\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\naddress &lt;- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num &lt;- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet &lt;- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet &lt;- str_remove(street, house_num) %&gt;% str_trim() # remove house number\n\ncity &lt;- str_extract(address, \",.*,\") %&gt;% str_remove_all(\",\") %&gt;% str_trim()\n\nzip &lt;- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\na|b will select a or b.\n\nIf you’ve captured information using (), you can reference that information using backreferences. In most languages, those look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants &lt;- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n\n[1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\n\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n\n[1] \"1234567980\" \"1234567890\" \"1234567890\"\n\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears &lt;- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n\n[[1]]\n[1] \"walk\"  \"treat\"\n\n\nPutting it all together, we can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\nstrings &lt;- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex &lt;- \"(walk|treat)\"\naddr_regex &lt;- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex &lt;- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n\n# A tibble: 6 × 5\n  text                                                   phone dog   addr  abab \n  &lt;chr&gt;                                                  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\n\n(semi-required) Go read more about regular expressions in r4ds.\nRead at least through section 17.4.1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special Data Types</span>"
    ]
  },
  {
    "objectID": "05-special-data-types.html#dates-times-with-lubridate",
    "href": "05-special-data-types.html#dates-times-with-lubridate",
    "title": "5  Special Data Types",
    "section": "5.2 Dates & Times with lubridate",
    "text": "5.2 Dates & Times with lubridate\nIn order to fill in an important part of our toolbox, we need to learn how to work with date variables. These variables feel like they should be simple and intuitive given we all work with schedules and calendars everyday. However, there are little nuances that we will learn to make working with dates and times easier.\n\n\n\n\n(Required) Go read about dates and times in r4ds. \n\n\n\n\nlubridate website\nDownload the lubridate cheatsheet\n\n\n\n\nLearn more about dates and times\n\nA more in-depth discussion of the POSIXlt and POSIXct data classes.\nA tutorial on lubridate - scroll down for details on intervals if you have trouble with %within% and %–%\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special Data Types</span>"
    ]
  },
  {
    "objectID": "06-version-control.html",
    "href": "06-version-control.html",
    "title": "6  Version Control",
    "section": "",
    "text": "Objectives\nReading: 5 minute(s) at 200 WPM\nVideos: 0 minute(s)\nMost of this section is either heavily inspired by Happy Git and Github for the UseR (Bryan, Hester, and The Stat 545 TAs 2021) or directly links to that book.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#ch6-objectives",
    "href": "06-version-control.html#ch6-objectives",
    "title": "6  Version Control",
    "section": "",
    "text": "Recognize the benefits of using version control to improve your coding practices and workflow.\nIdentify git/GitHub as a version control platform (and helper).\nRegister for a GitHub account so you can begin applying version control practices to your workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#what-is-version-control",
    "href": "06-version-control.html#what-is-version-control",
    "title": "6  Version Control",
    "section": "6.1 What is Version Control?",
    "text": "6.1 What is Version Control?\nVersion control is a system that (1) allows you to store your files in the cloud, (2) track change in those files over time, and (3) share your files with others.\n\nLearn more about version control\nIf you are unfamiliar with the idea of version control, this article describes what the principles of version control are.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#git",
    "href": "06-version-control.html#git",
    "title": "6  Version Control",
    "section": "6.2 Git",
    "text": "6.2 Git\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\n\nVersion control is the answer to this file naming problem.\n\n\nGit manages a collection of files in a structured way - rather like “track changes” in Microsoft Word or version history in Dropbox, but much more powerful.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R or _final_finalforreal.qmd to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this program is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with GitHub - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but GitHub is … essential.\n\n6.2.1 Git Basics\n\n\n\nIf that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything.\n\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently save over your old files, so long as you remember to commit frequently.\n\n\nEssential Reading: Git\nThe git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better.\nGo read Chapter 1, until it starts to become greek (aka over your head).\n\n\n\n\n\n\n\n\n\n\n Now that you have a general idea of how git works and why we might use it, let’s talk a bit about GitHub.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#setting-up-github",
    "href": "06-version-control.html#setting-up-github",
    "title": "6  Version Control",
    "section": "6.3 GitHub: Git on the Web",
    "text": "6.3 GitHub: Git on the Web\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git.\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don’t have in 3 different places is data you don’t care about.1\n\n\n\n\n\n\n\n\n\n\nSave your login information!\nMake sure you remember your username and password so you don’t have to try to hack into your own account during class this week.\nWrite your information down somewhere safe.\n\n\nOptional: Install a git client\nInstructions\n\nI personally like to use GitHub Desktop which allows me to interact with Git using a point-and-click interface.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#using-version-control-with-rstudio",
    "href": "06-version-control.html#using-version-control-with-rstudio",
    "title": "6  Version Control",
    "section": "6.4 Using Version Control (with RStudio)",
    "text": "6.4 Using Version Control (with RStudio)\nThis course will briefly introduced working with GitHub, but will not provide you with extensive practice using version control. By using version control, you will learn better habits for programming, and you’ll get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\nIn class this week, we will connect git/GitHub to RStudio so you can use version control for your code. We will then see what a typical git/GitHub workflow looks like.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#learn-more",
    "href": "06-version-control.html#learn-more",
    "title": "6  Version Control",
    "section": "Learn More",
    "text": "Learn More\n\nExtra Resources\n\nHappy Git and GitHub for the useR - Guide to using git, R, and RStudio together. (Bryan, Hester, and The Stat 545 TAs 2021)\nGit “Hello World” Tutorial on GitHub\nCrash course on git (30 minute YouTube video) (Traversy Media 2017)\nGit and GitHub for poets YouTube playlist (this is supposed to be the best introduction to Git out there…) (The Coding Train 2016)\nMore advanced git concepts, in comic form, by Erika Heidi (Erica Heidi 2020)\nA quick guide to the command line (Terminal) (Wei 2019)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "06-version-control.html#footnotes",
    "href": "06-version-control.html#footnotes",
    "title": "6  Version Control",
    "section": "",
    "text": "Yes, I’m aware that this sounds paranoid. It’s been a very rare occasion that I’ve needed to restore something from another backup. You don’t want to take chances.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Version Control</span>"
    ]
  },
  {
    "objectID": "07-functions.html",
    "href": "07-functions.html",
    "title": "7  Writing Functions",
    "section": "",
    "text": "Objectives\nReading: 18 minute(s) at 200 WPM\nVideos: 20 minute(s)\nA function is a set of actions that we group together and name. Throughout this course, you’ve used a bunch of different functions in R that are built into the language or added through packages: mean, ggplot, filter. In this chapter, we’ll be writing our own functions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#ch7-objectives",
    "href": "07-functions.html#ch7-objectives",
    "title": "7  Writing Functions",
    "section": "",
    "text": "Write your own functions in R\nMake good decisions about function arguments and returns\nInclude side effects and / or error messages in your functions\nExtend your use of good R coding style",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#when-to-write-a-function",
    "href": "07-functions.html#when-to-write-a-function",
    "title": "7  Writing Functions",
    "section": "7.1 When to write a function?",
    "text": "7.1 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead of copy pasting. The motivation behind this is the “don’t repeat yourself” (DRY) principle (Wickham and Grolemund 2016). There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\nLearn more in R4DS\nThere is some extensive material on this subject in R for Data Science on functions. If you want to really understand how functions work in R, that is a good place to go.\nIf you are interested in reading about “best practices”, I recommend reading Best Practices for Scientific Computing (Wilson et al. 2014).\n\n\n\nThis example is modified from R for Data Science (Wickham and Grolemund 2016, chap. 19).\nWhat does this code do? Does it work as intended?\n\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\nTo convert the code into a function, we first rewrite it using general names\nIn this case, it might help to replace df$a with x.\n\nx &lt;- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n\n [1] 0.01789197 1.00000000 0.47411065 0.86358785 0.79534337 0.31960237\n [7] 0.09209282 0.69526876 0.61377525 0.00000000\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng &lt;- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n\n [1] 0.01789197 1.00000000 0.47411065 0.86358785 0.79534337 0.31960237\n [7] 0.09209282 0.69526876 0.61377525 0.00000000\n\n\nFinally, we turn this code into a function\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n\n [1] 0.01789197 1.00000000 0.47411065 0.86358785 0.79534337 0.31960237\n [7] 0.09209282 0.69526876 0.61377525 0.00000000\n\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and }\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#syntax",
    "href": "07-functions.html#syntax",
    "title": "7  Writing Functions",
    "section": "7.2 Syntax",
    "text": "7.2 Syntax\n\n\n\nR syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted.\n\n\n\n\n\nIn R, functions are defined (or assigned names) the same as other variables, using &lt;-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#arguments-and-parameters",
    "href": "07-functions.html#arguments-and-parameters",
    "title": "7  Writing Functions",
    "section": "7.3 Arguments and Parameters",
    "text": "7.3 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a puppy’s name and returns “ is a good pup!”.\n\ndog &lt;- \"Eddie\"\n\ngoodpup &lt;- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n\n[1] \"Eddie is a good pup!\"\n\n\nIn this example R function, when we call goodpup(dog), dog is the argument. name is the parameter.\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\n\nExample\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be.\n\nFunctionAnswer\n\n\n\nmy_mean &lt;- function(x) {\n  censor_x &lt;- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nmy_mean(1:10)\n\n\n\n\nFunction name: my_mean\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: an average value for the censor_x numerical vector (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nmy_mean(1:10)\n\n[1] 6\n\n\n\n\n\n\n\n7.3.1 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\ndivide &lt;- function(x, y) {\n  x / y\n}\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\ndivide(3, 6)\n\n[1] 0.5\n\ndivide(x = 3, y = 6)\n\n[1] 0.5\n\ndivide(y = 6, x = 3)\n\n[1] 0.5\n\ndivide(6, 3)\n\n[1] 2\n\ndivide(x = 6, y = 3)\n\n[1] 2\n\ndivide(y = 3, x = 6)\n\n[1] 2\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#input-validation",
    "href": "07-functions.html#input-validation",
    "title": "7  Writing Functions",
    "section": "7.4 Input Validation",
    "text": "7.4 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() or if(){ } else{ } and then use stop(\"better error message\") in the body of the if or else statement.\n\nstopifnot()if(){ } else { }if(){stop()}\n\n\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n\nError in x + y: non-numeric argument to binary operator\n\nadd &lt;- function(x, y) {\n  stopifnot(is.numeric(x), \n            is.numeric(y)\n            )\n  x + y\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): is.numeric(x) is not TRUE\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nadd &lt;- function(x, y) {\n  if(is.numeric(x) & is.numeric(y)) {\n    x + y\n  } else {\n    stop(\"Argument input for x or y is not numeric\")\n  }\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): Argument input for x or y is not numeric\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nadd &lt;- function(x, y) {\n  if(!is.numeric(x) | !is.numeric(y)) {\n    stop(\"Argument input for x or y is not numeric\")\n  }\n    x + y\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): Argument input for x or y is not numeric\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused. If you’re interested, Wikipedia has more about defensive programming.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#scope",
    "href": "07-functions.html#scope",
    "title": "7  Writing Functions",
    "section": "7.5 Scope",
    "text": "7.5 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\n\n\n\n\n\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\n\n7.5.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\nWhat does this function return, 10 or 20?\n\nPseudocodeSketchR Function\n\n\na &lt;- 10\n\nmyfun &lt;- function() {\n  a &lt;- 20\n  a\n}\n\nmyfun()\n\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\n\n\n\n\na &lt;- 10\n\nmyfun &lt;- function() {\n  a &lt;- 20\n  a\n}\n\nmyfun()\n\n[1] 20\n\na\n\n[1] 10\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces. Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n\n7.5.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\nPseudocodeSketchR Function\n\n\nmyfun &lt;- function() {\n  if aa is not defined\n    aa &lt;- 1\n  else\n    aa &lt;- aa + 1\n}\n\nmyfun()\nmyfun()\n\nWhat does this output?\n\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\n\nmyfun &lt;- function() {\n  if (!exists(\"aa\")) {\n    aa &lt;- 1\n  } else {\n    aa &lt;- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n\n[1] 1\n\nmyfun()\n\n[1] 1\n\n\n\n\n\n\n\n\n7.5.3 Dynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\nPseudocodeSketchR\n\n\nmyfun &lt;- function(){\n  x + 1\n}\n\nx &lt;- 14\n\nmyfun()\n\nx &lt;- 20\n\nmyfun()\n\nWhat will the output be of this code?\n\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\n\nmyfun &lt;- function() {\n  x + 1\n}\n\nx &lt;- 14\n\nmyfun()\n\n[1] 15\n\nx &lt;- 20\n\nmyfun()\n\n[1] 21\n\n\n\n\n\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. (Taken from (Wickham 2015, chap. 6))\n\nCodeSolution\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n[1] 202\n\n\n\n\n\n\n\n\n\n7.5.4 Tutorial - Writing Functions\nVisitRStudio Primer - Writing Functions\nI highly recommend you work through every tutorial except Loops!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#debugging",
    "href": "07-functions.html#debugging",
    "title": "7  Writing Functions",
    "section": "7.6 Debugging",
    "text": "7.6 Debugging\nNow that you’re writing functions, it’s time to talk a bit about debugging techniques. This is a lifelong topic - as you become a more advanced programmer, you will need to develop more advanced debugging skills as well (because you’ll become more adept at screwing things up).\n\n\n\nThe faces of debugging (by Allison Horst)\n\n\nLet’s start with the basics: print debugging.\n\n7.6.1 Print Debugging\nThis technique is basically exactly what it sounds like. You insert a ton of print statements to give you an idea of what is happening at each step of the function.\nLet’s try it out on the previous example (see what I did there?)\n\nNote that I’ve modified the code slightly so that we store the value into returnval and then return it later - this allows us to see the code execution without calling functions twice (which would make the print output a bit more confusing).\n\nf &lt;- function(x) {\n  print (\"Entering Outer Function\")\n  print (paste(\"x =\", x))\n  f &lt;- function(x) {\n    print (\"Entering Middle Function\")\n    print (paste(\"x = \", x))\n    f &lt;- function() {\n      print (\"Entering Inner Function\")\n      print (paste(\"x = \", x))\n      print (paste(\"Inner Function: Returning\", x^2))\n      x ^ 2\n    }\n    returnval &lt;- f() + 1\n    print (paste(\"Middle Function: Returning\", returnval))\n    returnval\n  }\n  returnval &lt;- f(x) * 2\n  print (paste(\"Outer Function: Returning\", returnval))\n  returnval\n}\nf(10)\n\n[1] \"Entering Outer Function\"\n[1] \"x = 10\"\n[1] \"Entering Middle Function\"\n[1] \"x =  10\"\n[1] \"Entering Inner Function\"\n[1] \"x =  10\"\n[1] \"Inner Function: Returning 100\"\n[1] \"Middle Function: Returning 101\"\n[1] \"Outer Function: Returning 202\"\n\n\n[1] 202\n\n\n\n\n\n7.6.2 General Debugging Strategies\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nThe overall process is well described in Advanced R by H. Wickham1; I’ve copied it here because it’s such a succinct distillation of the process, but I’ve adapted some of the explanations to this class rather than the original context of package development.\n\nRealize that you have a bug\nGoogle! In R you can automate this with the errorist and searcher packages, but general Googling the error + the programming language + any packages you think are causing the issue is a good strategy.\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while.\nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation.\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed &lt;-- instead of &lt;- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck2. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. A more thorough explanation can be found at gitduck.com.\n\n\n\n\nYou may find it helpful to procure a rubber duck expert for each language you work in. I use color-your-own rubber ducks to endow my ducks with certain language expertise. Other people use plain rubber ducks and give them capes.\n\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known it has an xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea.\n\n\nThese next two sections are included as FYI, but you don’t have to read them just now. They’re important, but not urgent, if that makes sense.\n\n\nDividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solveable.\nWe’ll start with a non-programming example:\n\n\nGeneral problem statement : “I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\nSpecific problem statement: “I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\nSeparating things into solvable problems:\nMoving through the list above, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\nBrainstorm Solutions:\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\nApproach each sub-problem separately\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfulness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work.\n\n\nHere’s another example of how to break down a real-world personal problem in programming/debugging style.\nMinimal Working (or Reproducible) Examples\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\n\nSo, with that said, there are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\nminimal - as little code as possible to still reproduce the problem\ncomplete - everything necessary to reproduce the issue is contained in the description/question\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into an R script or notebook (e.g. Quarto document code chunk). Describe what you see and what you’d hope to see if the code were working.\nOther resources:\n\nreprex package: Do’s and Don’ts\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#styling-functions",
    "href": "07-functions.html#styling-functions",
    "title": "7  Writing Functions",
    "section": "7.7 Styling Functions",
    "text": "7.7 Styling Functions\nPart of writing reproducible and shareable code is following good style guidelines. Mostly, this means choosing good object names and using white space in a consistent and clear way.\nYou should have already seen the sections of the Tidyverse Style Guide relevant to piping, plotting, and naming objects. This week we are extending these style guides to functions.\n\nI would highly recommend reading through the style guide for naming functions, what to do with long lines, and the use of comments.\nRead the tidyverse style guide for functions.\n\nIn summary, designing functions is somewhat subjective, but there are a few principles that apply:\n\nChoose a good, descriptive names\n\n\nYour function name should describe what it does, and usually involves a verb.\nYour argument names should be simple and / or descriptive.\nNames of variables in the body of the function should be descriptive.\n\n\nOutput should be very predictable\n\n\nYour function should always return the same object type, no matter what input it gets.\nYour function should expect certain objects or object types as input, and give errors when it does not get them.\nYour function should give errors or warnings for common mistakes.\nDefault values of arguments should only be used when there is a clear common choice.\n\n\nThe body of the function should be easy to read.\n\n\nCode should use good style principles.\nThere should be occasional comments to explain the purpose of the steps.\nComplicated steps, or steps that are repeated many times, should be written into separate functions (sometimes called helper functions).\n\n\nFunctions should be self-contained.\n\n\nThey should not rely on any information besides what is given as input.\n(Relying on other functions is fine, though)\nThey should not alter the Global Environment\nFunctions should never load or install packages!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nWickham, H. 2015. Advanced R. Chapman & Hall/CRC The R Series. CRC Press. https://books.google.com/books?id=FfsYCwAAQBAJ.\n\n\nWickham, H., and G. Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. https://books.google.com/books?id=vfi3DQAAQBAJ.\n\n\nWilson, Greg, Dhavide A Aruliah, C Titus Brown, Neil P Chue Hong, Matt Davis, Richard T Guy, Steven HD Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLoS Biology 12 (1): e1001745.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "07-functions.html#footnotes",
    "href": "07-functions.html#footnotes",
    "title": "7  Writing Functions",
    "section": "",
    "text": "the 0th step is from the 1st edition, the remaining steps are from the 2nd.↩︎\nSome people use cats, but I find that they don’t meet the nonjudgmental criteria. Of course, they’re equally judgmental whether your code works or not, so maybe that works if you’re a cat person, which I am not. Dogs, in my experience, can work, but often will try to comfort you when they realize you’re upset, which both helps and lessens your motivation to fix the problem. A rubber duck is the perfect dispassionate listener.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html",
    "href": "08-functional-programming.html",
    "title": "8  Functional Programming",
    "section": "",
    "text": "Objectives\nReading: 19 minute(s) at 200 WPM\nVideos: 29 minute(s)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html#ch8-objectives",
    "href": "08-functional-programming.html#ch8-objectives",
    "title": "8  Functional Programming",
    "section": "",
    "text": "Use functional programming techniques to create code which is well organized and easier to understand and maintain",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html#introduction-to-iteration",
    "href": "08-functional-programming.html#introduction-to-iteration",
    "title": "8  Functional Programming",
    "section": "8.1 Introduction to Iteration",
    "text": "8.1 Introduction to Iteration\nWe just learned the rule of “don’t repeat yourself more than two times” and to instead automate our procedures with functions in order to remove duplication of code. We have used tools such as across() to help eliminate this copy-paste procedure even further. This is a form of iteration in programming as across() “iterates” over variables, applying a function to manipulate each variable and then doing the same for the next variable.\n\n\n\n\n\n\nwhile() and for() loops are a common form of iteration that can be extremely useful when logically thinking through a problem, however are extremely computationally intensive. Therefore, loops will not be the focus of this chapter. If you are interested, you can go read about loops in the pre-reading material of this text.\n\nRead more\nYou can read all about iteration in the previous version of R4DS.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html#review-of-lists-and-vectors",
    "href": "08-functional-programming.html#review-of-lists-and-vectors",
    "title": "8  Functional Programming",
    "section": "8.2 Review of Lists and Vectors",
    "text": "8.2 Review of Lists and Vectors\nIn the pre-reading, we introduce the different data structures we have worked with in R. We are going to do a review of some of the important data structures for this chapter.\nA vector is a 1-dimensional data structure that contains items of the same simple (‘atomic’) type (character, logical, integer, factor).\n\n(logical_vec &lt;- c(T, F, T, T))\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n(numeric_vec &lt;- c(3, 1, 4, 5))\n\n[1] 3 1 4 5\n\n(char_vec &lt;- c(\"A\", \"AB\", \"ABC\", \"ABCD\"))\n\n[1] \"A\"    \"AB\"   \"ABC\"  \"ABCD\"\n\n\nYou index a vector using brackets: to get the \\(i\\)th element of the vector x, you would use x[i] in R or x[i-1] in python (Remember, python is 0-indexed, so the first element of the vector is at location 0).\n\nlogical_vec[3]\n\n[1] TRUE\n\nnumeric_vec[3]\n\n[1] 4\n\nchar_vec[3]\n\n[1] \"ABC\"\n\n\nYou can also index a vector using a logical vector:\n\nnumeric_vec[logical_vec]\n\n[1] 3 4 5\n\nchar_vec[logical_vec]\n\n[1] \"A\"    \"ABC\"  \"ABCD\"\n\nlogical_vec[logical_vec]\n\n[1] TRUE TRUE TRUE\n\n\nA list is a 1-dimensional data structure that has no restrictions on what type of content is stored within it. A list is a “vector”, but it is not an atomic vector - that is, it does not necessarily contain things that are all the same type.\n\n(\n  mylist &lt;- list(\n    logical_vec, \n    numeric_vec, \n    third_thing = char_vec[1:2]\n  )\n)\n\n[[1]]\n[1]  TRUE FALSE  TRUE  TRUE\n\n[[2]]\n[1] 3 1 4 5\n\n$third_thing\n[1] \"A\"  \"AB\"\n\n\nList components may have names (or not), be homogeneous (or not), have the same length (or not).\n\n8.2.1 Indexing\nIndexing necessarily differs between R and python, and since the list types are also somewhat different (e.g. lists cannot be named in python), we will treat list indexing in the two languages separately.\n\n\n\n\n\n\n\n\nAn unusual pepper shaker which we’ll call pepper\n\n\n\n\n\n\n\nWhen a list is indexed with single brackets, pepper[1], the return value is always a list containing the selected element(s).\n\n\n\n\n\n\n\nWhen a list is indexed with double brackets, pepper[[1]], the return value is the selected element.\n\n\n\n\n\n\n\nTo actually access the pepper, we have to use double indexing and index both the list object and the sub-object, as in pepper[[1]][[1]].\n\n\n\n\n\n\nFigure 8.1: The types of indexing are made most memorable with a fantastic visual example from Grolemund and Wickham (2017), which I have repeated here.\n\n\n\nThere are 3 ways to index a list:\n\nWith single square brackets, just like we index atomic vectors. In this case, the return value is always a list.\n\n\nmylist[1]\n\n[[1]]\n[1]  TRUE FALSE  TRUE  TRUE\n\nmylist[2]\n\n[[1]]\n[1] 3 1 4 5\n\nmylist[c(T, F, T)]\n\n[[1]]\n[1]  TRUE FALSE  TRUE  TRUE\n\n$third_thing\n[1] \"A\"  \"AB\"\n\n\n\nWith double square brackets. In this case, the return value is the thing inside the specified position in the list, but you also can only get one entry in the main list at a time. You can also get things by name.\n\n\nmylist[[1]]\n\n[1]  TRUE FALSE  TRUE  TRUE\n\nmylist[[\"third_thing\"]]\n\n[1] \"A\"  \"AB\"\n\n\n\nUsing x$name. This is equivalent to using x[[\"name\"]]. Note that this does not work on unnamed entries in the list.\n\n\nmylist$third_thing\n\n[1] \"A\"  \"AB\"\n\n\nTo access the contents of a list object, we have to use double-indexing:\n\nmylist[[\"third_thing\"]][[1]]\n\n[1] \"A\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can get a more thorough review of vectors and lists from Jenny Bryan’s purrr tutorial introduction (Bryan n.d.).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html#vectorized-operations",
    "href": "08-functional-programming.html#vectorized-operations",
    "title": "8  Functional Programming",
    "section": "8.3 Vectorized Operations",
    "text": "8.3 Vectorized Operations\nOperations in R are (usually) vectorized - that is, by default, they operate on vectors. This is primarily a feature that applies to atomic vectors (and we don’t even think about it):\n\n\n\n\n(rnorm(10) + rnorm(10, mean = 3))\n\n [1]  1.10966844 -0.02203546  1.11826854  1.20485562  5.63590046  3.59198218\n [7]  3.28814951  1.57984223  0.99012104  5.74752617\n\n\nWith vectorized functions, we don’t have to use a for loop to add these two vectors with 10 entries each together. In languages which don’t have implicit support for vectorized computations, this might instead look like:\n\na &lt;- rnorm(10)\nb &lt;- rnorm(10, mean = 3)\n\nresult &lt;- rep(0, 10)\nfor (i in 1:10) {\n  result[i] &lt;- a[i] + b[i]\n}\n\nresult\n\n [1] 3.9540523 2.9006172 3.2878693 3.0098338 5.4685053 3.4746667 5.3620653\n [8] 4.1150999 3.7628398 0.5682941\n\n\nThat is, we would apply or map the + function to each entry of a and b. For atomic vectors, it’s easy to do this by default; with a list, however, we need to be a bit more explicit (because everything that’s passed into the function may not be the same type).\nI find the purrr package easier to work with, so we won’t be working with the base functions (the apply family) in this course. You can find a side-by-side comparison in the purrr tutorial.\n\nYou can also watch Dr. Theobold’s video to learn more:\n\n\n\n\n\nThe R package purrr (and similar base functions apply, lapply, sapply, tapply, and mapply) are based on extending “vectorized” functions to a wider variety of vector-like structures.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html#functional-programming",
    "href": "08-functional-programming.html#functional-programming",
    "title": "8  Functional Programming",
    "section": "8.4 Functional Programming",
    "text": "8.4 Functional Programming\nThe concept of functional programming is a bit hard to define rigorously at the level we’re working at, but generally, functional programming is concerned with pure functions: functions that have an input value that determines the output value and create no other side effects.\nWhat this means is that you describe every step of the computation using a function, and chain the functions together. At the end of the computations, you might save the program’s results to an object, but (in general), the goal is to not change things outside of the “pipeline” along the way.\nThis has some advantages:\n\nEasier parallelization\n\n“Side effects” generally make it hard to parallelize code because e.g. you have to update stored objects in memory, which is hard to do with multiple threads accessing the same memory.\n\nFunctional programming tends to be easier to read\n\nYou can see output and input and don’t have to work as hard to keep track of what is stored where .\n\nEasier Debugging\n\nYou can examine the input and output at each stage to isolate which function is introducing the problem.\n\n\nThe introduction of the pipe in R has made chaining functions together in a functional programming-style pipeline much easier. purrr is just another step in this process: by making it easy to apply functions to lists of things (or to use multiple lists of things in a single function), purrr makes it easier to write clean, understandable, debuggable code.\n\n\nFunctional Programming Example\nThis example is modified from the motivation section of the Functional Programming chapter in Advanced R (Wickham 2019).\n\nProblemNaive ApproachWriting a functionMapping a function\n\n\nSuppose we want to replace every -99 in the following sample dataset with an NA. (-99 is sometimes used to indicate missingness in datasets).\n\n# Generate a sample dataset\nset.seed(1014)\ndf &lt;- data.frame(replicate(6, sample(c(1:10, -99), 6, rep = TRUE)))\nnames(df) &lt;- letters[1:6]\ndf\n\n  a   b   c   d  e f\n1 7   5 -99   2  5 2\n2 5   5   5   3  6 1\n3 6   8   5   9  9 4\n4 4   2   2   6  6 8\n5 6   7   6 -99 10 6\n6 9 -99   4   7  5 1\n\n\n\n\nThe “beginner” approach is to just replace each individual -99 with an NA:\n\ndf1 &lt;- df\ndf1[6,2] &lt;- NA\ndf1[1,3] &lt;- NA\ndf1[5,4] &lt;- NA\n\ndf1\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis is tedious, and painful, and won’t work if we have a slightly different dataset where the -99s are in different places. So instead, we might consider being a bit more general:\n\ndf2 &lt;- df\ndf2$a[df2$a == -99] &lt;- NA\ndf2$b[df2$b == -99] &lt;- NA\ndf2$c[df2$c == -99] &lt;- NA\ndf2$d[df2$d == -99] &lt;- NA\ndf2$e[df2$e == -99] &lt;- NA\ndf2$f[df2$f == -99] &lt;- NA\ndf2\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis requires a few more lines of code, but is able to handle any data frame with 6 columns a - f. It also requires a lot of copy-paste and can leave you vulnerable to making mistakes.\n\n\nThe standard rule is that if you copy-paste the same code 3x, then you should write a function, so let’s try that instead:\n\nfix_missing &lt;- function(x, missing = -99){\n  x[x == missing] &lt;- NA\n  x\n}\n\ndf3 &lt;- df\ndf3$a &lt;- fix_missing(df$a)\ndf3$b &lt;- fix_missing(df$b)\ndf3$c &lt;- fix_missing(df$c)\ndf3$d &lt;- fix_missing(df$d)\ndf3$e &lt;- fix_missing(df$e)\ndf3$f &lt;- fix_missing(df$f)\ndf3\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis still requires a lot of copy-paste, and doesn’t actually make the code more readable. We can more easily change the missing value, though, which is a bonus.\n\n\nWe have a function that we want to apply or map to every column in our data frame. We could use a for() loop (doing this for demonstrative purposes only, I expect you to use more efficient tools in class):\n\nfix_missing &lt;- function(x, missing = -99){\n  x[x == missing] &lt;- NA\n  x\n}\n\ndf4 &lt;- df\nfor (i in 1:ncol(df)) {\n  df4[,i] &lt;- fix_missing(df4[,i])\n}\ndf4\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis is more understandable and flexible than the previous function approach as well as the naive approach - we don’t need to know the names of the columns in our data frame, or even how many there are. It is still quite a few lines of code, though.\nIterating through a list (or columns of a data frame) is a very common task, so R has a shorthand function for it. You could us lapply from base R, but we will be learning the map family of functions from the purrr package.\n\nfix_missing &lt;- function(x, missing = -99){\n  x[x == missing] &lt;- NA\n  x\n}\n\ndf5 &lt;- df\ndf5 &lt;- map_dfc(df5, fix_missing)\ndf5\n\n# A tibble: 6 × 6\n      a     b     c     d     e     f\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     7     5    NA     2     5     2\n2     5     5     5     3     6     1\n3     6     8     5     9     9     4\n4     4     2     2     6     6     8\n5     6     7     6    NA    10     6\n6     9    NA     4     7     5     1\n\n\nBy default, map returns a list (see below), but we can use map_dfc to return a data frame created by binding the columns together.\n\n\nmap() - returns a list\n\n\ndf6 &lt;- df\nmap(df6, fix_missing)\n\n$a\n[1] 7 5 6 4 6 9\n\n$b\n[1]  5  5  8  2  7 NA\n\n$c\n[1] NA  5  5  2  6  4\n\n$d\n[1]  2  3  9  6 NA  7\n\n$e\n[1]  5  6  9  6 10  5\n\n$f\n[1] 2 1 4 8 6 1\n\n\n\nWe’ve replaced 6 lines of code that only worked for 6 columns named a - f with a single line of code that works for any data frame with any number of rows and columns, so long as -99 indicates missing data. In addition to being shorter, this code is also somewhat easier to read and much less vulnerable to typos.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "08-functional-programming.html#introduction-to-map",
    "href": "08-functional-programming.html#introduction-to-map",
    "title": "8  Functional Programming",
    "section": "8.5 Introduction to map()",
    "text": "8.5 Introduction to map()\n\npurrr is a part of the tidyverse, so you should already have the package installed. When you load the tidyverse with library(), this also loads purrr.\n\ninstall.packages(\"purrr\")\nlibrary(purrr)\n\nDownload the purrr cheatsheet.\n\n\n\n (REQUIRED) Please read Sections 21.5 through 21.7 R for Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn More About Purrr\n\nThe Joy of Functional Programming (for Data Science): Hadley Wickham’s talk on purrr and functional programming. ~1h video and slides.\n(The Joy of Cooking meets Data Science, with illustrations by Allison Horst)\nPirating Web Content Responsibly with R and purrr (a blog post in honor of international talk like a pirate day) (Rudis 2017)\nHappy R Development with purrr\nWeb mining with purrr\nText Wrangling with purrr\nSetting NAs with purrr (uses the naniar package)\nMappers with purrr - handy ways to make your code simpler if you’re reusing functions a lot.\nFunction factories - code optimization with purrr\nStats and Machine Learning examples with purrr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nBryan, Jennifer. n.d. “Lessons and Examples.” Purrr Tutorial. Accessed November 14, 2022. https://jennybc.github.io/purrr-tutorial/index.html.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. 1st ed. O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nRudis, Bob. 2017. “Pirating Web Content Responsibly With R.” Rud.is. https://rud.is/b/2017/09/19/pirating-web-content-responsibly-with-r/.\n\n\nWickham, Hadley. 2019. “Functional Programming.” In Advanced R, 2nd ed. The R Series. Chapman; Hall/CRC. http://adv-r.had.co.nz/Functional-programming.html.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html",
    "href": "09-statistical-modeling-and-simulation.html",
    "title": "9  Simulating Distributions",
    "section": "",
    "text": "Objectives\nReading: 7 minute(s) at 200 WPM\nVideos: 48 minutes\nThis chapter is heavily from Dr. Theobold’s course-page material.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating Distributions</span>"
    ]
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#simulation",
    "href": "09-statistical-modeling-and-simulation.html#simulation",
    "title": "9  Simulating Distributions",
    "section": "9.1 Simulation",
    "text": "9.1 Simulation\nIn statistics, we often want to simulate data (or create fake data) for a variety of purposes. For example, in your first statistics course, you may have flipped coins to “simulate” a 50-50 chance. In this section, we will learn how to simulate data from statistical distributions using R.\n\n\n\n\nRequired Reading – R Programming for Data Science : Simulation\n\n\n9.1.1 Setting a Random Number Seed\nFunctions like rnorm() rely on something called pseudo-randomness. Because computers can never be truly random, complicated processes are implemented to make “random” number generation be so unpredictable as to behave like true randomness.\nThis means that projects involving simulation are harder to make reproducible. For example, here are two identical lines of code that give different results!\n\nrnorm(1, mean = 0, sd = 1)\n\n[1] -0.05323655\n\n\n\nrnorm(1, mean = 0, sd = 1)\n\n[1] 0.7589479\n\n\nFortunately, pseudo-randomness depends on a seed, which is an arbitrary number where the randomizing process starts. Normally, R will choose the seed for you, from a pre-generated vector:\n\nhead(.Random.seed)\n\n[1]       10403           4 -1914082434  1084320056   283215387  1017038713\n\n\nHowever, you can also choose your own seed using the set.seed() function. This guarantees your results will be consistent across runs (and hopefully computers):\n\nset.seed(1234)\nrnorm(1, mean = 0, sd = 1)\n\n[1] -1.207066\n\n\n\nset.seed(1234)\nrnorm(1, mean = 0, sd = 1)\n\n[1] -1.207066\n\n\nOf course, it doesn’t mean the results will be the same in every subsequent run if you forget or reset the seed in between each line of code!\n\nset.seed(1234)\nrnorm(1, mean = 0, sd = 1)\n\n[1] -1.207066\n\n## Calling rnorm() again without a seed \"resets\" the seed! \nrnorm(1, mean = 0, sd = 1)\n\n[1] 0.2774292\n\n\nIt is very important to always set a seed at the beginning of a Quarto document that contains any random steps, so that your rendered results are consistent.\n\nNote, though, that this only guarantees your rendered results will be the same if the code has not changed.\nChanging up any part of the code will re-randomize everything that comes after it!\n\nWhen writing up a report which includes results from a random generation process, in order to ensure reproducibility in your document, use `r ` to include your output within your written description with inline code.\n\n\n\nReproducibility: inline code example\n\n\nmy_rand &lt;- rnorm(1, mean = 0, sd = 1)\nmy_rand\n\n[1] 1.084441\n\n\nUsing r my_rand will display the result within my text:\nMy random number is 1.0844412.\nAlternatively, you could have put the rnorm code directly into the inline text r rnorm(1, mean = 0, sd = 1), but this can get messy if you have a result that requires a larger chunk of code.\n\n\n\n9.1.2 Plotting Density Distributions\nThe code below creates a tibble (read fancy data frame) of 100 heights randomly simulated (read drawn) from a normal distribution with a mean of 67 and standard deviation of 3.\n\nset.seed(93401)\nmy_samples &lt;- tibble(height = rnorm(n    = 100, \n                                    mean = 67, \n                                    sd   = 3)\n                     )\nmy_samples |&gt; \n  head()\n\n# A tibble: 6 × 1\n  height\n   &lt;dbl&gt;\n1   63.1\n2   66.7\n3   68.2\n4   63.4\n5   68.9\n6   71.4\n\n\nTo visualize the simulated heights, we can look at the density of the values. We plot the simulated values using geom_histogram() and define the local \\(y\\) aesthetic to plot calculate and plot the density of these values. We can then overlay the normal distribution curve (theoretical equation) with our specified mean and standard deviation using dnorm within stat_function()\n\nmy_samples |&gt; \n  ggplot(aes(x = height)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 1.75, \n                 fill = \"grey\"\n                 ) +\n  stat_function(fun = ~ dnorm(.x, mean = 67, sd = 3), \n                col = \"cornflowerblue\", \n                lwd = 2\n                ) + \n  xlim(c(55, 80))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating Distributions</span>"
    ]
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#linear-regression",
    "href": "09-statistical-modeling-and-simulation.html#linear-regression",
    "title": "9  Simulating Distributions",
    "section": "9.2 Linear Regression",
    "text": "9.2 Linear Regression\nYou now have the skills to import, wrangle, and visualize data. All of these tools help us prepare our data for statistical modeling. While we have sprinkled some formal statistical analyses throughout the course, in this section we will be formally reviewing Linear Regression. First let’s review simple linear regression. Linear regression models the linear relationship between two quantitative variables.\n\n\n\n\n\n\nReview of Simple Linear Regression and Conditions\nRecommended Reading – Modern Dive : Basic Regression\nHandy function shown in the reading! skim from the skimr package.\n\n\n9.2.1 Linear Regression in R\nTo demonstrate linear regression in R, we will be working with the penguins data set.\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\nWhen conducting linear regression with tools in R, we often want to visualize the relationship between the two quantitative variables of interest with a scatterplot. We can then use either geom_smooth(method = \"lm\") (or equivalently stat_smooth(method = \"lm\") to add a line of best fit (“regression line”) based on the ordinary least squares (OLS) equation to our scatter plot. The regression line is shown in a default blue line with the standard error uncertainty displayed in a gray transparent band (use se = FALSE to hide the standard error uncertainty band). These visual aesthetics can be changed just as any other plot aesthetics.\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, \n             y = bill_length_mm\n             )\n         ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"Bill Depth (mm)\",\n       subtitle = \"Bill Length (mm)\",\n       title = \"Relationship between penguin bill length and depth\"\n       ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\n\n\n\n\n\nBe careful of “overplotting” and use geom_jitter() instead of geom_point() if your data set is dense. This is strictly a data visualization tool and will not alter the original values.\n\nIn simple linear regression, we can define the linear relationship with a mathematical equation given by:\n\\[y = a + b\\cdot x\\]\n\n\n\nRemember \\(y = m\\cdot x+b\\) from eighth grade?!\n\nwhere\n\n\\(y\\) are the values of the response variable,\n\\(x\\) are the values of the explanatory/predictor variable,\n\\(a\\) is the \\(y\\)-intercept (average value of \\(y\\) when \\(x = 0\\)), and\n\\(b\\) is the slope coefficient (for every 1 unit increase in \\(x\\), the average of \\(y\\) increases by b)\n\n\n\n\nRemember “rise over run”!\n\nIn statistics, we use slightly different notation to denote this relationship with the estimated linear regression equation:\n\\[\\hat y = b_0 + b_1\\cdot x.\\]\nNote that the “hat” symbol above our response variable indicates this is an “estimated” value (or our best guess).\nWe can “fit” the linear regression equation with the lm function in R. The formula argument is denoted as y ~ x where the left hand side (LHS) is our response variable and the right hand side (RHS) contains our explanatory/predictor variable(s). We indicate the data set with the data argument and therefore use the variable names (as opposed to vectors) when defining our formula. We name (my_model) and save our fitted model just as we would any other R object.\n\nmy_model &lt;- lm(bill_length_mm ~ bill_depth_mm, \n               data = penguins\n               )\n\nNow that we have fit our linear regression, we might be wondering how we actually get the information out of our model. What are the y-intercept and slope coefficient estimates? What is my residual? How good was the fit? The code options below help us obtain this information.\n\nRaw CoefficientsModel SummaryTidy Model Summary\n\n\nThis is what is output when you just call the name of the linear model object you created (my_model). Notice, the output doesn’t give you much information and it looks kind of bad.\n\nmy_model\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = penguins)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\n\n\nThis is what is output when you use the summary() function on a linear model object. Notice, the output gives you a lot of information, some of which is really not that useful. And, the output is quite messy!\n\nsummary(my_model)\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8949  -3.9042  -0.3772   3.6800  15.5798 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    55.0674     2.5160  21.887  &lt; 2e-16 ***\nbill_depth_mm  -0.6498     0.1457  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.314 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\n\n\nThe tidy() function from the {broom} package takes a linear model object and puts the “important” information into a tidy tibble output.\nAh! Just right!\n\nlibrary(broom)\ntidy(my_model) \n\n# A tibble: 2 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     55.1       2.52      21.9  6.91e-67\n2 bill_depth_mm   -0.650     0.146     -4.46 1.12e- 5\n\n\nIf you are sad that you no longer have the statistics about the model fit (e.g., R-squared, adjusted R-squared, \\(\\sigma\\)), you can use the glance() function from the broom package to grab those!\n\nbroom::glance(my_model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0552        0.0525  5.31      19.9 0.0000112     1 -1056. 2117. 2129.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Simulating Distributions</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html",
    "href": "10-predictive-checks.html",
    "title": "10  Predictive Checks",
    "section": "",
    "text": "Objectives\nReading: 11 minute(s) at 200 WPM\nVideos: NA\nThis chapter is heavily from Dr. Theobold’s course-page material.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#ch10-objectives",
    "href": "10-predictive-checks.html#ch10-objectives",
    "title": "10  Predictive Checks",
    "section": "",
    "text": "Make predictions from a linear model\nInclude variability into predictions\nUnderstand what is assumed about the data generating process in a linear regression\nAssess if the assumed linear model accurately describes the observed data\n\n\n\n\n\n\n\n\n\nIt is difficult to include in a [statistical model] all of one’s knowledge about a problem, and so it is wise to investigate what aspects of reality are not captured by the model.\n\nAndrew Gelman, Bayesian Data Analysis",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#model-checking",
    "href": "10-predictive-checks.html#model-checking",
    "title": "10  Predictive Checks",
    "section": "10.1 Model Checking",
    "text": "10.1 Model Checking\nIn explanatory modeling, it is incredibly important to check if you can make sound inferences from the model that was fit. Whether the model is a “simple” difference in means or a more “complex” multiple linear regression, the estimated coefficients obtained from the model should not be used for inference if the conditions of the model are violated. Typically, model checking for a linear regression is taught through a “residual analysis,” inspecting different visualizations of the residuals from the model. This week, however, we will learn a different way to assess our regression model.\n\n10.1.1 Predictive Checks, Bayesian Modeling, and Subjectivity\nThe idea of predictive checks for a statistical model are most often seen in the context of Bayesian modeling. In Bayesian modeling, rather than obtaining a single estimate for a parameter, you obtain a distribution of plausible values for that parameter. The distribution suggests which values of that parameter are more or less likely, given what was seen in the data and the prior knowledge incorporated into the model.\nThe prior knowledge about what values we expect for the parameter comes in the form of a prior distribution. Different statisticians may choose a different prior distributions, much like different statisticians might choose different variables to include in their regression. Although there are large similarities between the choice of a prior distribution and the choice of a statistical model, prior distributions receive a great deal of flack for how the interlace “subjectivity” into the statistical modeling process.\nDue in part to these criticisms regarding the choice of a prior distribution, it is common in Bayesian analyses to perform predictive checks to assess the fit of the model to the data at hand. While we are not fitting Bayesian models, I believe there is a great deal we can learn from the concept of using predictive checks for any statistical model. Any good analysis should include a check of the “adequacy of the fit of the model to the data and the plausibility of the model for the purposes for which the model will be used” (Gelman 2014).\n\n\n10.1.2 Predictive Checks in a Linear Regression\nUnder a simple linear regression model, we assume that the responses can be modeled as a function of the explanatory variable and some error:\n\\[y = \\beta_0 + \\beta_1 \\cdot x_1 + \\epsilon\\]\nIn a linear model we assume that the errors (\\(\\epsilon\\)) follow a Normal distribution with mean 0 and standard deviation (\\(\\sigma\\)) – \\(\\epsilon \\sim N(0,\\sigma)\\).\nThis implies that we can simulate data that we would expect to come from this model, by adding normally distributed errors to the values predicted from the linear regression. Moreover, when we randomly generate these errors, we better understand what the “normality,” “equal variance,” and “independence” conditions mean in the context of the linear regression model.\n\n\n10.1.3 Connection to Model Conditions\nA linear regression model has four conditions, which follow the LINE acronym:\nLinear relationship between \\(x\\) and \\(y\\) Independence of observations Normality of residuals Equal variance of residuals\nThe first condition relates to the relationship between the explanatory (\\(x\\)) and response (\\(y\\)) variable. As you might expect, using a line to model a non-linear relationship is a bad choice! Thus, we will only make predictions from a linear regression where there appears to be a linear relationship between the variables.\nThe process of adding normally distributed errors to the predictions from the linear regression assumes three things:\n\nthe errors follow a specific distribution, namely \\(N(0,\\sigma)\\)\nthe errors all have the same variance (\\(\\sigma\\))\nthe errors are not related to each other\n\nThe Normality and equal variance conditions are manifested in using the \\(N(0,\\sigma)\\) distribution to generate the errors. The independence condition is manifested in the assumption that we can draw random errors for each observation (prediction), because the errors are not related to each other.\nThis might feel like a bit of a stretch, especially if you haven’t seen linear regression in a bit, so let me break this down a bit more. When we assume that observations are independent, we are saying that we shouldn’t be able to know the \\(y\\) value for one observation just from knowing the \\(y\\) value of another observation. If I have repeated observations, like values for the same country across multiple years, then it’s pretty likely that I can guess the value of a variable in 1991 from knowing the same variable’s value in 1990. Since we are using linear regression to model the relationship between the explanatory variable(s) and the response, this assumption can be rephrased in terms of each observation’s residual. Specifically, knowing the residual for one observation shouldn’t give us perfect information about the residual for another observation. Because we are assuming that the observations / residuals are independent, we can randomly draw an error for each observation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#performing-a-predicitive-check-for-a-linear-regression",
    "href": "10-predictive-checks.html#performing-a-predicitive-check-for-a-linear-regression",
    "title": "10  Predictive Checks",
    "section": "10.2 Performing a Predicitive Check for a Linear Regression",
    "text": "10.2 Performing a Predicitive Check for a Linear Regression\nI will be walking through how to carry out the process of performing a predictive check for a simple linear regression model in the context of the penguins data set from the palmerpenguins package.\n\nlibrary(palmerpenguins)\ndata(penguins)\n\nWe will start by fitting a simple linear regression model, modeling a penguin’s body_mass_g as a function of their flipper_length_mm.\n\npenguins |&gt; \n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"Flipper Length (mm)\", \n       y = \"\", \n       title = \"Relationship between Flipper Length (mm) and Body Mass (g)\", \n       subtitle = \"for Penguins on Palmer Archipelago\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\npenguin_lm &lt;- lm(body_mass_g ~ flipper_length_mm, data  = penguins)\n\n\n10.2.1 Obtaining Predictions predict()\nThe next step is to obtain the predictions (.fitted values) from the regression model. There is a handy built-in function for extracting the predicted values from a lm object – the predict() function.\n\nRecall we also have the augment() function that will provide us with the .fitted values – this is analogous to the predictions from predict().\n\n\npenguin_predict &lt;- predict(penguin_lm)\nhead(penguin_predict)\n\n       1        2        3        5        6        7 \n3212.256 3460.684 3907.854 3808.483 3659.426 3212.256 \n\n\n\n\n\n\n\n\n\n\n\n\n10.2.2 Extracting the Estimate of \\(\\sigma\\) sigma()\nThe residual standard error shown in the summary() output of a linear regression is the “best” estimate for the value of \\(\\sigma\\). We can extract this value using the built-in sigma() function and save it in an object for later use.\n\nsummary(penguin_lm)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1058.80  -259.27   -26.88   247.33  1288.69 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5780.831    305.815  -18.90   &lt;2e-16 ***\nflipper_length_mm    49.686      1.518   32.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.3 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.759, Adjusted R-squared:  0.7583 \nF-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\npenguin_sigma &lt;- sigma(penguin_lm)\npenguin_sigma\n\n[1] 394.2782\n\n\n\n\n10.2.3 Adding Errors / Noise to Predictions rnorm()\nAs stated before, under a linear regression model, we can simulate data that we would have expected to come from this model, by adding normally distributed errors to the values predicted from the linear regression.\nThus, it is useful for us to write a function that adds Normally distributed errors to a vector, x, given the value of sd input. I’ve written one such function here:\n\nnoise &lt;- function(x, mean = 0, sd){\n  x + rnorm(length(x), \n            mean, \n            sd)\n}\n\nNext, we can use this function to generate a new “fake” dataset with observations we would have expected to come from our linear regression. Note I’m storing these predictions in a tibble(), so that I can easily plot them later!\n\nsim_response &lt;- tibble(sim_body_mass_g = noise(penguin_predict, \n                                           sd = penguin_sigma)\n                   )\nhead(sim_response)\n\n# A tibble: 6 × 1\n  sim_body_mass_g\n            &lt;dbl&gt;\n1           2701.\n2           3422.\n3           4070.\n4           3339.\n5           3911.\n6           3792.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#plotting-predictions",
    "href": "10-predictive-checks.html#plotting-predictions",
    "title": "10  Predictive Checks",
    "section": "10.3 Plotting Predictions",
    "text": "10.3 Plotting Predictions\nThe best way to assess if / where there are differences between the simulated data and the observed data is through visualizations. We can do this in two ways, (1) visualizing the distribution of the responses, and (2) visualizing the relationship between the responses and the explanatory variables.\n\n10.3.1 Distribution of Responses\n\nobs_p &lt;- penguins |&gt;\n  ggplot(aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 350, \n                 color = \"white\"\n                 ) +\n  labs(x = \"Observed Body Mass (g)\",\n       y = \"\",\n       subtitle = \"Count\") +\n  xlim(2500, 6500) +\n  theme_bw()\n\nnew_p &lt;- sim_response |&gt;\n  ggplot(aes(x = sim_body_mass_g)) +\n  geom_histogram(binwidth = 350, \n                 color = \"white\"\n                 ) +\n  labs(x = \"Simulated Body Mass (g)\",\n       y = \"\",\n       subtitle = \"Count\") +\n  xlim(2500, 6500) +\n  theme_bw()\n\nlibrary(patchwork)\nobs_p + new_p\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Adding in the Observed x’s\nIf we are interested in seeing if the observed relationships between flipper_length_mm and body_mass_g are radically different from the simulated data, we need to add these variables into our simulated data set.\nRemember, by default lm() throws out any observations with missing values (NA) for any of the variables included in the regression. Thus, we will have fewer predictions than observations in our data set. To make sure the predictions match up with their corresponding row, we need to filter the missing values out of the data set before we bind the columns together.\n\nsim_data &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g), \n         !is.na(flipper_length_mm)\n         ) |&gt; \n  select(body_mass_g, flipper_length_mm) |&gt; \n  bind_cols(sim_response)\n\nhead(sim_data)\n\n# A tibble: 6 × 3\n  body_mass_g flipper_length_mm sim_body_mass_g\n        &lt;int&gt;             &lt;int&gt;           &lt;dbl&gt;\n1        3750               181           2701.\n2        3800               186           3422.\n3        3250               195           4070.\n4        3450               193           3339.\n5        3650               190           3911.\n6        3625               181           3792.\n\n\n\n\n10.3.3 Scatterplot of Relationships\nA scatterplot of the relationships modeled by the linear regression can give a more detailed idea for where the simulated data differ from the observed data. In the scatterplots below, we see that the data simulated from the regression more closely follows a linear relationship, but there are not dramatic differences between the two scatterplots.\n\nobs_reg_p &lt;- penguins |&gt;\n  ggplot(aes(y = body_mass_g,\n             x = flipper_length_mm)) +\n  geom_point() +\n  labs(title = \"Observed Body Mass\",\n       x = \"Flipper Length (mm)\",\n       y = \"\",\n       subtitle = \"Body Mass (g)\") +\n  theme_bw()\n\nsim_reg_p &lt;-sim_data |&gt;\n  ggplot(aes(y = sim_body_mass_g,\n             x = flipper_length_mm)\n         ) +\n  geom_point() +\n   labs(title = \"Simulated Body Mass \\nbased on Regression Model\",\n       x = \"Flipper Length (mm)\",\n       y = \"\",\n       subtitle = \"Body Mass (g)\") +\n  theme_bw()\n\nobs_reg_p + sim_reg_p",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#assessing-predictions",
    "href": "10-predictive-checks.html#assessing-predictions",
    "title": "10  Predictive Checks",
    "section": "10.4 Assessing Predictions",
    "text": "10.4 Assessing Predictions\nWe would expect that if the regression model is a good model for penguin body mass, then the simulated data should look similar to what was observed. We can compare the simulated data with the observed data using a scatterplot.\nIf the simulated data were identical to the observed data, they would all fall on the \\(y = x\\) line (dashed blue). Values above the \\(y = x\\) line correspond to simulated body masses larger than the observed body masses, and values below the line correspond to simulated body masses smaller than the observed body masses. Overall, it appears that there are about as many over estimates as underestimates. It appears there is a “moderate” relationship between the observed values and simulated values, as the points are fairly close to the line, but not extremely close.\n\nsim_data |&gt; \n  ggplot(aes(x = sim_body_mass_g, \n             y = body_mass_g)\n         ) + \n  geom_point() + \n   labs(x = \"Simulated Body Mass (g)\", \n        y = \"\",\n        subtitle = \"Observed Body Mass (g)\" ) + \n  geom_abline(slope = 1,\n              intercept = 0, \n              color = \"steelblue\",\n              linetype = \"dashed\",\n              lwd = 1.5) +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can use a statistic to summarize how “close” the simulated values and the observed values are. We can use any statistic that captures the residuals from this regression. I like \\(R^2,\\) since it has an easy reference value for what I would expect if my model did a good job (\\(R^2 \\approx 1\\)).\nI like the glance() function from the broom package, since it produces a nice table output of the summary measures from a linear regression.\n\nlm(body_mass_g ~ sim_body_mass_g, \n   data = sim_data\n   ) |&gt; \n  glance() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.553\n0.552\n536.778\n421.14\n0\n1\n-2633.944\n5273.888\n5285.392\n97964349\n340\n342\n\n\n\n\n\nI can then use the column names of the table to select() the r.squared column\n\nThis would be a great place to use inline code to report the $R^2 value!\n\n\nsim_r2 &lt;- lm(body_mass_g ~ sim_body_mass_g, \n             data = sim_data\n             ) |&gt; \n  glance() |&gt; \n  select(r.squared) |&gt; \n  pull()\nsim_r2\n\n[1] 0.5533018",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#iterating",
    "href": "10-predictive-checks.html#iterating",
    "title": "10  Predictive Checks",
    "section": "10.5 Iterating!",
    "text": "10.5 Iterating!\nWe are interested in seeing how our model performs for more than one simulated data set. Thus, we need to iterate through the process outlined above. Specifically, at each step we will need to:\n\nSimulate new data\nRegress the new data on the observed data\nSave the \\(R^2\\) from the regression\n\n\n10.5.1 Lots of Simulated Observations\nLuckily, we have already written the noise() function that helps us simulate new observations that could have occurred under our model. Since we are not changing any input to noise(), we can choose between passing an “arbitrary” input to map() (e.g., 1:100). This essentially rerun the same process multiple times. Since we want for every simulated data set to be bound together as columns, we can use the map_dfc() function!\n\nnsims &lt;- 100\nsims &lt;- map_dfc(.x = 1:nsims,\n                .f = ~ tibble(sim = noise(penguin_predict, \n                                          sd = penguin_sigma)\n                              )\n                )\n\nhead(sims)\n\n# A tibble: 6 × 100\n  sim...1 sim...2 sim...3 sim...4 sim...5 sim...6 sim...7 sim...8 sim...9\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1   3330.   2653.   3087.   3150.   3129.   3859.   3454.   3088.   3262.\n2   3446.   3754.   3335.   2994.   2989.   3979.   3456.   3929.   3401.\n3   3904.   4197.   3450.   3582.   4103.   4317.   3666.   4069.   3626.\n4   4445.   3608.   3342.   3672.   4412.   3510.   3543.   4393.   4261.\n5   3356.   3924.   3464.   3581.   3304.   4460.   3600.   3877.   3568.\n6   3100.   3072.   3534.   3054.   3006.   3769.   2769.   2546.   3224.\n# ℹ 91 more variables: sim...10 &lt;dbl&gt;, sim...11 &lt;dbl&gt;, sim...12 &lt;dbl&gt;,\n#   sim...13 &lt;dbl&gt;, sim...14 &lt;dbl&gt;, sim...15 &lt;dbl&gt;, sim...16 &lt;dbl&gt;,\n#   sim...17 &lt;dbl&gt;, sim...18 &lt;dbl&gt;, sim...19 &lt;dbl&gt;, sim...20 &lt;dbl&gt;,\n#   sim...21 &lt;dbl&gt;, sim...22 &lt;dbl&gt;, sim...23 &lt;dbl&gt;, sim...24 &lt;dbl&gt;,\n#   sim...25 &lt;dbl&gt;, sim...26 &lt;dbl&gt;, sim...27 &lt;dbl&gt;, sim...28 &lt;dbl&gt;,\n#   sim...29 &lt;dbl&gt;, sim...30 &lt;dbl&gt;, sim...31 &lt;dbl&gt;, sim...32 &lt;dbl&gt;,\n#   sim...33 &lt;dbl&gt;, sim...34 &lt;dbl&gt;, sim...35 &lt;dbl&gt;, sim...36 &lt;dbl&gt;, …\n\n\nSince all of the columns have the same name, dplyr automatically adds ... and a number after each column. So, our column names look like sim...1, sim...2, etc. It would be nice to replace the ...s with a _, which our friend str_replace_all() can help us do! Remember, the . is a special character and needs to be escaped!\n\ncolnames(sims) &lt;- colnames(sims) |&gt; \n  str_replace(pattern = \"\\\\.\\\\.\\\\.\",\n                  replace = \"_\")\n\nFinally, we can add the observed response (body_mass_g) into our simulated data set, for ease of modeling and visualizing.\n\nsims &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g), \n         !is.na(flipper_length_mm)) |&gt; \n  select(body_mass_g) |&gt; \n  bind_cols(sims)\n\nhead(sims)\n\n# A tibble: 6 × 101\n  body_mass_g sim_1 sim_2 sim_3 sim_4 sim_5 sim_6 sim_7 sim_8 sim_9 sim_10\n        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1        3750 3330. 2653. 3087. 3150. 3129. 3859. 3454. 3088. 3262.  3379.\n2        3800 3446. 3754. 3335. 2994. 2989. 3979. 3456. 3929. 3401.  3845.\n3        3250 3904. 4197. 3450. 3582. 4103. 4317. 3666. 4069. 3626.  3981.\n4        3450 4445. 3608. 3342. 3672. 4412. 3510. 3543. 4393. 4261.  4222.\n5        3650 3356. 3924. 3464. 3581. 3304. 4460. 3600. 3877. 3568.  4258.\n6        3625 3100. 3072. 3534. 3054. 3006. 3769. 2769. 2546. 3224.  2885.\n# ℹ 90 more variables: sim_11 &lt;dbl&gt;, sim_12 &lt;dbl&gt;, sim_13 &lt;dbl&gt;, sim_14 &lt;dbl&gt;,\n#   sim_15 &lt;dbl&gt;, sim_16 &lt;dbl&gt;, sim_17 &lt;dbl&gt;, sim_18 &lt;dbl&gt;, sim_19 &lt;dbl&gt;,\n#   sim_20 &lt;dbl&gt;, sim_21 &lt;dbl&gt;, sim_22 &lt;dbl&gt;, sim_23 &lt;dbl&gt;, sim_24 &lt;dbl&gt;,\n#   sim_25 &lt;dbl&gt;, sim_26 &lt;dbl&gt;, sim_27 &lt;dbl&gt;, sim_28 &lt;dbl&gt;, sim_29 &lt;dbl&gt;,\n#   sim_30 &lt;dbl&gt;, sim_31 &lt;dbl&gt;, sim_32 &lt;dbl&gt;, sim_33 &lt;dbl&gt;, sim_34 &lt;dbl&gt;,\n#   sim_35 &lt;dbl&gt;, sim_36 &lt;dbl&gt;, sim_37 &lt;dbl&gt;, sim_38 &lt;dbl&gt;, sim_39 &lt;dbl&gt;,\n#   sim_40 &lt;dbl&gt;, sim_41 &lt;dbl&gt;, sim_42 &lt;dbl&gt;, sim_43 &lt;dbl&gt;, sim_44 &lt;dbl&gt;, …\n\n\n\n\n10.5.2 Lots of Regressions & \\(R^2\\) Values\nNow, we want to regress each of these simulated body_mass_gs on the original, observed body_mass_g. Again, we will need to iterate through this process.\nBefore, I fit a linear regression between body_mass_g and the simulated data. Now, I have 100 different datasets I need to regress on body_mass_g. I can use map() to define a function that will be applied to each column of sims. This function regresses body_mass_g on each column (.x) from the sims dataset.\n\nsims |&gt; \n  map(~ lm(body_mass_g ~ .x, data = sims))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Inspecting the \\(R^2\\) Values\n\nsim_r_sq &lt;- sims |&gt; \n  map(~ lm(body_mass_g ~ .x, data = sims)) |&gt; \n  map(glance) |&gt; \n  map_dbl(~ .x$r.squared)\n\nIf I look at the first six \\(R^2\\) values, I see that the first value corresponds to the the following regression: lm(body_mass_g ~ body_mass_g)\nAll of the values thereafter are the \\(R^2\\) values from the simulated data.\n\nhead(sim_r_sq)\n\nbody_mass_g       sim_1       sim_2       sim_3       sim_4       sim_5 \n  1.0000000   0.5896863   0.5753450   0.5819932   0.5725102   0.5705712 \n\n\nI am interested in looking at the distribution of \\(R^2\\) values from the simulated data, so I’m going to remove this unwanted entry of the sim_r_sq vector.\n\nsim_r_sq &lt;- sim_r_sq[names(sim_r_sq) != \"body_mass_g\"]\n\n\n\n10.5.4 Plotting the Simulated \\(R^2\\) Values versus the Observed \\(R^2\\)\nThe final stage is to plot the statistics from the simulated data. The distribution of these \\(R^2\\) values will tell if our assumed model does a good job of producing data similar to what was observed. If the model produces data similar to what was observed, we would expect \\(R^2\\) values near 1.\n\ntibble(sims = sim_r_sq) |&gt; \n  ggplot(aes(x = sims)) + \n  geom_histogram(binwidth = 0.025) +\n  labs(x = expression(\"Simulated\"~ R^2),\n       y = \"\",\n       subtitle = \"Number of Simulated Models\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn this plot, we see that the simulated datasets have \\(R^2\\) vales between 0.5 and 0.65. This indicates the data simulated under this statistical model are moderately similar to what was observed. On average, our simulated data account for about 57.7% of the variability in the observed penguin body mass.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  },
  {
    "objectID": "10-predictive-checks.html#references",
    "href": "10-predictive-checks.html#references",
    "title": "10  Predictive Checks",
    "section": "References",
    "text": "References\n\n\n\n\nGelman, Andrew. 2014. “Model Checking.” In Bayesian Data Analysis. CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Predictive Checks</span>"
    ]
  }
]