[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 331/531 Statistical Computing with R",
    "section": "",
    "text": "This text has been modified from material by Dr. Susan VanderPlas. See UNL Stat 151: Introduction to Statistical Computing and UNL Stat 850: Computing Tools for Statisticians for her course books with integration of content and videos from Dr. Allison Theobold and Dr. Kelly Bodwin.\nThis text is designed to demonstrate statistical programming concepts and techniques in R. It is intended as a substitute for hours and hours of video lectures - watching someone code and talk about code is not usually the best way to learn how to code. It’s far better to learn how to code by … coding.\nI hope that you will work through this text week by week over the quarter. I have included comics, snark, gifs, YouTube videos, extra resources, and more: my goal is to make this a collection of the best information I can find on statistical programming.\nIn most cases, this text includes way more information than you need. Everyone comes into this class with a different level of computing experience, so I’ve attempted to make this text comprehensive. Unfortunately, that means some people will be bored and some will be overwhelmed. Use this text in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you.\n\n\nI’ve made an effort to use some specific formatting and enable certain features that make this text a useful tool for this class.\n\n\n\n\nWatch out sections contain things you may want to look out for - common errors, etc.\n\n\n\nExample sections contain code and other information. Don’t skip them!\n\n\n\nSometimes, there are better resources out there than something I could write myself. When you see this section, go read the enclosed link as if it were part of the book.\n\n\n\nLearn More sections contain other references that may be useful on a specific topic. Suggestions are welcome (email me to suggest a new reference that I should add), as there’s no way for one person to catalog all of the helpful programming resources on the internet!\n\n\n\nThese sections contain things you should definitely not consider as fact and should just take with a grain of salt.\n\n\n\nNote sections contain clarification points (anywhere I would normally say “note that ….)\n\n\n\nCheck-in sections contain quizzes or preview activities you must complete and submit to Canvas for credit.\n\n\n\nTry it out sections contain the required weekly practice activities (and sometimes additional/optional activities) you should do to reinforce the things you’ve just read.\n\n\n\n\n\nThese are expandable sections, with additional information when you click on the line\n\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\nMany times, examples will be in expandable sections\n\nThis keeps the code and output from obscuring the actual information in the textbook that I want you to retain. You can always look up the syntax, but you do need to absorb the details I’ve written out.\n\n\n\n\n\n\nReferences or additional readings may come from the following texts:\n\nR for Data Science (2e)\nAdvanced R\nModern Dive\nStat 545 (Data wrangling, exploration, and analysis with R) by Jenny Bryan\n\nYou can find additional help for Coding in R form the following resources:\n\nRStudio Education Page\nPosit Primers\n“R Bootcamp” Practice"
  },
  {
    "objectID": "00-prereading.html",
    "href": "00-prereading.html",
    "title": "Pre-reading",
    "section": "",
    "text": "Reading: 26 minute(s) at 200 WPM.\nVideos: 28 minutes"
  },
  {
    "objectID": "00-prereading.html#ch0-objectives",
    "href": "00-prereading.html#ch0-objectives",
    "title": "Pre-reading",
    "section": "Objectives",
    "text": "Objectives\nA prerequisite for this course is an introductory programming course. Therefore, there is some assumed knowledge. Maybe you have not seen these concepts in R, but you have already developed a base for logically thinking through a computing problem in another language.\nThis chapter is meant to provide a resource for the basics of programming (in R) and gives me a place to refer back to (as need be) in future chapters.\nIn this chapter you will:\n\nLearn the basics of a computer system.\nRefresh your mathematical logic and apply it to variables, vectors, and matrices.\nKnow the different types of variables and how to assign them to objects in R.\nUnderstand how to create and index vectors and matrices in R.\nExtend your logic to control structures with if-then statements and loops."
  },
  {
    "objectID": "00-prereading.html#computer-basics",
    "href": "00-prereading.html#computer-basics",
    "title": "Pre-reading",
    "section": "Computer Basics",
    "text": "Computer Basics\nIt is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\nHardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\n\nOperating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\n\nFile Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\n\n\nStop watching at 4:16.\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\n\n\n\n\nRecommend watching - helpful for understanding file paths!\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home.\nIn Chapter 1, we will discuss Directories, Paths, and Projects as they relate to R and getting setup to be successful in this course."
  },
  {
    "objectID": "00-prereading.html#vectors-matrices-and-arrays",
    "href": "00-prereading.html#vectors-matrices-and-arrays",
    "title": "Pre-reading",
    "section": "Vectors, Matrices, and Arrays",
    "text": "Vectors, Matrices, and Arrays\n\n\nThis section introduces some of the most important tools for working with data: vectors, matrices, loops, and if statements. It would be nice to gradually introduce each one of these topics separately, but they tend to go together, especially when you’re talking about programming in the context of data processing.\nMathematical Logic\nBefore we start talking about data structures and control structures, though, we’re going to take a minute to review some concepts from mathematical logic. This will be useful for both data structures and control structures, so stick with me for a few minutes.\nAnd, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT.\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\nx <- c(TRUE, FALSE, TRUE, FALSE)\ny <- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n\n[1]  TRUE FALSE FALSE FALSE\n\nx | y # OR\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n!x & y # NOT X AND Y\n\n[1] FALSE  TRUE FALSE FALSE\n\nx & !y # X AND NOT Y\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nDe Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\nDefinitions\nDeMorgan’s First Law\nDeMorgan’s Second Law\n\n\n\n Suppose that we set the convention that .\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)\n\n\n\n\n\nBasic Data Types\nWhile we will discuss data types more in depth during class, it is important to have a base grasp on the types of data you might see in a programming language.\nValues and Types\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn R, there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float or numeric- decimal numbers.\n\n\nfloat is short for floating-point value.\n\ndouble is a floating-point value with more precision (“double precision”).1\n\n\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, R has a function to help you with that.\n\nclass(FALSE)\nclass(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nclass(2)\nclass(\"Hello, programmer!\")\n\n[1] \"logical\"\n[1] \"integer\"\n[1] \"numeric\"\n[1] \"character\"\n\n\n\nIn R, boolean values are TRUE and FALSE. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000. Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.\n\nVariables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nIn R, we assign variables values using the syntax object_name <- value You can read this as “object name gets value” in your head.\n\nmessage <- \"So long and thanks for all the fish\"\nyear <- 2025\nthe_answer <- 42L\nearth_demolished <- FALSE\n\n\nNote that in R, we assign variables values using the <- operator. Technically, = will work for assignment, but <- is more common than = in R by convention.\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\nValid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R.\nWhat happens if we try to create a variable name that isn’t valid?\nStarting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol”.\n\n1st_thing <- \"check your variable names!\"\n\nError: <text>:1:2: unexpected symbol\n1: 1st_thing\n     ^\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\nThere are a few different conventions for naming things that may be useful:\n\n\nsome_people_use_snake_case, where words are separated by underscores\n\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nsome.people.use.periods\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated.\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.\nType Conversions\nWe talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. R is dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\nmode(2L + 3.14159) # add integer 2 and pi\n\n[1] \"numeric\"\n\nmode(2L + TRUE) # add integer 2 and TRUE\n\n[1] \"numeric\"\n\nmode(TRUE + FALSE) # add TRUE and FALSE\n\n[1] \"numeric\"\n\n\nAll of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how R handles type conversions.\nTest it out!\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “*” or “.” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell R that we want our string to be treated as a number, or vice-versa.\nIn R, we can explicitly convert a variable’s type using as.XXX() functions, where XXX is the type you want to convert to (as.numeric, as.integer, as.logical, as.character, etc.).\n\nx <- 3\ny <- \"3.14159\"\n\nx + y\n\nError in x + y: non-numeric argument to binary operator\n\nx + as.numeric(y)\n\n[1] 6.14159\n\n\nOperators and Functions\nIn addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\nOperation\nR symbol\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nInteger Division\n%/%\n\n\nModular Division\n%%\n\n\nExponentiation\n^\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nSo 14 %/% 3 would be 4, and 14 %% 3 would be 2.\n\n14 %/% 3\n\n[1] 4\n\n14 %% 3\n\n[1] 2\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated.\nOrder of Operations\nR operates under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n(1+1)^(5-2) # 2 ^ 3 = 8\n\n[1] 8\n\n1 + 2^3 * 4 # 1 + (8 * 4)\n\n[1] 33\n\n3*1^3 # 3 * 1\n\n[1] 3\n\n\nString Operations\nYou will have to use functions to perform operations on strings, as R does not have string operators. In R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n\n[1] \"first second\"\n\npaste(\"first\", \"second\", collapse = \" \")\n\n[1] \"first second\"\n\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works on separate parameters\n\n[1] \"first\"  \"second\"\n\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n\n[1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n\n[1] \"a-first b-second c-third d-fourth\"\n\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n\n[1] \"a\" \"b\" \"c\"\n\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n\n[1] \"abc\"\n\n\nYou don’t need to understand the details of this at this point in the class, but it is useful to know how to combine strings.\nFunctions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. Instead, I’m going to show you how to use functions.\nIt may be helpful at this point to print out the R reference card3. This cheat sheet contains useful functions for a variety of tasks.\nMethods are a special type of function that operate on a specific variable type. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\nData Structures\nIn the previous section, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\nLists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nx <- list(\"a\", 3, FALSE)\nx\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] FALSE\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\nIndexing\nEvery element in a list has an index (a location, indicated by an integer position)4.\nIn R, we count from 1.\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\nx <- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n\n[[1]]\n[1] \"a\"\n\nx[1:2] # This returns multiple elements in the list\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\nx[[1]] # This returns the item\n\n[1] \"a\"\n\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n\nError in x[[1:2]]: subscript out of bounds\n\n\nList indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\nVectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \n\nIndexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is. I’m going to demonstrate indices with the string vector\n\n\n\n\n\n\nR\n\n\n\n\n1-indexed language\n\n\n\nCount elements as 1, 2, 3, 4, …, N\n\n\n\n\n\n\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi <- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n\n[1] 3\n\ndigits_pi[2]\n\n[1] 1\n\ndigits_pi[3]\n\n[1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n\nnumeric(0)\n\ndigits_pi[11]\n\n[1] 5\n\n# Print out the vector\ndigits_pi\n\n [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\nfavorite_cats <- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"    \n\nfavorite_cats[2] <- \"Nyan Cat\"\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"    \n\n\nIf you’re curious about any of these cats, see the footnotes5.\nIndexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n# Define a character vector\nweekdays <- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend <- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days <- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days <- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n\n[1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days <- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n\nReviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n\n[1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n\n[1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n\n[1] 2 0\n\n\nAs a reminder, this is an example of implicit type conversion - R decides what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\nMatrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\nMatrix (Lego)\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nMost of the problems we’re going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\nIndexing in Matrices\nR uses [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\nmy_mat <- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] <- 500\n\nmy_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]  500   10   11   12\n\n\nMatrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\n\ntranspose - flip the matrix across the left top -> right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\n\nmatrix multiplication (dot product) - you will learn more about this in linear algebra, but here’s a preview. Here is a better explanation of the cross product \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\nx <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny <- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n3 * x\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n# Transpose\nt(x)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nt(y)\n\n     [,1] [,2]\n[1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n\n     [,1]\n[1,]   17\n[2,]   39\n\n\nArrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\narray(1:8, dim = c(2,2,2))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement."
  },
  {
    "objectID": "00-prereading.html#control-structures",
    "href": "00-prereading.html#control-structures",
    "title": "Pre-reading",
    "section": "Control Structures",
    "text": "Control Structures\nThe focus of this course is more on working with data - however in prior programming courses you have likely developed the logical thinking to work with Control structures. Control structures are statements in a program that determine when code is evaluated (and how many times it might be evaluated). There are two main types of control structures: if-statements and loops.\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\n\n\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\n\n\n\nHere’s another example:\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’\n\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time.\nConditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\n\nLet’s try this out:\n\nx <- 3\ny <- 1\n\nif (x > 2) { \n  y <- 8\n} else {\n  y <- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nThe logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x > 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx <- 3\ny <- 1\n\nif (x > 2) y <- 8 else y <- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\nRepresenting Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n\nUS Tax brackets\n\n\nProblem\nSolution\nProgram Flow Chart\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n# Start with total income\nincome <- 200000\n\n# x will hold income that hasn't been taxed yet\nx <- income\n# y will hold taxes paid\ny <- 0\n\nif (x <= 10275) {\n  y <- x*.1 # tax paid\n  x <- 0 # All money has been taxed\n} else {\n  y <- y + 10275 * .1\n  x <- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x <= (41775 - 10275)) {\n  y <- y + x * .12\n  x <- 0\n} else {\n  y <- y + (41775 - 10275) * .12\n  x <- x - (41775 - 10275) \n}\n\nif (x <= (89075 - 41775)) {\n  y <- y + x * .22\n  x <- 0\n} else {\n  y <- y + (89075 - 41775) * .22\n  x <- x - (89075 - 41775)\n}\n\nif (x <= (170050 - 89075)) {\n  y <- y + x * .24\n  x <- 0\n} else {\n  y <- y + (170050 - 89075) * .24\n  x <- x - (170050 - 89075)\n}\n\nif (x <= (215950 - 170050)) {\n  y <- y + x * .32\n  x <- 0\n} else {\n  y <- y + (215950 - 170050) * .32\n  x <- x - (215950 - 170050)\n}\n\nif (x <= (539900 - 215950)) {\n  y <- y + x * .35\n  x <- 0\n} else {\n  y <- y + (539900 - 215950) * .35\n  x <- x - (539900 - 215950)\n}\n\nif (x > 0) {\n  y <- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n\n[1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\nLet’s explore using program flow maps for a slightly more complicated problem: The tax bracket example that we used to demonstrate if statement syntax.\n\n\n\n\n\nThe control flow diagram for the code in the previous example\n\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\nChaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\nAge brackets\nFor instance, suppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\nProgram Flow Map\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as <18, 18-25, 26-40, 41-55, 56-65, and >65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\n\nThe important thing to realize when examining this program flow map is that if age <= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age <= 18), (age <= 25), and (age <= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - <40 is evaluated first, and so <= 25 and <= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\n\nIn code, we would write this statement using else-if (or elif) statements.\n\nage <- 40 # change this as you will to see how the code works\n\nif (age < 18) {\n  bracket <- \"<18\"\n} else if (age <= 25) {\n  bracket <- \"18-25\"\n} else if (age <= 40) {\n  bracket <- \"26-40\"\n} else if (age <= 55) {\n  bracket <- \"41-55\" \n} else if (age <= 65) {\n  bracket <- \"56-65\"\n} else {\n  bracket <- \">65\"\n}\n\nbracket\n\n[1] \"26-40\"\n\n\n\n\n\nLoops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\nWhile Loops\nWe just discussed conditional statements, where a block of code is only executed if a logical statement is true.\nThe simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\nFlow map showing while-loop pseudocode (while x <= N) { # code that changes x in some way} and the program flow map expansion where we check if x > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\nx <- 0\n\nwhile (x < 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x <- x + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n\n\nWhile loops\n\n\nProblem\nMath Notation\nSolution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nYou will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk <- 1\nprod <- 1\nans <- (exp(pi) - exp(-pi))/(2*pi)\ndelta <- 0.0001\n\nwhile (abs(prod - ans) >= 0.0001) {\n  prod <- prod * (1 + 1/k^2)\n  k <- k + 1\n}\n\nk\n\n[1] 36761\n\nprod\n\n[1] 3.675978\n\nans\n\n[1] 3.676078\n\n\n\n\n\nFor Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\nFlow Map\nR\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nFor loops are often run from 1 to N but in essence, a for loop is run for every value of a vector (which is why loops are included in the same chapter as vectors).\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n\n[1] \"January\"\n[1] \"February\"\n[1] \"March\"\n[1] \"April\"\n[1] \"May\"\n[1] \"June\"\n[1] \"July\"\n[1] \"August\"\n[1] \"September\"\n[1] \"October\"\n[1] \"November\"\n[1] \"December\"\n\n\n\nAvoiding Infinite Loops\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\nThis while loop runs until either x < 10 or n > 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n>50 check to the loop so that we don’t tie up the computer for eternity.\n\nx <- 0\nn <- 0 # count the number of times the loop runs\n\nwhile (x < 10) { \n  print(x)\n  x <- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n <- n + 1\n  if (n > 50) \n    break # this stops the loop if n > 50\n}\n\n[1] 0\n[1] -0.2984257\n[1] -0.8049591\n[1] 0.7023727\n[1] 1.5954\n[1] 0.8606832\n[1] 1.064491\n[1] 0.6408406\n[1] 1.876348\n[1] 0.8428358\n[1] 0.8631968\n[1] 0.4156385\n[1] -0.5332541\n[1] -1.010971\n[1] -2.336364\n[1] -3.507409\n[1] -3.732658\n[1] -4.593124\n[1] -4.650528\n[1] -4.964994\n[1] -4.68372\n[1] -5.446541\n[1] -5.883251\n[1] -6.926738\n[1] -5.697704\n[1] -6.592909\n[1] -7.849048\n[1] -9.13494\n[1] -9.85854\n[1] -9.878529\n[1] -11.10149\n[1] -12.74681\n[1] -14.51639\n[1] -14.85616\n[1] -14.99609\n[1] -15.05961\n[1] -16.93765\n[1] -18.67281\n[1] -19.34363\n[1] -19.54931\n[1] -20.89679\n[1] -20.115\n[1] -19.79292\n[1] -20.9806\n[1] -22.29348\n[1] -22.94335\n[1] -21.82923\n[1] -20.8951\n[1] -19.36147\n[1] -20.19296\n[1] -19.73008\n\n\nIn the example above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\nControlling Loops\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n\nBreak Statement\nNext/Continue Statement\n\n\n\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n\n[1] 1\n[1] 2\n[1] \"Divisible by 3\"\n[1] 4\n[1] \"Divisible by 5\"\n[1] \"Divisible by 3\"\n[1] 7\n[1] 8\n[1] \"Divisible by 3\"\n[1] \"Divisible by 5\"\n[1] 11\n[1] \"Divisible by 3\"\n[1] 13\n[1] 14\n[1] \"Exiting now\"\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use."
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "Reading: 24 minute(s) at 200 WPM.\nVideos: 47 minutes"
  },
  {
    "objectID": "01-introduction.html#ch1-objectives",
    "href": "01-introduction.html#ch1-objectives",
    "title": "\n1  Introduction\n",
    "section": "Objectives",
    "text": "Objectives\n\nSet up necessary software for this class on personal machines.\nDetect and resolve problems related to file systems, working directories, and system paths when troubleshooting software installation.\nCreate R Markdown or Quarto documents with good reproducible principles."
  },
  {
    "objectID": "01-introduction.html#getting-started-checkins",
    "href": "01-introduction.html#getting-started-checkins",
    "title": "\n1  Introduction\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are four check-ins for this week:\n\nCheck-in 1.1: Syllabus & Learning Community Quiz\n\n\nComplete the Syllabus & Learning Community Quiz on Canvas.\nThe course syllabus includes information about class dates, course structure, and expectations.\nI will also be asking you some questions about yourself so I can best support you in this course! If you plan to take this quiz more than once (you have up to three retakes on check-ins) you may want to copy your open ended answers into another document so you do not have to retype them each time.\n\nCheck-in 1.2: Discord Server\n\n\nIntroduce yourself in the “introductions” channel of the Discord Server (under WEEK 0: BASICS OF CODING).\n\nUsing the Class Discord\nWe’ll be using Discord to interact with our peers and instructor outside of class.\nDiscord is a platform for text chatting, voice chatting, and screen sharing.\nI will often be on Discord when in my office.\nJoin the server\nJoin the Stat 331/531 Server (link to join can be found on canvas) to start experimenting with the interface.\nWhen you join the server, you will be given some suggestions to get started.\n\nI recommend you click through these - and in particular, it is probably a good idea to download the desktop version of Discord, and perhaps to install it on your phone if you wish.\nSet up your account\nVerify your email\nTo use this Discord server, you must have a verified email.\nNobody (including your professors) will be able to see this email, and it does not have to be your Cal Poly email. This is simply to keep the server from being overrun by temporary accounts.\nCreate your identity\nThe first thing you should do is decide what name and picture you would like to use.\n\nI would like to strongly encourage you to use your real name and picture, so that everyone can get to know you. However, if you prefer to remain anonymous, you are free to do so.\n\n(Please do not be like Regina and use the name of another student, however! This kind of impersonation will result in a permanent ban from the server.)\nDecide about privacy and notifications\nThe default settings on the channel are probably just fine for you.\nFeel free to make any changes that work for you, though.\nYou can change your message notifications:\n\nYou can edit your privacy settings, although most things are already private:\n\nConnect other apps\nYou can connect other apps to Discord, either for productivity or just for fun.\n\nUsing the Channels\nThe server is made up of many channels. Some are text chatrooms, while some are “Voice Channels” that connect you via audio to everyone else in the channel.\nText Channels\nUse the #general channel for anything and everything:\n\nIf your question is about course logistics, rather than the material itself, consider using the #class-logistics channel:\n\nYou can use the specific weekly channels to ask questions about the material…\n\n… or the specific lab assignment.\n\nNotice that you can use tick marks (```), like in R Markdown / Quarto, to make your code appear in a formatted code box.\nVoice Channels\nTo join a voice channel, simply click it! Make sure you are careful about when you are muted or unmuted.\n\nThe extra “Side Chat” channels are limited to 4 or 8 people, if you would like to start an impromptu study conversation without being heard by me and / or the rest of the class. (I’ll only drop in if you invite us!)\nVoice channels can also be used for people to “Go Live”, and share their screen with everyone else.\n\nWhile this will usually be something professors use to demonstrate code, you can go live, too! But you may need to download the desktop version of Discord to do so.\nPrivate messages\nIt is also easy to send private messages, to your professor or to each other. These private messages can also easily be used to launch a private video chat and / or screen sharing.\n\nCreating your own server\nLast but not least - for the teams you are a part of, you may want to use Discord to communicate with each other about the weekly assignments. You can do this by creating your own server! You can easily hop between servers during work parties, to ask each other questions or just to take a break and chat about life.\n\nIntroduce yourself in the “introductions” channel of the Discord Server under WEEK 0: BASICS OF CODING. Share your name, major, year in school, and either (1) your favorite comfort food or (2) your go to study spot on campus or in SLO. Take a screenshot of your introduction on the discord channel and upload it to the Canvas assignment.\n\nCheck-in 1.3: RStudio Project\n\n\nGet set-up!\nFollow the instructions for Installing R, RStudio, and Quarto. Then create a class directory and RStudio project for this course. You are set up and ready to go!\nTake a screenshot of your class directory, showing the RProject and the folder organization, including a week-1 or practice-activity folder. Upload this screenshot to the Canvas assignment.\n\nCheck-in 1.4: Introduction to R & RStudio"
  },
  {
    "objectID": "01-introduction.html#getting-started",
    "href": "01-introduction.html#getting-started",
    "title": "\n1  Introduction\n",
    "section": "\n1.1 Getting Started",
    "text": "1.1 Getting Started\nSetting up your computer\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class message board or contact me for help.\nIf you already have R downloaded, please follow these steps anyways, to make sure you have the most recent version of R. Do not ignore these instructions. If you neglect to update your version of R, you may find that updating a package will make it so your code will not run.\n\nCheck-in 1.3: Installing R, RStudio, and Quarto\nInstalling R, RStudio, and Quarto\n\n\nDownload and run the R installer for your operating system from CRAN:\n\nWindows: https://cran.rstudio.com/bin/windows/base/\n\nMac: https://cran.rstudio.com/bin/macosx/ (double check your macOS version)\nLinux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\n(Required) If you are on Windows, you should also install the Rtools4 package; this will ensure you get fewer warnings later when installing packages. Make sure you double check the version compatability.\nMore detailed instructions for Windows are available here\n\nDownload and install the latest version of RStudio for your operating system (see Step 2 Installers). RStudio is a integrated development environment (IDE) for R - it contains a set of tools designed to make writing R code easier.\nDownload and install the latest version of Quarto for your operating system. Quarto is a command-line tool released by RStudio that allows Rstudio to work with python and other R specific tools in a unified way. We will talk more about Quarto in a later section, but for now just know this is the “notebook” you will be completing and writing all your assignments in.\n\n\n\n\n\nIf you would like a video tutorial on downloading RStudio, here is one:\n\n\n\n\n\n\n\nRStudio organization has recently re-branded to posit. Rstudio is still the name of the IDE, but these two names may be used interchangeably. See a video of Hadley Wickham talking about the re-branding.\n\n\n\n\n\n\nIntroduction to R and RStudio\nIn this section, we will learn about some of the tools you just installed. You may have worked with R and RStudio in previous classes, but never had a course dedicated to learning about their functionality.\nIntroduction to R\n\nR is a statistical programming language. Unlike more general-purpose languages, R is optimized for working with data and doing statistics. R was created by Ross Ihaka and Robert Gentleman in 1993 (hence “R”) and was formally released by the R Core Group in 1997 (a group of 20ish volunteers who are the only people who can change the base - built in- functionality of R). If you want to build an independent, standalone graphical interface, or run a web server, R is probably not the ideal language to use (you might want C/python or PHP/python, respectively). If you want to vacuum up a bunch of data, fit several regression models, and then compare the models, R is a great option and will be faster than working in a more general-purpose language like C or base python.\n\n\n\n\n\n\nR is\n\nvector-based\n1 indexed (start counting 1, 2, 3, …)\na scripting language (R code does not have to be compiled before it is run)\n\nOne thing to know about R is that it is open-source. This means that no company owns R (like there is for SAS or Matlab) and that developers cannot charge for the use of their R software. This does not mean that all of your code needs to be public (you can keep your code private), but it is important to be a good open-source citizen by sharing your code publicly when possible (later we will learn about GitHub), contributing to public projects and packages, creating your own packages, and using R for ethical and respectful projects.\n\nNote that RStudio is NOT R, but a platform to help you use R through and that it is a way to make money around the culture of R.\n\n\nThe History of R\n\nThe History of R\nRStudio: the IDE\nAn IDE is an integrated development environment - a fancy, souped up text editor that is built to make programming easier. Back in the dark ages, people wrote programs in text editors and then used the command line to compile those programs and run them.\n\n\n\n\n\n\nRStudio provides a cheat-sheet for the IDE if you are so inclined.\n\nRStudio is not R - it’s just a layer on top of R. So if you have a question about the user interface, you have an RStudio question. If you have a question about the code, you have an R question.\n\nNavigating RStudio\n\n\n\n\nThe RStudio window will look something like this.\n\n\n\n\nIn the top-left pane is the text editor. This is where you’ll do most of your work.\nIn the top right, you’ll find the environment, history, and connections tabs. The environment tab shows you the objects available in R (variables, data files, etc.), the history tab shows you what code you’ve run recently, and the connections tab is useful for setting up database connections.\nOn the bottom left is the console. There are also other tabs to give you a terminal (command line) prompt, and a jobs tab to monitor progress of long-running jobs. In this class we’ll primarily use the console tab.\n\nOn the bottom right, there are a set of tabs:\n\nfiles (to give you an idea of where you are working, and what files are present),\nplots (which will be self-explanatory),\npackages (which extensions to R are installed and loaded),\nthe help window (where documentation will show up), and\nthe viewer window, which is used for interactive graphics or previewing HTML documents.\n\n\nInstalling Packages\nOne of R’s strengths is the package repository, CRAN (Comprehensive R Archive Network), that allows anyone (yes, even you!) to write an R package. Packages contain “extra” functionality (outside the base functionality of R). This means that R generally has the latest statistical methods available, and one of the best ways to ensure someone uses your work is to write an R package to make that work accessible to the general population of statisticians/biologists/geneticists.\nTo install the tibble package in R, we would use the following code:\n\ninstall.packages(\"tibble\")\n\nThen, to use the functions within that package, we need to load the package:\n\nlibrary(\"tibble\")\n\nWhen you load a package, all of the functions in that package are added to your R Namespace (this is a technical term) - basically the list of all of the things R knows about. This may be problematic if you have two packages with the same function name.\n\nYou only need to install a package once and your computer will be able to find the package on your computer when it needs to. However, you need to load the package every time R is restarted or you switch to a new project.\n\nIf you want to use a function from a package without loading the package into your namespace, you can do that by using pkgname::function syntax.\nFor instance, this code creates a sample data frame using the tribble function in the tibble package.\n\ntibble::tribble(~col1, ~col2, 1, 'a', 2, 'b', 3, 'c')\n\n# A tibble: 3 × 2\n   col1 col2 \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 c    \n\n\nTest your setup\nWe will find our way in R and learn more about Quarto in the following sections, but for now open RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console:\n\n\ninstall.packages(\n\n  c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\")\n\n)\n\n\nCan you find the text editor?\n\nCreate a new quarto document (File &gt; New File &gt; Quarto Document).\nCompile the document using the Render button and use the Viewer pane to see the result.\nIf this all worked, you have RStudio, Quarto, and R set up correctly on your machine.\n\n\nCheck-in 1.4: Introduction to R & RStudio\nAnswer the following questions on the Canvas Quiz.\nQuestion 1: What does it mean for R to be “open-source”?\nQuestion 2: How often do you need to install a package on your computer?\nQuestion 3: What is the tidyverse?\n\nAdditional Resources: Basics of R and RStudio\nBasics of R Programming\nBasics of R\nRStudio Primer, Basics of Programming in R\nIntroduction to RStudio\nA tour of RStudio, BasicsBasics1\nQuick tour of RStudio"
  },
  {
    "objectID": "01-introduction.html#directories-and-projects",
    "href": "01-introduction.html#directories-and-projects",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Directories, Paths, and Projects",
    "text": "1.2 Directories, Paths, and Projects\nIn the pre-reading section, File Systems, we learned how to organize our personal files and locate them using absolute and relative file paths. The idea of a “base folder” or starting place was introduced as a working directory. In R, there are two ways to set up your file path and file system organization:\n\n\n\n\n\n\n\nSet your working directory in R (do not reccommend)\nUse RProjects (preferred!)\n\nWorking Directories in R\n\nTo find where your working directory is in R, you can either look at the top of your console or type getwd() into your console.\n\n\n\n\n\n\ngetwd()\n\n[1] \"C:/Users/erobin17/OneDrive - Cal Poly/stat331-calpoly-text\"\n\n\nAlthough it is not recommended, you can set your working directory in R with setwd().\n\nsetwd(\"/path/to/my/assignment/folder\")\n\nRprojects\nSince there are often many files necessary for a project (e.g. data sources, images, etc.), R has a nice built in system for setting up your project organization with RProjects.\nTo create an Rproject, first open RStudio on your computer and click File &gt; New Project, then:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you plan to use the Studio computers during class, I recommend having a way to access and save your material between the Studio computers and your personal computer (e.g. put your class directory and Rproject on OneDrive). The idea with Rprojects is then your relative file paths will work in either computer without any changes!\n\nThis new folder, stat331-test-student should now live in your OneDrive folder (or whatever sub-directory you save it to). This is your new “home” base for this class - whenever you refer to a file with a relative path it will look for it here.\nCheck-in 1.3: RProject\n\nTake a screenshot of your class directory, showing the RProject and the folder organization, including a week-1 or practice-activity folder. Upload this screenshot to the Canvas assignment.\n\nFile Paths in R\n\nA quick warning on file paths is that Mac/Linux and Windows differ in the direction of their backslash to separate folder locations. Mac/Linux use / (e.g. practice-activities/PA1.pdf) while Window’s uses \\ (e.g. practice-activities\\PA1.pdf).\nR can work with both, however, a backslash \\ means something different to R so if you copy a file path from your file filder in Windows, you will need to replace all backslashes with a double backslash \\\\ (e.g. practice-activities\\\\PA1.pdf)\n\nWorkflow\nWorkflow and Projects\nSoftware Carpentry – Project Managment with RStudio"
  },
  {
    "objectID": "01-introduction.html#scripts-and-notebooks",
    "href": "01-introduction.html#scripts-and-notebooks",
    "title": "\n1  Introduction\n",
    "section": "\n1.3 Scripts and Notebooks",
    "text": "1.3 Scripts and Notebooks\nIn this class, we’ll be using markdown notebooks to keep our code and notes in the same place. One of the advantages of both R is that it is a scripting language, but it can be used within notebooks as well. This means that you can have an R script file, and you can run that file, but you can also create a document (like the one you’re reading now) that has code AND text together in one place. This is called literate programming and it is a very useful workflow both when you are learning programming and when you are working as an analyst and presenting results.\nScripts\nBefore I show you how to use literate programming, let’s look at what it replaces: scripts. Scripts are files of code that are meant to be run on their own. They may produce results, or format data and save it somewhere, or scrape data from the web – scripts can do just about anything.\nScripts can even have documentation within the file, using # characters (at least, in R) at the beginning of a line. # indicates a comment – that is, that the line does not contain code and should be ignored by the computer when the program is run. Comments are incredibly useful to help humans to understand what the code does and why it does it.\n\nPlotting a logarithmic spiral\nThis code will use concepts we have not yet introduced - feel free to tinker with it if you want, but know that you’re not responsible for being able to write this code yet. You just need to read it and get a sense for what it does. I have heavily commented it to help with this process.\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta &lt;- seq(0, 4*pi, .01) \n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta\nr &lt;- seq(0, 5, length.out = length(theta))\n\n# Now define x and y in cartesian coordinates\nx &lt;- r * cos(theta)\ny &lt;- r * sin(theta)\n\nplot(x, y, type = \"l\")\n\n\n\nFigure 1.1: A Cartesian Spiral in R\n\n\n\nTo create your first script, click File &gt; New File &gt; R Script and copy paste the code from above. You can save this script on your computer just as you would any other file such as a word document, pdf, or image.\nScripts can be run in Rstudio by clicking the Run button  at the top of the editor window when the script is open.\n\n\n\n\n\n\nMost of the time, you will run scripts interactively - that is, you’ll be sitting there watching the script run and seeing what the results are as you are modifying the script. However, one advantage to scripts over notebooks is that it is easy to write a script and schedule it to run without supervision to complete tasks which may be repetitive.\nNotebooks\nNotebooks are an implementation of literate programming. R has native notebooks that allow you to code in R. This book is written using Quarto markdown, which is an extension of Rmarkdown.\nIn this class, we’re going to use Quarto/R markdown. This matters because the goal is that you learn something useful for your own coding and then you can easily apply it when you go to work as an analyst somewhere to produce impressive documents.\nTo create a quarto document click File &gt; New File &gt; Quarto Document. This will open a notebook template using quarto. You can then Render the document to a pdf or html file.\nIntroduction to Quarto\n(Required) Read through the following resources to introduce Quarto:\n\nR4DS: Quarto\nIntro to Quarto\n\nWhile in this class we will be using Quarto, before Quarto there was RMarkdown (and it is still widely used). If you wish, you can read about Rmarkdown here.\nDownload and save the Markdown syntax Cheat Sheet.\nLearn more about Notebooks and Quarto\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\nYihui Xie is the person responsible for knitr and Rmarkdown.\n\nYou can learn more about the functionality of Quarto at the following links:\n\nComputations in Quarto\nAuthoring & Formatting Quarto Documents\n\nYou can find the entire list of options you can use to format your HTML file with Quarto here. Poke around the gallery of cool HTML documents rendered with Quarto:\n\nInteractive document published to the web\nAdvanced Layout with HTML\nPimp my RMarkdown\nQuarto Tip A Day"
  },
  {
    "objectID": "01-introduction.html#getting-help",
    "href": "01-introduction.html#getting-help",
    "title": "\n1  Introduction\n",
    "section": "\n1.4 Getting help",
    "text": "1.4 Getting help\nIn R, you can access help with a ?. Suppose we want to get help on a for loop. In the R console, we can run this line of code to get help on for loops.\n\n?`for`\n\nstarting httpd help server ... done\n\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name.\n(You will have to run this in interactive mode for it to work)\nw3schools has an excellent R help on basic functions that may be useful as well - usually, these pages will have examples.\nGoogle is your friend.\nThe R community has an enormous aresenal of online learning resources. I will linked a few throughout the “read more” sections in this text, but you can always find more!\n\nLearn to:\n\nGoogle for tutorials and examples\nUse Stack Overflow\n\nAsk questions on Twitter\nMake good use of the vast and welcoming R network on the internet"
  },
  {
    "objectID": "01-introduction.html#pa-1-find-the-mistakes",
    "href": "01-introduction.html#pa-1-find-the-mistakes",
    "title": "\n1  Introduction\n",
    "section": "PA 1: Find the Mistakes",
    "text": "PA 1: Find the Mistakes\nYou can access PA1: Find the Mistakes one of two ways:\n\nClick here to access an RStudio Cloud project which we work on in groups on Day 1 of the course (Note: if you do not have an RStudio (Posit) Cloud account, you will be asked to create one). Make sure to save a permanent copy!\nIf you have already installed R, RStudio, and Quarto, you can create a new Quarto document, and copy the practice activity template into the file (Note: make sure your quarto file is on Source mode and not Visual mode when you copy the template).\n\nThe components of the Practice Activity are described below:\nPart One:\nThis file has many mistakes in the code. Some are errors that will prevent the file from knitting; some are mistakes that do NOT result in an error.\nFix all the problems in the code chunks.\nPart Two:\nFollow the instructions in the file to uncover a secret message.\nSubmit the name of the poem as the answer to the Canvas Quiz question."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html",
    "href": "02-tidy-data-and-basics-of-graphics.html",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "",
    "text": "Reading: 30 minute(s) at 200 WPM.\nVideos: 14 minutes"
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#ch2-objectives",
    "href": "02-tidy-data-and-basics-of-graphics.html#ch2-objectives",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "Objectives",
    "text": "Objectives\n\nRecognize tidy data formats and identify the variables and columns in the data sets\nRead in data from common formats into R\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data\nEvaluate existing charts and develop new versions that improve accessibility and readability"
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#ch2-checkins",
    "href": "02-tidy-data-and-basics-of-graphics.html#ch2-checkins",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are two check-ins for this week:\n\nCheck-in 2.1: Loading in Data\nCheck-in 2.2: Introduction to Data Visualization with ggplot"
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#tidy-data",
    "href": "02-tidy-data-and-basics-of-graphics.html#tidy-data",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "\n2.1 Tidy Data",
    "text": "2.1 Tidy Data\nIn the pre-reading, we learned about Basic Data Types (strings/characters, numeric/double/floats, integers, and logical/booleans) and Data Structures (1D - vectors and lists; 2D - matrices and data frames). This class will mainly focus on working with data frames it is important to know the type and format of the data you’re working with. It is much easier to create data visualizations or conduct statistical analyses if the data you use is in the right shape.\nData can be formatted in what are often referred to as wide format or long format. Wide format data has variables spread across columns and typically uses less space to display. This format is how you would typically choose to present your data as there is far less repetition of labels and row elements. Long format data is going to have each variable in a column and each observation in a row; this is likely not the data’s most compact form.\nLong formatted data is often what we call tidy data - a specific format of data in which each variable is a column, each observation is a row and each type of observational unit forms a table (or in R, a data frame).\n\n\n\n\nWhat is tidy data? Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\n\nSame data, different layouts\nCan you determine which of the following data sets follows the tidy data format?\n\n\nOption 1:\nOption 2:\nOption 3:\nSolution\n\n\n\n\n\nName\nTreatment A\nTreatment B\n\n\n\nBrian Boatwright\nNA\n18\n\n\nTrenna Porras\n4\n1\n\n\nHannaa Kumar\n6\n7\n\n\n\n\n\n\n\nTreatment\nBrian Boatwright\nTrenna Porras\nHannaa Kumar\n\n\n\nA\nNA\n4\n6\n\n\nB\n18\n1\n7\n\n\n\n\n\n\n\nName\nTreatment\nMeasurement\n\n\n\nBrian Boatwright\nA\nNA\n\n\nTrenna Porras\nA\n4\n\n\nHannaa Kumar\nA\n6\n\n\nBrian Boatwright\nB\n18\n\n\nTrenna Porras\nB\n1\n\n\nHannaa Kumar\nB\n7\n\n\n\n\n\nOption 3 follows the tidy data format since each variable (Name, Treatment, and Measurement) belong to their own columns and each observation taken is identified by a single row.\n\n\n\nData frames are a specific object type in R, data.frame(), and can be indexed the same as matrices. It may be useful to look at your data before beginning to work with it to see the format of the data. The following funcitons in R help us learn information about our data sets:\n\n\nclass(): outputs the object type\n\nhead(): outputs the first 6 rows of the data set\n\nsummary() outputs 6-number summaries or frequencies for all variables in the data set depending on the variables data type\n\ndata$variable: extracts a specific variable (column) from the data set\n\nYou may also choose to click on the data set name in your Environment window pane in R and the data set will pop up in a new tab in the script pane.\n\nWorking with data sets in R\nThe cars data set is a default data set that lives in R (for examples like this!).\n\n# outputs the class of the cars data set (data.frame)\nclass(cars)\n\n[1] \"data.frame\"\n\n# outputs the first six rows of the cars data set\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# outputs\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n# outputs the second row of the cars data set\ncars[2,]\n\n  speed dist\n2     4   10\n\n# outputs the first column of the cars data set as a vector\ncars[,1]\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n# outputs the speed variable of the cars data set as a vector (notice this is the same as above)\ncars$speed\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\n\n:::\nIn a perfect world, all data would come in the right format for our needs, but this is often not the case. We will spend the next few weeks learning about how to use R to reformat our data to follow the tidy data framework and see why this is so important. For now, we will work with nice clean data sets but you should be able to identify when data follows a tidy data format and when it does not."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#loading-external-data",
    "href": "02-tidy-data-and-basics-of-graphics.html#loading-external-data",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "\n2.2 Loading External Data",
    "text": "2.2 Loading External Data\n\n2.2.1 An Overview of External Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables (like tidy data we learned about above!). This type of data can be stored on the computer in multiple ways:\n\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\n\n.csv “Comma-separated”\n\n.txt plain text - could be just text, comma-separated, tab-separated, etc.\n\n\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n.xls\n.xlsx\n\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying tweets here.\nAlso, there’s this amazing comic:\n\nTo be minimally functional in R, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files.\nLearn more about external data\n\nAdditional external data formats\nWe will not cover binary files and databases, but you can consult one or more online references if you are interested in learning about these.\n\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs. We aren’t going to do anything with binary formats, just know they exist.\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which may be more common:\n\nWeb related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis.\nSpatial files: Shapefiles are by far the most common version of spatial files1. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information.\n\n2.2.2 Where does my data live?\n\n\n\n\n\n\nThe most common way to read data into R is with read_csv() and you may provide the file input as either a url to the data set or a path for the file input.\n\nThe read_csv() function belongs to the readr package.\ninstall.packages(\"readr\") or click on your Packages tab to install.\n\n\nread_csv()\n\n# load the readr package\nlibrary(readr)\n\n# this will work for everyone!\nsurveys <- read_csv(file = \"https://www.dropbox.com/s/1qqyshx5ikt9zoc/surveys.csv?dl=1\")\n\n# this will work only on my computer\n# notice this is an absolute file path on my computer\nsurveys <- read_csv(file = \"C:/Users/erobin17/OneDrive - Cal Poly/stat331/labs/lab2/surveys.csv\")\n\nRecall our discussion about Directories, Paths, and Projects. Often you will specify a relative file path to your data set rather than an absolute file path. This works if the file is in the same directory as the code or within a sub-folder of the same directory as the code.\nRelative file paths\n\n# this will work if the file is in the same directory as the code\n# (i.e., the Quarto document and the data re in the same folder)\nsurveys <- read_csv(file = \"surveys.csv\")\n\n# this will work if a sub-folder called \"data\" is in the same directory as the code\nsurveys <- read_csv(file = \"data/surveys.csv\")\n\n\n\n\nI often organize my workflow with sub-folders so that I would have a sub-folder called data to reference from my base directory location.\nYou can either choose to create an overall data sub-folder right within your stat331 folder (e.g., stat331 > data) and store all of your data for your class there… or you may choose to store the data associated with each individual assignment within that folder (e.g., stat331 > labs > lab2 > data).\n\n\n\nBut wait! I thought we created our RProjects to indicate our “home base” directory? When working with a Quarto document, this changes your base directory (for any code running within the .qmd file) to the same folder the .qmd file lives in.\nOpen your class directory RProject and type getwd() into the Console. This should lead you to the folder directory your RProject was created in.\nNow open your lab1 assignment and type getwd() into a code chunk. This should lead you to the folder directory your lab1.qmd file is saved in.\n\nA great solution to consistency in file paths is the here package:\n\n\n\n\n\n\nWorkflows that shred…, by Allison Horst\n\n\n\n\ninstall.packages(\"here\")\nThis package thinks “here” is the your directory folder the RProject lives in (e.g., your stat331 folder). This makes a global “home base” that overrides any other directory path.\n\n# shows you the file path the function here() will start at.\nhere::dr_here()\n\nThe here package\nIf my stat331 file structure looks like this:\n\n\n\n\n\nthen my RProject sets the working directory to this stat331 folder. However, if my lab1.qmd file lives in C:/Users/erobin17/OneDrive - Cal Poly/stat331/labs/lab1 then when I try to load data in within the .qmd file, it will be looking for that relative file path from the lab1 folder and not from the stat331 folder.\nIf my data is stored within a data subfolder as shown above, within my .qmd file, my relative file path must “backtrack” out of the lab1 folder two moves before entering into the data folder to find my data.\n\nsurveys <- read_csv(\"../../data/surveys.csv\")\n\nWith the here package however, within my .qmd file I can ALWAYS assume I am in my stat331 folder and enter directly into the data folder.\n\nsurveys <- read_csv(here::here(\"data\", \"surveys.csv\"))\n\nNote the folder and file names are in quotations because they are names of files and not objects in R.\n\n\n\nUsing “::” before the function is a way of telling R which package the function lives in without having to load that entire package (e.g., PackageName::FunctionName). There are some common functions the R community does this for mainly just out of practice. Essentially it saves memory from not having to load unnecessary functions if you only need one funciton from that package.\n\n\n\nI have a complicated relationship with the here package. I see the strong uses for it, but I don’t always set up my workflow in this way. You will form your own opinions, but the important thing for you to know is how to find the correct file path to read your data into your .qmd files.\n\n\n2.2.3 How do I load in my data?\n\n\n\n\n\n\nIt may be helpful to save this data import with the tidyverse cheat sheet.\n\n(required) Read the following to learn about importing data into R\n\nR4DS: Data Import\n\n\nPreviously, we learned about common types of data files and briefly introduced the read_csv() and function. There are many functions in R that read in specific formats of data:\n\n\nBase R\nreadr & readxl\n\n\n\nBase R contains the read.table() and read.csv() functions. In read.table() you must specify the delimiter with the sep = argument. read.csv() is just a specific subset function of read.table() that automatically assumes a comma delimiter.\n\n# comma delimiter\nread.table(file = \"ages.txt\", sep = \",\")\n\n# tab delimiter\nread.table(file = \"ages.txt\", sep = \"\\t\")\n\n\n\nThe tidyverse has some cleaned-up versions in the readr and readxl packages:\n\n\nread_csv() works like read.csv, with some extra stuff\n\nread_txv() is for tab-separated data\n\nread_table() is for any data with “columns” (white space separating)\n\nread_delim() is for special “delimiters” separating data\n\nread_xlsx() is specifically for dealing with Excel files\n\n\n\n\n\nLearn more\n\nRSQLite vignette\n\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds.\n\nCheck-in 2.1: Loading in Data\nFor this check-in you are asked to work through reading in different data sets. The folder Age_Data contains several data sets with the names and ages of five individuals. The data sets are stored as different file types. Download Ages_data.zip here, make sure to unzip the folder, save these in a reasonable place (e.g., stat331 > check-ins > checkin2.1 or stat331 > week2), then use the readr and readxl packages to complete the following and fill in the “gap” code on the Canvas:\n\n\nMy file folder structure looks like this:\n\n\n\nLoad the appropriate packages for reading in data.\nRead in the data set ages.csv\n\nRead in the data set ages_tab.txt\n\nRead in the data set ages_mystery.txt\n\nRead in the data set ages.xlsx\n\nFind a way to use read_csv() to read ages.csv with the variable “Name” as a factor and “Age” as a character."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#basics-of-graphics",
    "href": "02-tidy-data-and-basics-of-graphics.html#basics-of-graphics",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "\n2.3 Basics of Graphics",
    "text": "2.3 Basics of Graphics\nNow that we understand the format of tidy data and how to load external data into R, we want to be able to do something with that data! We are going to start with creating data visualizations.\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts.\n\nThis is one of the less serious schemes I’ve seen\nBut, in my opinion, Randall missed the opportunity to put a pie chart as Neutral Evil.\n\nHopefully by the end of this, you will be able to at least make the charts which are most commonly used to show data and statistical concepts.\n\n2.3.1 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John Tukey, Data Based Graphics: Visual Display in the Decades to Come\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\nConsider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\nOur brains are very good at processing large amounts of visual information quickly. Evolutionarily, it’s important to be able to e.g. survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIt’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.\n\nYou’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while ggplot has a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it.\n\n\n\n2.3.2 ggplot2\nIn this class, we’re going to use the ggplot2 package to create graphics in R. This package is already installed in the tidyverse, but can be installed:\n\n\n\n\nBuilding a masterpiece, by Allison Horst\n\n\ninstall.packages(\"ggplot2\")\nand/or loaded:\nlibrary(\"tidyverse\") (my preference!) or library(\"ggplot2\")\nWe will learn about all of these different pieces and the process of creating graphics by working through examples, but there is a general “template” for creating graphics in ggplot:\n\nggplot(data = <DATA>) +\n  <GEOM FUNCTION>(mapping = aes(<MAPPINGS>)) +\n  any other arugments ...\n\nwhere\n\n is the name of the data set\n is the name of the geom you want for the plot (e.g., geom_histogram(), geom_line(), geom_point())\n is where variables from the data are mapped to parts of the plot (e.g., x = speed, y = distance, color = carmodel)\nany other arguments could include the theme, labels, faceting, etc.\n\nYou may want to download and save cheat sheets and reference guides for ggplot.\n\n2.3.3 The Grammar of Graphics\nThe grammar of graphics is an approach first introduced in Leland Wilkinson’s book (Wilkinson 2005). Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the data set itself relates to the components of the chart.\nThis has a few advantages:\n\nIt’s relatively easy to represent the same data set with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: (Sarkar 2018)\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\nExploratory Data Analysis with the grammar of graphics\n\n\nSketch\nggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing) # this data set is housed internally to R\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point()\n\n\n\n\n\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions of your dataset you want to visualize.\n\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom.\n\n\nDecide the geometric objects (geoms) you want to represent your variables with on the chart. For example, a scatterplot displays the information as points by mapping the \\(x\\) and \\(y\\) aesthetics to points on the graph.\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from Sarkar (2018)).\n\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of ggplot2’s features.\nBefore we start exploring, let’s add a title and label our axes, so that we’re creating good, informative charts:\nxlab(), ylab(), and ggtitle().\nAlternatively, you could use labs(title = , x = , y = ) to add a title and label the axes.\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point() + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\ngeom_smooth(method = \"<LINE TYPE>\") where method = lm adds a linear regression model and method = loess adds a loess regression smoother.\n\n\nSketch\nggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n...aes(...color = <VARIABLE>)\n\n\nSketch\nggplot\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) + \n  # geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities). Don’t worry too much about the subsetting code yet, we will get to that next week!\n\ncitylist <- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub <- dplyr::filter(txhousing, city %in% citylist)\n\nggplot(data = housingsub, aes(x = date, y = median, color = city)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nAnother way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\nfacet_wrap(), facet_grid()\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, aes(x = date, y = median)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  facet_wrap(~city) + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\nggplot(data = housingsub, aes(x = date, y = median, size = sales)) + \n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") + \n  facet_wrap(~city) +\n  # Remove extra information from the legend - \n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales', \n                             override.aes = list(linetype = NA, \n                                                 fill = 'transparent'))) + \n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0)) + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nUp to this point, we’ve used the same position information - date for the \\(x\\) axis, median sale price for the \\(y\\) axis. Let’s switch that up a bit so that we can play with some transformations on the \\(x\\) and \\(y\\) axis and add variable mappings to a continuous variable.\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) + \n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation2. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\nscale_x_log10(), scale_y_log10()\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) + \n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() + \n  scale_y_log10() + \n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in \\(x\\) and \\(y\\), and mapping the size or color of the point to number of sales.\n\nhouston <- dplyr::filter(txhousing, city == \"Houston\")\n\nggplot(data = houston, aes(x = date, y = inventory, size = sales)) + \n  geom_point(shape = 1) + \n  xlab(\"Date\") + ylab(\"Months of Inventory\") + \n  guides(size = guide_legend(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\nggplot(data = houston, aes(x = date, y = inventory, color = sales)) + \n  geom_point() + \n  xlab(\"Date\") + ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, aes(x = sales, y = inventory, color = date)) + \n  geom_point() + \n  xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Date\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nIs that easier or harder to read?\n\n\n\nSpecial properties of aesthetics\nGlobal vs Local aesthetics\nAny aesthetics assigned to the mapping within the first line, ggplot(), will be inherited by the rest of the geometric, geom_xxx(), lines. This is called a global aesthetic.\n\nggplot(data = housingsub, mapping = aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nAny aesthetics assigned to the mapping within a geometric object is only applied to that specific geom. Notice how color is no longer mapped to the regression line and there is only one overall regression line for all cities.\n\nggplot(data = housingsub, mapping = aes(x = date, y = median)) +\n  geom_point(mapping = aes(color = city)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nAssigning your aesthetics vs Setting your aesthetics\nWhen you assign a variable from your data set to your aesthetics, you put it inside aes() without quotation marks around the variable name:\n\nggplot(data = housingsub) +\n  geom_point(mapping = aes(x = date, y = median, color = city))\n\n\n\n\nIf you want to set a specific color, you could put this outside the aes() mapping in quotation marks because blue is not an object in R. Notice this color is assigned specifically in that geometric object.\n\nggplot(data = housingsub) +\n  geom_point(mapping = aes(x = date, y = median), color = \"blue\")\n\n\n\n\nYou should NOT put your color inside the aes() parentheses.\n\nggplot(data = txhousing) +\n  geom_point(mapping = aes(x = date, y = median, color = \"blue\"))\n\n\n\n\n\n2.3.3.1 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type. Consider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through catalogues like the R Graph Gallery to see what visualizations match your data and use-case.\n\n2.3.4 Creating Good Charts\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.\n\n2.3.4.1 Perceptual and Cognitive Factors\nColor\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, <1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\nYou can take a test designed to screen for colorblindness here\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n\n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\nIt is possible to simulate the effect of color blindness and color deficiency on an image.\n\n\n\n\n\nOriginal image using a rainbow color scale\n\n\n\n\n\n\nRed cone deficient\n\n\n\n\n\n\nGreen cone deficient\n\n\n\n\n\n\nBlue cone deficient\n\n\n\n\n\n\n\n\nRed cone absent\n\n\n\n\n\n\nGreen cone absent\n\n\n\n\n\n\nBlue cone absent\n\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\nImplications and Guidelines\n\nDo not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -> dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -> blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.3\n\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women4.\n\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\nShort Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\nTry it out!\n\nClick here, read the information, and then click to hide it.\n\n1 4 2 2 3 9 8 0 7 8\n\nWait a few seconds, then expand this section\n\nWhat was the third number?\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\nGrouping and Sense-making\nImposing order on visual chaos.\n\n\nAmbiguous Images\nIllusory Contours\nFigure/Ground\n\n\n\nWhat does the figure below look like to you?\n\n\nIs it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\nConsider this image - what do you see?\n\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\nfbiwide <- read.csv(\"https://github.com/srvanderplas/Stat151/raw/main/data/fbiwide.csv\")\nlibrary(dplyr)\n\nfbiwide %>% \n  filter(Year %in% c(1980, 2010)) %>%\n  filter(State %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\")) %>% \n  ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) + \n  geom_col(position = \"dodge\") +\n  coord_flip() + \n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\nOr, I could use a line chart\n\nfbiwide %>% \n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) + \n  geom_line() + \n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\nOr, I could use a box plot\n\nfbiwide %>% \n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = factor(Year), y = Murder/Population*100000)) + \n  geom_boxplot() + \n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.5\nGeneral guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.6\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n.\n\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\n\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration).\nR graphics\n\nggplot2 cheat sheet\n\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nggplot tricks\nR graph cookbook\nData Visualization in R\n\nCheck-in 2.2: Introduction to Data Visualization with ggplot2\nComplete the Canvas quiz to make sure you understand the basics of using ggplot2 to create graphics."
  },
  {
    "objectID": "02-tidy-data-and-basics-of-graphics.html#pa-2-using-data-visualization-to-find-the-penguins",
    "href": "02-tidy-data-and-basics-of-graphics.html#pa-2-using-data-visualization-to-find-the-penguins",
    "title": "\n2  Tidy Data & Basics of Graphics\n",
    "section": "PA 2: Using Data Visualization to Find the Penguins",
    "text": "PA 2: Using Data Visualization to Find the Penguins\nYou will be exploring different types of visualizations to uncover which species of penguins reside on different islands.\nVisit PA2: Using Data Visualization to Find the Penguins to see instructions for completing this activity."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html",
    "href": "03-data-cleaning-and-manipulation.html",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "",
    "text": "Reading: 18 minute(s) at 200 WPM.\nVideos: 60 minutes"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#ch3-objectives",
    "href": "03-data-cleaning-and-manipulation.html#ch3-objectives",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Objectives",
    "text": "Objectives\n\nApply data manipulation verbs (filter, select, group by, summarize, mutate) to prepare data for analysis\nIdentify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately and understand the consequences of data cleaning steps\nCreate summaries of data appropriate for analysis or display using data manipulation techniques"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#ch3-checkins",
    "href": "03-data-cleaning-and-manipulation.html#ch3-checkins",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere is one check-in for this week:\n\nCheck-in 3.1: Data Wrangling"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#tibbles",
    "href": "03-data-cleaning-and-manipulation.html#tibbles",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.1 Tibbles",
    "text": "3.1 Tibbles\nWe have been talking about our data in terms of data.frame objects in R. This is meant to inform you there is another object type in R called tibbles. Essentially, Tibbles are data frames, but they have certain features that make them easier to work with and provide additional cool features that can be useful (e.g., see nest()).\n\ntibble(\n  team   = c(\"A\", \"B\", \"C\", \"D\"), \n  points = c(22, 30, 18, 54)\n)\n\n# A tibble: 4 × 2\n  team  points\n  <chr>  <dbl>\n1 A         22\n2 B         30\n3 C         18\n4 D         54\n\n\nYou can use as_tibble() to convert data.frame objects in R to a tibble object.\nLearn more about tibbles\nYou can read more about Tibbles in R for Data Science: Tibbles"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#introduction-to-dplyr",
    "href": "03-data-cleaning-and-manipulation.html#introduction-to-dplyr",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.2 Introduction to dplyr\n",
    "text": "3.2 Introduction to dplyr\n\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for data visualization or statistical analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\n\n\n\n\nData wrangling (by Allison Horst)\n\n\nWe will be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations.\n\n\n\n\n\n\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\nMain dplyr verbs\n\nfilter()\narrange()\nselect()\nmutate()\nsummarize()\nUse group_by() to perform group wise operations\nUse the pipe operator (|> or %>%) to chain together data wrangling operations\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#motivation-example-dataset",
    "href": "03-data-cleaning-and-manipulation.html#motivation-example-dataset",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Motivation & Example Dataset",
    "text": "Motivation & Example Dataset\nLast week we learned all about creating graphics in ggplot2. I am hoping to use data visualization as motivation going forward in this class – how do we get our data look like what we need in order to create the graph we want?\n\nLet’s explore how the dplyr verbs work, using the starwars data set, which contains a comprehensive list of the characters in the Star Wars movies and information about their height, mass, hair_color, skin_color, eye_color, birth_year, sex, gender, homeworld, species, films, vehicles, and starships.\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load data set into memory. The loading isn’t complete until we actually use the data set though… so let’s look at our variables and types and print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstr(starwars)\nstarwars\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    blond \n    fair \n    blue \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n C-3PO \n    167 \n    75 \n    NA \n    gold \n    yellow \n    112.0 \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n R2-D2 \n    96 \n    32 \n    NA \n    white, blue \n    red \n    33.0 \n    none \n    masculine \n    Naboo \n    Droid \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Leia Organa \n    150 \n    49 \n    brown \n    light \n    brown \n    19.0 \n    female \n    feminine \n    Alderaan \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    brown \n    light \n    blue \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n  \n\n R5-D4 \n    97 \n    32 \n    NA \n    white, red \n    red \n    NA \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n Biggs Darklighter \n    183 \n    84 \n    black \n    light \n    brown \n    24.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    auburn, white \n    fair \n    blue-gray \n    57.0 \n    male \n    masculine \n    Stewjon \n    Human \n  \n\n\n\n\nWe could create a scatterplot of the character’s height by mass, color by species, and facet by homeworld.\n\nlibrary(ggplot2)\nggplot(data = starwars, aes(x = height, \n                            y = mass, \n                            color = species)\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\nThere is way too much going on in these plots to see anything of importance. Let’s break it down into the parts we are interested in."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#filter-pick-cases-rows-based-on-their-values",
    "href": "03-data-cleaning-and-manipulation.html#filter-pick-cases-rows-based-on-their-values",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.3 filter(): Pick cases (rows) based on their values",
    "text": "3.3 filter(): Pick cases (rows) based on their values\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\n\n\n\n\n\n\ndplyr filter() by Allison Horst\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Indexing Matrices, but here we’ll focus on using specific functions to perform the same operation.\nThe dplyr verb for selecting rows is filter(). filter() takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\nLet’s say we were interested in only the people, we could create a new data set starwars_people and filter on the species variable.\n\n# Get only the people\nstarwars_people <- filter(starwars, species == \"Human\")\nstarwars_people\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    blond \n    fair \n    blue \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Leia Organa \n    150 \n    49 \n    brown \n    light \n    brown \n    19.0 \n    female \n    feminine \n    Alderaan \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    brown \n    light \n    blue \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n  \n\n\n\n\nWe can create the same plot with our new subset of data.\n\nCodeggplot(data = starwars_people, aes(x = height, \n                            y = mass, \n                            color = species)\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\nThis looks better, but what if we only care about the people who come from Tatooine? Starting with our original starwars data set, we can combine logical AND statements with a comma to define a data subset called starwars_tatoonie_people.\n\n# Get only the people who come from Tatooine\nstarwars_tatooine_people <- filter(starwars, species == \"Human\", homeworld == \"Tatooine\")\nstarwars_tatooine_people\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    blond \n    fair \n    blue \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    brown \n    light \n    blue \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n  \n\n Biggs Darklighter \n    183 \n    84 \n    black \n    light \n    brown \n    24.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n\n\n\n\nCodeggplot(data = starwars_tatooine_people, aes(x = height, \n                            y = mass, \n                            color = species)\n       ) +\n  geom_point() +\n  facet_wrap(~ homeworld)\n\n\n\n\n\n\nUseful comparison operations in R\nWe might not always want to only filter on a variable set equal to a certain category or value, the following operations can help you combine logical operations in filter().\n\n\n> greater than\n\n< less than\n\n== equal to\n\n%in% identifies if an element belongs to a vector\n\n| or\n\n3.3.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements.\n\n\n\n\n\n\nFiltering by row number\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\nNotice how we now have C-3PO, Darth Vader, Beru Whites, Anakin Skywalker, etc. (rows 2, 4, 6, …) from the original starwars data set output above.\n\nfilter(starwars, (row_number() %% 2 == 0)) \n\n\n\nEven Rows\nOriginal starwars\n\n\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n C-3PO \n    167 \n    75 \n    NA \n    gold \n    yellow \n    112.0 \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n R5-D4 \n    97 \n    32 \n    NA \n    white, red \n    red \n    NA \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    auburn, white \n    fair \n    blue-gray \n    57.0 \n    male \n    masculine \n    Stewjon \n    Human \n  \n\n\n\n\n\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    blond \n    fair \n    blue \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n C-3PO \n    167 \n    75 \n    NA \n    gold \n    yellow \n    112.0 \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n R2-D2 \n    96 \n    32 \n    NA \n    white, blue \n    red \n    33.0 \n    none \n    masculine \n    Naboo \n    Droid \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Leia Organa \n    150 \n    49 \n    brown \n    light \n    brown \n    19.0 \n    female \n    feminine \n    Alderaan \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    brown \n    light \n    blue \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n  \n\n R5-D4 \n    97 \n    32 \n    NA \n    white, red \n    red \n    NA \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n Biggs Darklighter \n    183 \n    84 \n    black \n    light \n    brown \n    24.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    auburn, white \n    fair \n    blue-gray \n    57.0 \n    male \n    masculine \n    Stewjon \n    Human \n  \n\n\n\n\n\n\n\n\n\narrange() Sorting rows by variable values\nAnother common operation is to sort your data frame by the values of one or more variables.\narrange() is a dplyr verb for sorting rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\nThe code below arranges the starwars characters tallest to shortest.\n\narrange(starwars, desc(height))\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Yarael Poof \n    264 \n    NA \n    none \n    white \n    yellow \n    NA \n    male \n    masculine \n    Quermia \n    Quermian \n  \n\n Tarfful \n    234 \n    136 \n    brown \n    brown \n    blue \n    NA \n    male \n    masculine \n    Kashyyyk \n    Wookiee \n  \n\n Lama Su \n    229 \n    88 \n    none \n    grey \n    black \n    NA \n    male \n    masculine \n    Kamino \n    Kaminoan \n  \n\n Chewbacca \n    228 \n    112 \n    brown \n    unknown \n    blue \n    200.0 \n    male \n    masculine \n    Kashyyyk \n    Wookiee \n  \n\n Roos Tarpals \n    224 \n    82 \n    none \n    grey \n    orange \n    NA \n    male \n    masculine \n    Naboo \n    Gungan \n  \n\n Grievous \n    216 \n    159 \n    none \n    brown, white \n    green, yellow \n    NA \n    male \n    masculine \n    Kalee \n    Kaleesh \n  \n\n Taun We \n    213 \n    NA \n    none \n    grey \n    black \n    NA \n    female \n    feminine \n    Kamino \n    Kaminoan \n  \n\n Rugor Nass \n    206 \n    NA \n    none \n    green \n    orange \n    NA \n    male \n    masculine \n    Naboo \n    Gungan \n  \n\n Tion Medon \n    206 \n    80 \n    none \n    grey \n    black \n    NA \n    male \n    masculine \n    Utapau \n    Pau'an \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n\n\n\n\nKeep the top \\(n\\) values of a variable\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nThe code below outputs the 5 tallest characters in star wars.\n\nslice_max(starwars, order_by = height, n = 5)\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Yarael Poof \n    264 \n    NA \n    none \n    white \n    yellow \n    NA \n    male \n    masculine \n    Quermia \n    Quermian \n  \n\n Tarfful \n    234 \n    136 \n    brown \n    brown \n    blue \n    NA \n    male \n    masculine \n    Kashyyyk \n    Wookiee \n  \n\n Lama Su \n    229 \n    88 \n    none \n    grey \n    black \n    NA \n    male \n    masculine \n    Kamino \n    Kaminoan \n  \n\n Chewbacca \n    228 \n    112 \n    brown \n    unknown \n    blue \n    200 \n    male \n    masculine \n    Kashyyyk \n    Wookiee \n  \n\n Roos Tarpals \n    224 \n    82 \n    none \n    grey \n    orange \n    NA \n    male \n    masculine \n    Naboo \n    Gungan \n  \n\n\n\n\n\nOf course, there is a similar slice_min() function as well:\n\nThe code below outputs the 5 shortest characters in star wars.\n\nslice_min(starwars, order_by = height, n = 5)\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Yoda \n    66 \n    17 \n    white \n    green \n    brown \n    896 \n    male \n    masculine \n    NA \n    Yoda's species \n  \n\n Ratts Tyerell \n    79 \n    15 \n    none \n    grey, blue \n    unknown \n    NA \n    male \n    masculine \n    Aleen Minor \n    Aleena \n  \n\n Wicket Systri Warrick \n    88 \n    20 \n    brown \n    brown \n    brown \n    8 \n    male \n    masculine \n    Endor \n    Ewok \n  \n\n Dud Bolt \n    94 \n    45 \n    none \n    blue, grey \n    yellow \n    NA \n    male \n    masculine \n    Vulpter \n    Vulptereen \n  \n\n R2-D2 \n    96 \n    32 \n    NA \n    white, blue \n    red \n    33 \n    none \n    masculine \n    Naboo \n    Droid \n  \n\n R4-P17 \n    96 \n    NA \n    none \n    silver, red \n    red, blue \n    NA \n    none \n    feminine \n    NA \n    Droid \n  \n\n\n\n\nBy default, slice_max() and slice_min() return values tied with the nth value as well, which is why our result above has 6 rows.\n\n\n\nUse with_ties = FALSE.\n\nslice_min(starwars, order_by = height, n = 5, with_ties = FALSE)\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Yoda \n    66 \n    17 \n    white \n    green \n    brown \n    896 \n    male \n    masculine \n    NA \n    Yoda's species \n  \n\n Ratts Tyerell \n    79 \n    15 \n    none \n    grey, blue \n    unknown \n    NA \n    male \n    masculine \n    Aleen Minor \n    Aleena \n  \n\n Wicket Systri Warrick \n    88 \n    20 \n    brown \n    brown \n    brown \n    8 \n    male \n    masculine \n    Endor \n    Ewok \n  \n\n Dud Bolt \n    94 \n    45 \n    none \n    blue, grey \n    yellow \n    NA \n    male \n    masculine \n    Vulpter \n    Vulptereen \n  \n\n R2-D2 \n    96 \n    32 \n    NA \n    white, blue \n    red \n    33 \n    none \n    masculine \n    Naboo \n    Droid \n  \n\n\n\n\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nThe code below outputs the shortest 1% of characters in star wars.\n\nslice_max(starwars, order_by = height, prop = .01)\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#select-pick-columns",
    "href": "03-data-cleaning-and-manipulation.html#select-pick-columns",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.4 select(): Pick columns",
    "text": "3.4 select(): Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\nAn exhaustive(?) list of ways to select variables in dplyr\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(<set of variables>) will give you any columns that aren’t in the set of variables in parentheses\n\n\n(<set of vars 1>) & (<set of vars 2>) will give you any variables that are in both set 1 and set 2. (<set of vars 1>) | (<set of vars 2>) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\nLet’s try these selector functions out and see what we can accomplish!\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting simple, let’s only subset and keep only the following variables from the starwars data set: name, height, mass, birth_year, species, and homeworld.\n\nselect(starwars, name, height, mass, birth_year, species, homeworld)\n\n\n\n\n\n\n name \n    height \n    mass \n    birth_year \n    species \n    homeworld \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    19.0 \n    Human \n    Tatooine \n  \n\n C-3PO \n    167 \n    75 \n    112.0 \n    Droid \n    Tatooine \n  \n\n R2-D2 \n    96 \n    32 \n    33.0 \n    Droid \n    Naboo \n  \n\n Darth Vader \n    202 \n    136 \n    41.9 \n    Human \n    Tatooine \n  \n\n Leia Organa \n    150 \n    49 \n    19.0 \n    Human \n    Alderaan \n  \n\n Owen Lars \n    178 \n    120 \n    52.0 \n    Human \n    Tatooine \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    47.0 \n    Human \n    Tatooine \n  \n\n R5-D4 \n    97 \n    32 \n    NA \n    Droid \n    Tatooine \n  \n\n Biggs Darklighter \n    183 \n    84 \n    24.0 \n    Human \n    Tatooine \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    57.0 \n    Human \n    Stewjon \n  \n\n\n\n\nSince name, height, and mass are next to each other, we could have specified name:mass to tell us to select all of the columns between and including name to mass.\n\nselect(starwars, name:mass, birth_year, species, homeworld)\n\n\n\n\n\n\n name \n    height \n    mass \n    birth_year \n    species \n    homeworld \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    19.0 \n    Human \n    Tatooine \n  \n\n C-3PO \n    167 \n    75 \n    112.0 \n    Droid \n    Tatooine \n  \n\n R2-D2 \n    96 \n    32 \n    33.0 \n    Droid \n    Naboo \n  \n\n Darth Vader \n    202 \n    136 \n    41.9 \n    Human \n    Tatooine \n  \n\n Leia Organa \n    150 \n    49 \n    19.0 \n    Human \n    Alderaan \n  \n\n Owen Lars \n    178 \n    120 \n    52.0 \n    Human \n    Tatooine \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    47.0 \n    Human \n    Tatooine \n  \n\n R5-D4 \n    97 \n    32 \n    NA \n    Droid \n    Tatooine \n  \n\n Biggs Darklighter \n    183 \n    84 \n    24.0 \n    Human \n    Tatooine \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    57.0 \n    Human \n    Stewjon \n  \n\n\n\n\n\nThe select column is also useful for reordering the variables in your data set.\n:::\nPerhaps we want the birth_year, sex, gender, homeworld, and species to follow the name of the star wars character. We can use the everything() function to specify we want all the other variables to follow.\n\nselect(starwars, name, birth_year:species, everything())\n\n\n\n\n\n\n name \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n  \n\n\n Luke Skywalker \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n    172 \n    77 \n    blond \n    fair \n    blue \n  \n\n C-3PO \n    112.0 \n    none \n    masculine \n    Tatooine \n    Droid \n    167 \n    75 \n    NA \n    gold \n    yellow \n  \n\n R2-D2 \n    33.0 \n    none \n    masculine \n    Naboo \n    Droid \n    96 \n    32 \n    NA \n    white, blue \n    red \n  \n\n Darth Vader \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n    202 \n    136 \n    none \n    white \n    yellow \n  \n\n Leia Organa \n    19.0 \n    female \n    feminine \n    Alderaan \n    Human \n    150 \n    49 \n    brown \n    light \n    brown \n  \n\n Owen Lars \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n    178 \n    120 \n    brown, grey \n    light \n    blue \n  \n\n Beru Whitesun lars \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n    165 \n    75 \n    brown \n    light \n    blue \n  \n\n R5-D4 \n    NA \n    none \n    masculine \n    Tatooine \n    Droid \n    97 \n    32 \n    NA \n    white, red \n    red \n  \n\n Biggs Darklighter \n    24.0 \n    male \n    masculine \n    Tatooine \n    Human \n    183 \n    84 \n    black \n    light \n    brown \n  \n\n Obi-Wan Kenobi \n    57.0 \n    male \n    masculine \n    Stewjon \n    Human \n    182 \n    77 \n    auburn, white \n    fair \n    blue-gray \n  \n\n\n\n\nNote that everything() won’t duplicate columns you’ve already added.\n:::\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\ndplyr::relocate\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n\n# move numeric variables to the front\nrelocate(starwars, where(is.numeric))\n\n# A tibble: 87 × 11\n   height  mass birth_year name     hair_color skin_color eye_color sex   gender\n    <int> <dbl>      <dbl> <chr>    <chr>      <chr>      <chr>     <chr> <chr> \n 1    172    77       19   Luke Sk… blond      fair       blue      male  mascu…\n 2    167    75      112   C-3PO    <NA>       gold       yellow    none  mascu…\n 3     96    32       33   R2-D2    <NA>       white, bl… red       none  mascu…\n 4    202   136       41.9 Darth V… none       white      yellow    male  mascu…\n 5    150    49       19   Leia Or… brown      light      brown     fema… femin…\n 6    178   120       52   Owen La… brown, gr… light      blue      male  mascu…\n 7    165    75       47   Beru Wh… brown      light      blue      fema… femin…\n 8     97    32       NA   R5-D4    <NA>       white, red red       none  mascu…\n 9    183    84       24   Biggs D… black      light      brown     male  mascu…\n10    182    77       57   Obi-Wan… auburn, w… fair       blue-gray male  mascu…\n# ℹ 77 more rows\n# ℹ 2 more variables: homeworld <chr>, species <chr>"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#mutate-add-and-transform-variables",
    "href": "03-data-cleaning-and-manipulation.html#mutate-add-and-transform-variables",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.5 mutate(): Add and transform variables",
    "text": "3.5 mutate(): Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our data set in various ways (i.e., remove rows or columns from the original data set). But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate().\n\n\n\n\n\n\n\n\nMutate (by Allison Horst)\n\n\n\nLet’s create a new variable, BMI calculated from existing columns – mass/height\\(^2\\)\n\nmutate(starwars, BMI = mass/height^2, .after = mass)\n\n\n\n\n\n\n name \n    height \n    mass \n    BMI \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    0.0026028 \n    blond \n    fair \n    blue \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n C-3PO \n    167 \n    75 \n    0.0026892 \n    NA \n    gold \n    yellow \n    112.0 \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n R2-D2 \n    96 \n    32 \n    0.0034722 \n    NA \n    white, blue \n    red \n    33.0 \n    none \n    masculine \n    Naboo \n    Droid \n  \n\n Darth Vader \n    202 \n    136 \n    0.0033330 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Leia Organa \n    150 \n    49 \n    0.0021778 \n    brown \n    light \n    brown \n    19.0 \n    female \n    feminine \n    Alderaan \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    0.0037874 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    0.0027548 \n    brown \n    light \n    blue \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n  \n\n R5-D4 \n    97 \n    32 \n    0.0034010 \n    NA \n    white, red \n    red \n    NA \n    none \n    masculine \n    Tatooine \n    Droid \n  \n\n Biggs Darklighter \n    183 \n    84 \n    0.0025083 \n    black \n    light \n    brown \n    24.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    0.0023246 \n    auburn, white \n    fair \n    blue-gray \n    57.0 \n    male \n    masculine \n    Stewjon \n    Human \n  \n\n\n\n\nBy default, the new variable will be tacked on to the end of the data set as the last column. Using .after or .before arguments allows you to place the new variable in the middle of the data set.\n\nWe can combine the mutate function with other variables such as ifelse().\n\nLet’s replace the species variable to indicate Human or Not Human.\n\nmutate(starwars, \n       species = ifelse(species == \"Human\", species, \"Not Human\")\n)\n\n\n\n\n\n\n name \n    height \n    mass \n    hair_color \n    skin_color \n    eye_color \n    birth_year \n    sex \n    gender \n    homeworld \n    species \n  \n\n\n Luke Skywalker \n    172 \n    77 \n    blond \n    fair \n    blue \n    19.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n C-3PO \n    167 \n    75 \n    NA \n    gold \n    yellow \n    112.0 \n    none \n    masculine \n    Tatooine \n    Not Human \n  \n\n R2-D2 \n    96 \n    32 \n    NA \n    white, blue \n    red \n    33.0 \n    none \n    masculine \n    Naboo \n    Not Human \n  \n\n Darth Vader \n    202 \n    136 \n    none \n    white \n    yellow \n    41.9 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Leia Organa \n    150 \n    49 \n    brown \n    light \n    brown \n    19.0 \n    female \n    feminine \n    Alderaan \n    Human \n  \n\n Owen Lars \n    178 \n    120 \n    brown, grey \n    light \n    blue \n    52.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Beru Whitesun lars \n    165 \n    75 \n    brown \n    light \n    blue \n    47.0 \n    female \n    feminine \n    Tatooine \n    Human \n  \n\n R5-D4 \n    97 \n    32 \n    NA \n    white, red \n    red \n    NA \n    none \n    masculine \n    Tatooine \n    Not Human \n  \n\n Biggs Darklighter \n    183 \n    84 \n    black \n    light \n    brown \n    24.0 \n    male \n    masculine \n    Tatooine \n    Human \n  \n\n Obi-Wan Kenobi \n    182 \n    77 \n    auburn, white \n    fair \n    blue-gray \n    57.0 \n    male \n    masculine \n    Stewjon \n    Human \n  \n\n\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. We will cover special data types like these in a few weeks!\n\nMutate and new challenges\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation and google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\n\nSource\n\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#summarize",
    "href": "03-data-cleaning-and-manipulation.html#summarize",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.6 summarize()\n",
    "text": "3.6 summarize()\n\nThe next verb is one that we’ve already implicitly seen in action: summarize() takes a data frame with potentially many rows of data and reduces it down to one row of data using some function.\n\n\n\n\n\n\n\nHere (in a trivial example), I compute the overall average height of a star war’s character.\n\nsummarize(starwars,\n          avg_height = mean(height, na.rm = T)\n          )\n\n# A tibble: 1 × 1\n  avg_height\n       <dbl>\n1       174.\n\n\nThe na.rm = T argument says to ignore/remove the missing (NA) values in calculating the average.\n\nThe real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#group_by-group-by-power",
    "href": "03-data-cleaning-and-manipulation.html#group_by-group-by-power",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.7 group_by() Group By + (?) = Power!",
    "text": "3.7 group_by() Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\n\n\n\n\nsplit my data set into smaller data sets - one for each day\ncompute summary values for each smaller data set\nput my summarized data back together into a single data set\n\nThis is known as the split-apply-combine “Group by: Split-Apply-Combine” (2022) or sometimes, map-reduce (Dean and Ghemawat 2008) strategy (though map-reduce is usually on specifically large data sets and performed in parallel).\nIn tidy parlance, group_by() is the verb that accomplishes the first task. summarize() accomplishes the second task and implicitly accomplishes the third as well.\n\nLet’s see how things change when we calculate the average height of star wars characters by their species.\n\nstarwars |> \n  group_by(species) |> \n  summarize(height = mean(height, na.rm = T))\n\n# A tibble: 38 × 2\n   species   height\n   <chr>      <dbl>\n 1 Aleena       79 \n 2 Besalisk    198 \n 3 Cerean      198 \n 4 Chagrian    196 \n 5 Clawdite    168 \n 6 Droid       131.\n 7 Dug         112 \n 8 Ewok         88 \n 9 Geonosian   183 \n10 Gungan      209.\n# ℹ 28 more rows\n\n\nThe next section Pipe Operator will introduce and talk about what the |> symbol is, this example is just hard to show without it!\n\n\n\n\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\nWhen you group_by() a variable, your result carries this grouping with it. summarize() will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#pipe",
    "href": "03-data-cleaning-and-manipulation.html#pipe",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "\n3.8 Pipe Operator",
    "text": "3.8 Pipe Operator\nThe powerhouse of the tidyverse package comes from the pipe operator. This specifies a sequence of operations (kind of like how we layered our graphics in ggplot2). The output from the previous line (often a subset) is automatically passed into the first argument of the next line (remember, data first! data =).\nThe native pipe operator is |>, but the magrittr pipe operator %>% was used up until recently (and still is often used!).\n\n\n\n\n\n\n\n\n\n\n\n\nThe keyboard shortcut for adding a pipe operator to your code is Ctrl/Cmd + Shift + M.\nHowever, if you want to use this shortcut for the native pipe, you need to change your global R settings:\nTools > Global Options > Code > checkbox Use native pipe operator, |>\n\n\n\n(required) Read more about the pipe operators at Workflow Pipes.\n\n\n\nLet’s combine all of our new skills with the pipe operator!\n\n\nUse filter() to subset our data to only Human’s and Droid’s\nUse mutate() to create the new variable, BMI,\nUse group_by() to create groups by species,\nUse summarize() to calculate the mean and standard deviation of BMI\n\nUse mutate() to calculate the average plus/minus one standard deviation.\n\nWe could either assign this new data set that has summary values of BMI by species or we could pipe the data set directly into a plot – recall the first argument for ggplot() is data =.\n\nstarwars |> \n  filter(species %in% c(\"Human\", \"Droid\")) |> \n  mutate(BMI = mass/height^2) |> \n  group_by(species) |> \n  summarize(avg_BMI = mean(BMI, na.rm = TRUE),\n            sd_BMI = sd(BMI, na.rm = TRUE)\n            ) |> \n  mutate(BMI_1sd_below = avg_BMI - sd_BMI,\n         BMI_1sd_above = avg_BMI + sd_BMI\n         ) |> \n  ggplot(aes(x = species, \n             y = avg_BMI)\n         ) +\n  geom_point() +\n  geom_errorbar(aes(ymin = BMI_1sd_below,\n                    ymax = BMI_1sd_above),\n                width = 0.2\n                ) +\n  labs(x = \"Species\", \n       subtitle = \"Average BMI\") +\n  theme(axis.title.y = element_blank())\n\n\n\n\nAs with ggplot, formatting your code so it is readable will help both you and me!"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#checkin3-1",
    "href": "03-data-cleaning-and-manipulation.html#checkin3-1",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Check-in 3.1: Data Wrangling",
    "text": "Check-in 3.1: Data Wrangling\nQ1: Arrange the pipeline\nWorking with the Palmer Penguins data set:\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <fct>, year <int>\n\n\nSuppose we would like to study how the ratio of penguin body mass to flipper size differs across the species. Arrange the following steps into an order that accomplishes this goal (assuming the steps are connected with a |> or a %>%).\narrange(med_mass_flipper_ratio)\ngroup_by(species)\npenguins\nsummarize(med_mass_flipper_ratio = median(mass_flipper_ratio))\nmutate(mass_flipper_ratio = body_mass_g / flipper_length_mm)\n\nQ2 - Q7: dplyr pipelines\nConsider the base R code below.\n\nmean(penguins[penguins$species == \"Adelie\", ]$body_mass_g, na.rm = T)\n\n[1] 3700.662\n\n\nFor each of the following dplyr pipelines, indicate which of the following is true:\n\nIt returns the exact same thing as the (above) base R code\nIt returns the correct information, but the wrong object type\nIt returns incorrect information\nIt returns an error\n\nQ2\n\npenguins  |>\n  filter(\"body_mass_g\") |>\n  pull(\"Adelie\") |>\n  mean(na.rm = T)\n\nQ3\n\npenguins |>\n  filter(species == \"Adelie\") |>\n  select(body_mass_g) |>\n  summarize(mean(body_mass_g, na.rm = T))\n\nQ4\n\npenguins |>\n  pull(body_mass_g) |>\n  filter(species == \"Adelie\") |>\n  mean(na.rm = T)\n\nQ5\n\npenguins |>\n  filter(species == \"Adelie\") |>\n  select(body_mass_g) |>\n  mean(na.rm = T)\n\nQ6\n\npenguins |>\n  filter(species == \"Adelie\") |>\n  pull(body_mass_g) |>\n  mean(na.rm = T)\n\nQ7\n\npenguins |>\n  select(species == \"Adelie\") |>\n  filter(body_mass_g) |>\n  summarize(mean(body_mass_g, na.rm = T))"
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#additional-resources",
    "href": "03-data-cleaning-and-manipulation.html#additional-resources",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nModern Dive: Data Wrangling\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#pa-3-identify-the-mystery-college",
    "href": "03-data-cleaning-and-manipulation.html#pa-3-identify-the-mystery-college",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "PA 3: Identify the Mystery College",
    "text": "PA 3: Identify the Mystery College\nToday you will be using the dplyr package to clean a data set and then using that cleaned data set to figure out what college Ephelia has been accepted to.\nVisit PA 3: Identify the Mystery College for instructions.\nSubmit the full name of the college Ephelia will attend to the Canvas Quiz."
  },
  {
    "objectID": "03-data-cleaning-and-manipulation.html#references",
    "href": "03-data-cleaning-and-manipulation.html#references",
    "title": "\n3  Data Cleaning and Manipulation\n",
    "section": "References",
    "text": "References\n\n\n\n\nDean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” Communications of the ACM 51 (1): 107–13. https://doi.org/10.1145/1327452.1327492.\n\n\n“Group by: Split-Apply-Combine.” 2022. In Pandas 1.4.3 Documentation. Python. https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40: 1–29."
  },
  {
    "objectID": "04-data-joins-and-transformations.html",
    "href": "04-data-joins-and-transformations.html",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "",
    "text": "Reading: 15 minute(s) at 200 WPM.\nVideos: 26 minutes."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#ch4-objectives",
    "href": "04-data-joins-and-transformations.html#ch4-objectives",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "Objectives",
    "text": "Objectives\nBroadly, your objective while reading this chapter is to be able to identify data sets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should be master the following concepts:\n\nDetermine what data format is necessary to generate a desired plot or statistical model\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures\nUnderstand relational data formats and how to use data joins to assemble data from multiple tables into a single table.\n\n\nFunctions covered this week:\nlibrary(tidyr)\n\n\npivot_longer(), pivot_wider()\n\n\nseparate(), unite()\n\n\nlibrary(dplyr)\n\n\nleft_join(), right_join(), full_join()\n\n\nsemi_join(), anti_join()"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#ch4-checkins",
    "href": "04-data-joins-and-transformations.html#ch4-checkins",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are two check-ins for this week:\n\nCheck-in 4.1: Pivoting Cereal\nCheck-in 4.2: Practice with Joins"
  },
  {
    "objectID": "04-data-joins-and-transformations.html#identifying-the-problem-messy-data",
    "href": "04-data-joins-and-transformations.html#identifying-the-problem-messy-data",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.1 Identifying the problem: Messy data",
    "text": "4.1 Identifying the problem: Messy data\nThe illustrations below are lifted from an excellent blog post (Lowndes and Horst 2020) about tidy data; they’re reproduced here because\n\n\n\n\n\n\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\nFigure 4.1: Tidy data format, illustrated.\n\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the dataset, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own dataset.\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\n\nFigure 4.2: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\n\nExamples: Messy Data\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\nFor each of the data set, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\nAll of these data sets are “built-in” to the tidyverse package\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\n\nTable 1\n \n country \n    year \n    cases \n    population \n  \n\n\n Afghanistan \n    1999 \n    745 \n    19987071 \n  \n\n Afghanistan \n    2000 \n    2666 \n    20595360 \n  \n\n Brazil \n    1999 \n    37737 \n    172006362 \n  \n\n Brazil \n    2000 \n    80488 \n    174504898 \n  \n\n China \n    1999 \n    212258 \n    1272915272 \n  \n\n China \n    2000 \n    213766 \n    1280428583 \n  \n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\n\nTable 2\n \n country \n    year \n    type \n    count \n  \n\n\n Afghanistan \n    1999 \n    cases \n    745 \n  \n\n Afghanistan \n    1999 \n    population \n    19987071 \n  \n\n Afghanistan \n    2000 \n    cases \n    2666 \n  \n\n Afghanistan \n    2000 \n    population \n    20595360 \n  \n\n Brazil \n    1999 \n    cases \n    37737 \n  \n\n Brazil \n    1999 \n    population \n    172006362 \n  \n\n Brazil \n    2000 \n    cases \n    80488 \n  \n\n Brazil \n    2000 \n    population \n    174504898 \n  \n\n China \n    1999 \n    cases \n    212258 \n  \n\n China \n    1999 \n    population \n    1272915272 \n  \n\n China \n    2000 \n    cases \n    213766 \n  \n\n China \n    2000 \n    population \n    1280428583 \n  \n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\n\nTable 3\n \n country \n    year \n    rate \n  \n\n\n Afghanistan \n    1999 \n    745/19987071 \n  \n\n Afghanistan \n    2000 \n    2666/20595360 \n  \n\n Brazil \n    1999 \n    37737/172006362 \n  \n\n Brazil \n    2000 \n    80488/174504898 \n  \n\n China \n    1999 \n    212258/1272915272 \n  \n\n China \n    2000 \n    213766/1280428583 \n  \n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\n\nTable 4a\n \n country \n    1999 \n    2000 \n  \n\n\n Afghanistan \n    745 \n    2666 \n  \n\n Brazil \n    37737 \n    80488 \n  \n\n China \n    212258 \n    213766 \n  \n\n\n\n\n\n\nTable 4b\n \n country \n    1999 \n    2000 \n  \n\n\n Afghanistan \n    19987071 \n    20595360 \n  \n\n Brazil \n    172006362 \n    174504898 \n  \n\n China \n    1272915272 \n    1280428583 \n  \n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\n\nTable 5\n \n country \n    century \n    year \n    rate \n  \n\n\n Afghanistan \n    19 \n    99 \n    745/19987071 \n  \n\n Afghanistan \n    20 \n    00 \n    2666/20595360 \n  \n\n Brazil \n    19 \n    99 \n    37737/172006362 \n  \n\n Brazil \n    20 \n    00 \n    80488/174504898 \n  \n\n China \n    19 \n    99 \n    212258/1272915272 \n  \n\n China \n    20 \n    00 \n    213766/1280428583 \n  \n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n:::\n\nBy the end of this chapter, you will have the skills needed to wrangle the most common “messy” data sets into “tidy” form."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#pivot-operations",
    "href": "04-data-joins-and-transformations.html#pivot-operations",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.2 Pivot Operations",
    "text": "4.2 Pivot Operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\nThe two operations we’ll learn here are wide -> long and long -> wide.\n\nThis animation uses the functions pivot_wider() and pivot_longer() from the tidyr package in R – Animation source.\n\n4.2.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\n\n\n\n\n\n\nPicture the Operation\npivot_longer()\n\n\n\nTables 4a and 4b (from above) are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  <chr>            <dbl>      <dbl>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntable4a |> \n  pivot_longer(cols = `1999`:`2000`, \n               names_to = \"year\", \n               values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\ntable4b |> \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"population\")\n\n# A tibble: 6 × 3\n  country     year  population\n  <chr>       <chr>      <dbl>\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables (columns) we want to pivot (see table4a pivot)\n\nselect variables (columns) we don’t want to pivot, using - to remove them (see table4b pivot)\n\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\n\n\n\n\n4.2.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\n\n\n\n\n\n\nPicture the Operation\npivot_wider()\n\n\n\nTable 2 (from above) is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\n\ntable2 |>\n  pivot_wider(names_from  = type, \n              values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\nLearn More in R4DS\nRead more about pivoting in r4ds.\n\nCheck-in 4.1: Pivoting Cereal\nLoad in the cereal data set:\n\nlibrary(liver)\ndata(cereal)\nhead(cereal)\n\n                       name manuf type calories protein fat sodium fiber carbo\n1                 100% Bran     N cold       70       4   1    130  10.0   5.0\n2         100% Natural Bran     Q cold      120       3   5     15   2.0   8.0\n3                  All-Bran     K cold       70       4   1    260   9.0   7.0\n4 All-Bran with Extra Fiber     K cold       50       4   0    140  14.0   8.0\n5            Almond Delight     R cold      110       2   2    200   1.0  14.0\n6   Apple Cinnamon Cheerios     G cold      110       2   2    180   1.5  10.5\n  sugars potass vitamins shelf weight cups   rating\n1      6    280       25     3      1 0.33 68.40297\n2      8    135        0     3      1 1.00 33.98368\n3      5    320       25     3      1 0.33 59.42551\n4      0    330       25     3      1 0.50 93.70491\n5      8     -1       25     3      1 0.75 34.38484\n6     10     70       25     1      1 0.75 29.50954\n\n\nCreate a new data set called cereals_long, that has three columns:\n\nThe name of the cereal\nA column called Nutrient with values “protein”, “fat”, or “fiber”.\nA column called Amount with the corresponding amount of the nutrient.\n\nComplete the code template to complete the task in the Canvas Quiz."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#separating-and-uniting-variables",
    "href": "04-data-joins-and-transformations.html#separating-and-uniting-variables",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.3 Separating and Uniting Variables",
    "text": "4.3 Separating and Uniting Variables\nWe will talk about strings and regular expressions next week, but there’s a task that is fairly commonly encountered with functions that belong to the tidyr package: separating variables into two different columns separate() and it’s complement, unite(), which is useful for combining two variables into one column.\n\n\nseparate()\nunite()\n\n\n\n\n\n\n\nA visual representation of what separating variables means for data set operations.\n\n\n\n\n\ntable3 |>\n  separate(col    = rate,\n           into   = c(\"cases\", \"population\"),\n           sep    = \"/\",\n           remove = F\n           )\n\n# A tibble: 6 × 5\n  country      year rate              cases  population\n  <chr>       <dbl> <chr>             <chr>  <chr>     \n1 Afghanistan  1999 745/19987071      745    19987071  \n2 Afghanistan  2000 2666/20595360     2666   20595360  \n3 Brazil       1999 37737/172006362   37737  172006362 \n4 Brazil       2000 80488/174504898   80488  174504898 \n5 China        1999 212258/1272915272 212258 1272915272\n6 China        2000 213766/1280428583 213766 1280428583\n\n\nI’ve left the rate column in the original data frame (remove = F) just to make it easy to compare and verify that yes, it worked.\n\n\nAnd, of course, there is a complementary operation, which is when it’s necessary to join two columns to get a useable data value.\n\n\n\n\nA visual representation of what uniting variables means for data set operations.\n\n\n\n\n\ntable5 |>\n  unite(col = \"year\",\n        c(century, year),\n        sep = ''\n        )\n\n# A tibble: 6 × 3\n  country     year  rate             \n  <chr>       <chr> <chr>            \n1 Afghanistan 1999  745/19987071     \n2 Afghanistan 2000  2666/20595360    \n3 Brazil      1999  37737/172006362  \n4 Brazil      2000  80488/174504898  \n5 China       1999  212258/1272915272\n6 China       2000  213766/1280428583\n\n\n\n\n\n\nLearn More in R4DS\nRead more about separate() and unite() in r4ds."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#merging-tables",
    "href": "04-data-joins-and-transformations.html#merging-tables",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "\n4.4 Merging Tables",
    "text": "4.4 Merging Tables\nThe final essential data tidying and transformation skill you need to acquire is joining tables. It is common for data to be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.\n\nExamples: Relational Data Example: Primary School Records\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n\nWe could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment\nIt would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren’t well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nkeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs\n\n\n4.4.1 Animating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\nMutating Joins\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins.\n\n\nInner Join\nLeft Join\nRight Join\nFull Join\n\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\nDemonstration: Mutating Joins\n\nt1 <- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 <- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n\n# A tibble: 2 × 3\n  x         y     z\n  <chr> <dbl> <dbl>\n1 B         2     2\n2 D         3     5\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\nleft_join(t1, t2)\n\n# A tibble: 3 × 3\n  x         y     z\n  <chr> <dbl> <dbl>\n1 A         1    NA\n2 B         2     2\n3 D         3     5\n\nleft_join(t2, t1)\n\n# A tibble: 3 × 3\n  x         z     y\n  <chr> <dbl> <dbl>\n1 B         2     2\n2 C         4    NA\n3 D         5     3\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n\n# A tibble: 3 × 3\n  x         y     z\n  <chr> <dbl> <dbl>\n1 B         2     2\n2 D         3     5\n3 C        NA     4\n\nright_join(t2, t1)\n\n# A tibble: 3 × 3\n  x         z     y\n  <chr> <dbl> <dbl>\n1 B         2     2\n2 D         5     3\n3 A        NA     1\n\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join.\n\nfull_join(t1, t2)\n\n# A tibble: 4 × 3\n  x         y     z\n  <chr> <dbl> <dbl>\n1 A         1    NA\n2 B         2     2\n3 D         3     5\n4 C        NA     4\n\n\n\nI’ve included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins here.\nFiltering Joins\n\n\nSemi Join\nAnti Join\n\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\n\nLearn More in R4DS\nRead more about joins in r4ds\n\nCheck-in 4.2: Practice with Joins\nThe following code creates three data sets:\n\nprof_info <- data.frame(\n  professor =\n    c(\"Bodwin\", \"Glanz\", \"Carlton\", \"Sun\", \"Robinson\"),\n  undergrad_school =\n    c(\"Harvard\", \"Cal Poly\", \"Berkeley\", \"Harvard\", \"Winona State University\"),\n  grad_school =\n    c(\"UNC\", \"Boston University\", \"UCLA\", \"Stanford\", \"University of Nebraska-Lincoln\")\n)\n\nprof_course <- data.frame(\n  professor = c(\"Bodwin\", \"Glanz\", \"Carlton\", \"Theobold\", \"Robinson\"),\n  Stat_331 = c(TRUE, TRUE, TRUE, TRUE, TRUE),\n  Stat_330 = c(FALSE, TRUE, TRUE, FALSE, TRUE),\n  Stat_431 = c(TRUE, TRUE, FALSE, TRUE, FALSE)\n)\n\ncourse_info <- data.frame(\n  course = c(\"Stat_331\", \"Stat_330\", \"Stat_431\"),\n  num_sections = c(8, 3, 1)\n)\n\nHere is what they look like once created:\n\nprof_info\n\n  professor        undergrad_school                    grad_school\n1    Bodwin                 Harvard                            UNC\n2     Glanz                Cal Poly              Boston University\n3   Carlton                Berkeley                           UCLA\n4       Sun                 Harvard                       Stanford\n5  Robinson Winona State University University of Nebraska-Lincoln\n\n\n\nprof_course\n\n  professor Stat_331 Stat_330 Stat_431\n1    Bodwin     TRUE    FALSE     TRUE\n2     Glanz     TRUE     TRUE     TRUE\n3   Carlton     TRUE     TRUE    FALSE\n4  Theobold     TRUE    FALSE     TRUE\n5  Robinson     TRUE     TRUE    FALSE\n\n\n\ncourse_info\n\n    course num_sections\n1 Stat_331            8\n2 Stat_330            3\n3 Stat_431            1\n\n\nThese data sets contain information about five Cal Poly professors, their educational history, the classes they are able to teach, and the number of sections of each class that need to be assigned.\n\n\nQuestion 1 Combine data sets prof_info and prof_course to make this data set:\n\n\n\n\n\n\n professor \n    undergrad_school \n    grad_school \n    Stat_331 \n    Stat_330 \n    Stat_431 \n  \n\n\n Bodwin \n    Harvard \n    UNC \n    TRUE \n    FALSE \n    TRUE \n  \n\n Glanz \n    Cal Poly \n    Boston University \n    TRUE \n    TRUE \n    TRUE \n  \n\n Carlton \n    Berkeley \n    UCLA \n    TRUE \n    TRUE \n    FALSE \n  \n\n Robinson \n    Winona State University \n    University of Nebraska-Lincoln \n    TRUE \n    TRUE \n    FALSE \n  \n\n\n\n\n\n\nQuestion 2 Combine data sets prof_info and prof_course to make this data set:\n\n\n\n\n\n\n professor \n    undergrad_school \n    grad_school \n    Stat_331 \n    Stat_330 \n    Stat_431 \n  \n\n\n Bodwin \n    Harvard \n    UNC \n    TRUE \n    FALSE \n    TRUE \n  \n\n Glanz \n    Cal Poly \n    Boston University \n    TRUE \n    TRUE \n    TRUE \n  \n\n Carlton \n    Berkeley \n    UCLA \n    TRUE \n    TRUE \n    FALSE \n  \n\n Sun \n    Harvard \n    Stanford \n    NA \n    NA \n    NA \n  \n\n Robinson \n    Winona State University \n    University of Nebraska-Lincoln \n    TRUE \n    TRUE \n    FALSE \n  \n\n\n\n\n\n\nQuestion 3 Transform and combine data sets prof_course and course_info to make this data set:\n\n\n\n\n\n\n professor \n    course \n    can_teach \n    num_sections \n  \n\n\n Bodwin \n    Stat_331 \n    TRUE \n    8 \n  \n\n Bodwin \n    Stat_330 \n    FALSE \n    3 \n  \n\n Bodwin \n    Stat_431 \n    TRUE \n    1 \n  \n\n Glanz \n    Stat_331 \n    TRUE \n    8 \n  \n\n Glanz \n    Stat_330 \n    TRUE \n    3 \n  \n\n Glanz \n    Stat_431 \n    TRUE \n    1 \n  \n\n Carlton \n    Stat_331 \n    TRUE \n    8 \n  \n\n Carlton \n    Stat_330 \n    TRUE \n    3 \n  \n\n Carlton \n    Stat_431 \n    FALSE \n    1 \n  \n\n Theobold \n    Stat_331 \n    TRUE \n    8 \n  \n\n Theobold \n    Stat_330 \n    FALSE \n    3 \n  \n\n Theobold \n    Stat_431 \n    TRUE \n    1 \n  \n\n Robinson \n    Stat_331 \n    TRUE \n    8 \n  \n\n Robinson \n    Stat_330 \n    TRUE \n    3 \n  \n\n Robinson \n    Stat_431 \n    FALSE \n    1 \n  \n\n\n\n\nComplete the code templates to complete these tasks in the Canvas Quiz."
  },
  {
    "objectID": "04-data-joins-and-transformations.html#pa-4-government-spending",
    "href": "04-data-joins-and-transformations.html#pa-4-government-spending",
    "title": "\n4  Data Joins and Transformations\n",
    "section": "PA 4: Government Spending",
    "text": "PA 4: Government Spending\nThis week you will be tidying untidy data to explore the relationship between countries of the world and military spending.\nVisit PA 4: Government Spending for instructions.\nSubmit your answers to associated questions to the Canvas quiz.\n\nWhat four regions were NOT removed from the military_clean data set?\nWhat year was the second largest military expenditure? What country had this expenditure?"
  },
  {
    "objectID": "05-special-data-types.html",
    "href": "05-special-data-types.html",
    "title": "\n5  Special Data Types\n",
    "section": "",
    "text": "Reading: 21 minute(s) at 200 WPM + r4ds required readings\nVideos: 32 minutes"
  },
  {
    "objectID": "05-special-data-types.html#ch5-objectives",
    "href": "05-special-data-types.html#ch5-objectives",
    "title": "\n5  Special Data Types\n",
    "section": "Objectives",
    "text": "Objectives\nThis chapter is heavily outsourced to r4ds as they do a much better job at providing examples and covering the extensive functionality of each of the packages than I myself would ever be able to.\n\nUse forcats to reorder and relabel factor variables in data cleaning steps and data visualizations.\nClean and extract information from character strings using stringr\n\nWork with date and time variables using lubridate"
  },
  {
    "objectID": "05-special-data-types.html#ch5-checkins",
    "href": "05-special-data-types.html#ch5-checkins",
    "title": "\n5  Special Data Types\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are three check-ins for this week:\n\nCheck-in 5.1: Functions from forcats\nCheck-in 5.2: Functions from stringr\nCheck-in 5.3: Functions from lubridate\n\nFeel free to share answers with your classmates in the Discord server, to make sure you understand. Check-ins are meant for you to learn and practice!"
  },
  {
    "objectID": "05-special-data-types.html#factors-with-forcats",
    "href": "05-special-data-types.html#factors-with-forcats",
    "title": "\n5  Special Data Types\n",
    "section": "\n5.1 Factors with forcats\n",
    "text": "5.1 Factors with forcats\n\nWe have been floating around the idea of factor data types. In this chapter, we will formally define factors and why they are needed for data visualization and analysis. We will then learn useful functions for working with factors in our data cleaning steps.\n\n\n\n\n\n\nIn short, factors are categorical variables with a fixed number of values (think a set number of groups). One of the main features that set factors apart from groups is that you can reorder the groups to be non-alphabetical. In this section we will be using the forcats package (part of the tidyverse!) to create and manipulate factor variables.\n\n(Required) Go read about factors in r4ds. \n\n\nforcats\nUseful functions from the forcats package (not an exhaustive list):\n\n\nfactor(), levels()\n\nfact_relevel()\nfct_reorder()\nfct_collapse()\n\nDownload the forcats cheatsheet."
  },
  {
    "objectID": "05-special-data-types.html#checkin5-1",
    "href": "05-special-data-types.html#checkin5-1",
    "title": "\n5  Special Data Types\n",
    "section": "Check-in 5.1: Functions from forcats\n",
    "text": "Check-in 5.1: Functions from forcats\n\nAnswer the following questions in the Canvas Quiz.\n1. Which of the following tasks can fct_recode() accomplish?\n\nchanges the values of the factor levels\nreorders the levels of a factor\nremove levels of a factor you don’t want\ncollapse levels of a factor into a new level\n\n2. Which of the following tasks can fct_relevel() accomplish?\n\nreorders the levels of a factor\nchanges the values of the factor levels\nremove levels of a factor you don’t want\ncollapse levels of a factor into a new level\n\n3. What is the main difference between fct_collapse() and fct_recode()?\n\n\nfct_recode() uses strings to create factor levels\n\nfct_recode() uses groups to create factor levels\n\nfct_recode() cannot create an “Other” group\n\n4. What ordering do you get with fct_reorder()?\n\nlargest to smallest based on another variable\norder of appearance\nlargest to smallest based on counts\nalphabetical order\n\n5. What ordering do you get with fct_inorder()?\n\norder of appearance\nalphabetical order\nlargest to smallest based on counts\nlargest to smallest based on another variable"
  },
  {
    "objectID": "05-special-data-types.html#string-operations-with-stringr",
    "href": "05-special-data-types.html#string-operations-with-stringr",
    "title": "\n5  Special Data Types\n",
    "section": "\n5.2 String Operations with stringr\n",
    "text": "5.2 String Operations with stringr\n\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\n\n\n\n\n\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\nAlternately, the xkcd version of the above quote\n\n\n\n(required) Go read about strings in r4ds. \n\n\nstringr\nDownload the stringr cheatsheet.\n\nTable of string functions in the R stringr package. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\n\nTask\nstringr\n\n\n\nReplace pattern with replacement\n\n\nstr_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\n\n\nConvert case\n\nstr_to_lower(x), str_to_upper(x) , str_to_title(x)\n\n\n\nStrip whitespace from start/end\n\nstr_trim(x) , str_squish(x)\n\n\n\nPad strings to a specific length\nstr_pad(x, …)\n\n\nTest if the string contains a pattern\nstr_detect(x, pattern)\n\n\nCount how many times a pattern appears in the string\nstr_count(x, pattern)\n\n\nFind the first appearance of the pattern within the string\nstr_locate(x, pattern)\n\n\nFind all appearances of the pattern within the string\nstr_locate_all(x, pattern)\n\n\nDetect a match at the start/end of the string\n\nstr_starts(x, pattern) ,str_ends(x, pattern)\n\n\n\nSubset a string from index a to b\nstr_sub(x, a, b)\n\n\nConvert string encoding\nstr_conv(x, encoding)\n\n\n\n5.2.1 Regular Expressions\nMatching exact strings is easy - it’s just like using find and replace.\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears <- str_extract(human_talk, \"walk\")\ndog_hears\n\n[1] \"walk\"\n\n\nBut, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler.\n\nShort Regular Expressions Primer\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions. The subset of regular expression syntax we’re going to cover here is fairly limited, but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn <- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n\n[1] \"123-45-6789\"\n\n\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n\n* means repeat between 0 and inf times\n\n+ means 1 or more times\n\n? means 0 or 1 times – most useful when you’re looking for something optional\n\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times”\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n\n[1] \"banana\"\n\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n\n[1] \"ba\"\n\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n\n[[1]]\n[1] \"ba\" \"na\" \"na\"\n\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n\n[1] \"anan\"\n\n\n\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn <- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n\n[1] \"123-45-6789\"\n\nphone <- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n\n[1] \"123-456-7890\"\n\nnuid <- str_extract(num_string, \"[0-9]{8}\")\nnuid\n\n[1] \"12345678\"\n\nbank_balance <- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n\n[1] \"$50,000,000.23\"\n\n\nThere are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end)\n\n\n^ has multiple meanings:\n\nif it’s the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c”.\n\n\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\naddress <- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num <- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet <- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet <- str_remove(street, house_num) %>% str_trim() # remove house number\n\ncity <- str_extract(address, \",.*,\") %>% str_remove_all(\",\") %>% str_trim()\n\nzip <- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\n\na|b will select a or b.\n\nIf you’ve captured information using (), you can reference that information using backreferences. In most languages, those look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants <- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n\n[1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\n\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n\n[1] \"1234567980\" \"1234567890\" \"1234567890\"\n\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears <- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n\n[[1]]\n[1] \"walk\"  \"treat\"\n\n\nPutting it all together, we can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\nstrings <- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex <- \"(walk|treat)\"\naddr_regex <- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex <- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n\n# A tibble: 6 × 5\n  text                                                   phone dog   addr  abab \n  <chr>                                                  <lgl> <lgl> <lgl> <lgl>\n1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\n(semi-required) Go read more about regular expressions in r4ds.\nRead at least through section 17.4.1."
  },
  {
    "objectID": "05-special-data-types.html#checkin5-2",
    "href": "05-special-data-types.html#checkin5-2",
    "title": "\n5  Special Data Types\n",
    "section": "Check-in 5.2: Functions from stringr\n",
    "text": "Check-in 5.2: Functions from stringr\n\n1 Which of the follow are differences between length() and str_length()?\n\n\nlength() gives the number of elements in a vector\n\nstr_length() gives the number of characters in a string\n\nstr_length() gives the number of strings in a vector\n\nlength() gives the dimensions of a dataframe\n\n2 What of the following is true about str_replace()?\n\n\nstr_replace() replaces the first instance of the pattern\n\nstr_replace() replaces the last instance of the pattern\n\nstr_replace() replaces every instance of the pattern\n\n3 str_trim() allows you to remove whitespace on what sides\n\nboth\nleft\nright\n\n4 Which of the following does str_sub() use to create a substring?\n\nstarting position\nending position\npattern to search for\n\n5 Which of the following does str_subset() use to create a substring?\n\nstarting position\nending position\npattern to search for\n\n6 What does the collapse argument do in str_c()?\n\nspecifies a string to be used when combining inputs into a single string\nspecifies whether the string should be collapsed"
  },
  {
    "objectID": "05-special-data-types.html#dates-times-with-lubridate",
    "href": "05-special-data-types.html#dates-times-with-lubridate",
    "title": "\n5  Special Data Types\n",
    "section": "\n5.3 Dates & Times with lubridate\n",
    "text": "5.3 Dates & Times with lubridate\n\nIn order to fill in an important part of our toolbox, we need to learn how to work with date variables. These variables feel like they should be simple and intuitive given we all work with schedules and calendars everyday. However, there are little nuances that we will learn to make working with dates and times easier.\n\n\n\n\n\n\n\n(Required) GO read about dates and times in r4ds. \n\n\n\n\nlubridate website\nDownload the lubridate cheatsheet\n\n\n\nLearn more about dates and times\n\nA more in-depth discussion of the POSIXlt and POSIXct data classes.\nA tutorial on lubridate - scroll down for details on intervals if you have trouble with %within% and %–%"
  },
  {
    "objectID": "05-special-data-types.html#checkin5-3",
    "href": "05-special-data-types.html#checkin5-3",
    "title": "\n5  Special Data Types\n",
    "section": "Check-in 5.3: Functions from lubridate\n",
    "text": "Check-in 5.3: Functions from lubridate\n\n1 Which of the following is true about the year() function?\n\n\nyear() extracts the year of a datetime object\n\nyear() creates a duration object to be added to a datetime\n\n2 What tz would you use for San Luis Obispo?\n\n3 Which of the following is true about the %within% operator?\n\nit checks if a date is included in an interval\nit returns a logical value\nit creates an interval with a start and end time\n\n4 Which of the following is true about the %--% operator?\n\nit creates an interval with a start and end time\nit returns a logical value\nit checks if a date is included in an interval\n\n5 What day of the month does the make_date() function use as default if no day argument is provided?"
  },
  {
    "objectID": "05-special-data-types.html#practice-activities",
    "href": "05-special-data-types.html#practice-activities",
    "title": "\n5  Special Data Types\n",
    "section": "Practice Activities",
    "text": "Practice Activities\nPA 5.1: Zodiac Killer\nOne of the most famous mysteries in California history is the identity of the so-called “Zodiac Killer”, who murdered 7 people in Northern California between 1968 and 1969. A new murder was committed last year in California, suspected to be the work of a new Zodiac Killer on the loose.\nUnfortunately, the date and time of the murder is not known. You have been hired to crack the case. Use the clues below to discover the murderer’s identity.\nVisit PA 5.1: Zodiac Killer for instructions.\n\nSubmit the name of the killer to the Canvas Quiz.\n\n\n\n\nWe will have two practice activities this week as opposed to our usual one. The first practice activity, PA 5.1 Zodiac Killer will be completed in class on Monday and due Wednesday morning. The second practice activity, PA 5.2 Scrambled Message will be completed in class on Wednesday and due Friday morning.\n\n\nPA 5.2: Scrambled Message\nIn this activity, you will be using regular expressions to decode a message.\nVisit PA 5.2: Scrambled Message for instructions.\n\nSubmit the movie name the quote contained in the secret message is from to the Canvas Quiz\n\nReferences"
  },
  {
    "objectID": "06-version-control.html",
    "href": "06-version-control.html",
    "title": "\n6  Version Control\n",
    "section": "",
    "text": "Reading: 15 minute(s) at 200 WPM\nVideos: 0 minute(s)"
  },
  {
    "objectID": "06-version-control.html#ch6-objectives",
    "href": "06-version-control.html#ch6-objectives",
    "title": "\n6  Version Control\n",
    "section": "Objectives",
    "text": "Objectives\nMost of this section is either heavily inspired by Happy Git and Github for the UseR (Bryan, Hester, and The Stat 545 TAs 2021) or directly links to that book.\n\nRecognize the benefits of using version control to improve your coding practices and workflow.\nIdentify git/GitHub as a version control platform (and helper).\nRegister for a GitHub account so you can begin applying version control practices to your workflow."
  },
  {
    "objectID": "06-version-control.html#ch6-checkins",
    "href": "06-version-control.html#ch6-checkins",
    "title": "\n6  Version Control\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are two check-ins for this week:\n\nCheck-in 6.1: Install Git\nCheck-in 6.2: Create a GitHub Account"
  },
  {
    "objectID": "06-version-control.html#what-is-version-control",
    "href": "06-version-control.html#what-is-version-control",
    "title": "\n6  Version Control\n",
    "section": "\n6.1 What is Version Control?",
    "text": "6.1 What is Version Control?\nVersion control is a system that (1) allows you to store your files in the cloud, (2) track change in those files over time, and (3) share your files with others.\nLearn more about version control\nIf you are unfamiliar with the idea of version control, this article describes what the principles of version control are."
  },
  {
    "objectID": "06-version-control.html#git",
    "href": "06-version-control.html#git",
    "title": "\n6  Version Control\n",
    "section": "\n6.2 Git",
    "text": "6.2 Git\n\n\n\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\nVersion control is the answer to this file naming problem.\n\n\nGit manages a collection of files in a structured way - rather like “track changes” in Microsoft Word or version history in Dropbox, but much more powerful.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R or _final_finalforreal.qmd to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this program is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with GitHub - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but GitHub is … essential.\n\n6.2.1 Git Basics\n\n\nIf that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything.\n\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently save over your old files, so long as you remember to commit frequently.\nEssential Reading: Git\nThe git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better.\nGo read Chapter 1, until it starts to become greek (aka over your head).\n\nCheck-in 6.1: Install Git\nWe will be working with Git/GitHub in groups during class this week. In order to be prepared, please install Git onto your computer – we will be connecting this to RStudio together, but having this installed ahead of time avoids installation time during class.\n\nInstall Git for Windows\nInstall Git for Macs\n\nIf you run into issues, read more about installing Git.\nOnce you have installed Git, tell me “yes” in the Canvas Quiz.\n Now that you have a general idea of how git works and why we might use it, let’s talk a bit about GitHub."
  },
  {
    "objectID": "06-version-control.html#setting-up-github",
    "href": "06-version-control.html#setting-up-github",
    "title": "\n6  Version Control\n",
    "section": "\n6.3 GitHub: Git on the Web",
    "text": "6.3 GitHub: Git on the Web\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git.\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don’t have in 3 different places is data you don’t care about.1\nCheck-in 6.2: Register a GitHub Account\nFollow the instructions in Registering a GitHub Account to create a free GitHub account.\nCopy and paste the link to your GitHub profile into the Canvas assignment.\n\nYour GitHub profile link should look like – https://github.com/USERNAME\nHere is mine! https://github.com/earobinson95\n\n\n(Optional) You may want to check out GitHub Education and sign up for the GitHub Student Developer Pack.\n\nSave your login information!\nMake sure you remember your username and password so you don’t have to try to hack into your own account during class this week.\nWrite your information down somewhere safe.\nOptional: Install a git client\nInstructions\n\nI personally like to use GitHub Desktop which allows me to interact with Git using a point-and-click interface."
  },
  {
    "objectID": "06-version-control.html#using-version-control-with-rstudio",
    "href": "06-version-control.html#using-version-control-with-rstudio",
    "title": "\n6  Version Control\n",
    "section": "\n6.4 Using Version Control (with RStudio)",
    "text": "6.4 Using Version Control (with RStudio)\nThis course will briefly introduced working with GitHub, but will not provide you with extensive practice using version control. By using version control, you will learn better habits for programming, and you’ll get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\nIn class this week, we will connect git/GitHub to RStudio so you can use version control for your code. We will then see what a typical git/GitHub workflow looks like."
  },
  {
    "objectID": "06-version-control.html#learn-more",
    "href": "06-version-control.html#learn-more",
    "title": "\n6  Version Control\n",
    "section": "Learn More",
    "text": "Learn More\nExtra Resources\n\nHappy Git and GitHub for the useR - Guide to using git, R, and RStudio together. (Bryan, Hester, and The Stat 545 TAs 2021)\nGit “Hello World” Tutorial on GitHub\nCrash course on git (30 minute YouTube video) (Traversy Media 2017)\nGit and GitHub for poets YouTube playlist (this is supposed to be the best introduction to Git out there…) (The Coding Train 2016)\nMore advanced git concepts, in comic form, by Erika Heidi (Erica Heidi 2020)\nA quick guide to the command line (Terminal) (Wei 2019)"
  },
  {
    "objectID": "07-functions.html",
    "href": "07-functions.html",
    "title": "\n7  Writing Functions\n",
    "section": "",
    "text": "Reading: 18 minute(s) at 200 WPM\nVideos: 20 minute(s)\nA function is a set of actions that we group together and name. Throughout this course, you’ve used a bunch of different functions in R that are built into the language or added through packages: mean, ggplot, filter. In this chapter, we’ll be writing our own functions."
  },
  {
    "objectID": "07-functions.html#ch7-objectives",
    "href": "07-functions.html#ch7-objectives",
    "title": "\n7  Writing Functions\n",
    "section": "Objectives",
    "text": "Objectives\n\nWrite your own functions in R\nMake good decisions about function arguments and returns\nInclude side effects and / or error messages in your functions\nExtend your use of good R coding style"
  },
  {
    "objectID": "07-functions.html#ch7-checkins",
    "href": "07-functions.html#ch7-checkins",
    "title": "\n7  Writing Functions\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere is one check-in for this week:\n\nCheck-in 7.1: Writing Functions"
  },
  {
    "objectID": "07-functions.html#when-to-write-a-function",
    "href": "07-functions.html#when-to-write-a-function",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.1 When to write a function?",
    "text": "7.1 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead of copy pasting. The motivation behind this is the “don’t repeat yourself” (DRY) principle (Wickham and Grolemund 2016). There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\n\n\nLearn more in R4DS\nThere is some extensive material on this subject in R for Data Science on functions. If you want to really understand how functions work in R, that is a good place to go.\nIf you are interested in reading about “best practices”, I recommend reading Best Practices for Scientific Computing (Wilson et al. 2014).\n\n\nThis example is modified from R for Data Science (Wickham and Grolemund 2016, chap. 19).\nWhat does this code do? Does it work as intended?\n\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\nTo convert the code into a function, we first rewrite it using general names\nIn this case, it might help to replace df$a with x.\n\nx <- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n\n [1] 0.7049782 1.0000000 0.2148165 0.4259105 0.5291736 0.8765509 0.2173011\n [8] 0.2523844 0.8106021 0.0000000\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng <- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n\n [1] 0.7049782 1.0000000 0.2148165 0.4259105 0.5291736 0.8765509 0.2173011\n [8] 0.2523844 0.8106021 0.0000000\n\n\nFinally, we turn this code into a function\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n\n [1] 0.7049782 1.0000000 0.2148165 0.4259105 0.5291736 0.8765509 0.2173011\n [8] 0.2523844 0.8106021 0.0000000\n\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and }\n\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function."
  },
  {
    "objectID": "07-functions.html#syntax",
    "href": "07-functions.html#syntax",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.2 Syntax",
    "text": "7.2 Syntax\n\n\nR syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted.\n\n\n\n\n\n\n\n\nIn R, functions are defined (or assigned names) the same as other variables, using <-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned."
  },
  {
    "objectID": "07-functions.html#arguments-and-parameters",
    "href": "07-functions.html#arguments-and-parameters",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.3 Arguments and Parameters",
    "text": "7.3 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a puppy’s name and returns “ is a good pup!”.\n\ndog <- \"Eddie\"\n\ngoodpup <- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n\n[1] \"Eddie is a good pup!\"\n\n\nIn this example R function, when we call goodpup(dog), dog is the argument. name is the parameter.\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\nExample\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be.\n\n\nFunction\nAnswer\n\n\n\n\nmy_mean <- function(x) {\n  censor_x <- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nmy_mean(1:10)\n\n\n\n\nFunction name: my_mean\n\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: an average value for the censor_x numerical vector (varies each time the function is run unless you set the seed)\n\n\n\nset.seed(3420523)\nmy_mean(1:10)\n\n[1] 6\n\n\n\n\n\n\n7.3.1 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\ndivide <- function(x, y) {\n  x / y\n}\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\ndivide(3, 6)\n\n[1] 0.5\n\ndivide(x = 3, y = 6)\n\n[1] 0.5\n\ndivide(y = 6, x = 3)\n\n[1] 0.5\n\ndivide(6, 3)\n\n[1] 2\n\ndivide(x = 6, y = 3)\n\n[1] 2\n\ndivide(y = 3, x = 6)\n\n[1] 2\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters."
  },
  {
    "objectID": "07-functions.html#input-validation",
    "href": "07-functions.html#input-validation",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.4 Input Validation",
    "text": "7.4 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() or if(){ } else{ } and then use stop(\"better error message\") in the body of the if or else statement.\n\n\nstopifnot()\nif(){ } else { }\nif(){stop()}\n\n\n\n\nadd <- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n\nError in x + y: non-numeric argument to binary operator\n\nadd <- function(x, y) {\n  stopifnot(is.numeric(x), \n            is.numeric(y)\n            )\n  x + y\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): is.numeric(x) is not TRUE\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\n\nadd <- function(x, y) {\n  x + y\n}\n\nadd <- function(x, y) {\n  if(is.numeric(x) & is.numeric(y)) {\n    x + y\n  } else {\n    stop(\"Argument input for x or y is not numeric\")\n  }\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): Argument input for x or y is not numeric\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\n\nadd <- function(x, y) {\n  x + y\n}\n\nadd <- function(x, y) {\n  if(!is.numeric(x) | !is.numeric(y)) {\n    stop(\"Argument input for x or y is not numeric\")\n  }\n    x + y\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): Argument input for x or y is not numeric\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused. If you’re interested, Wikipedia has more about defensive programming."
  },
  {
    "objectID": "07-functions.html#scope",
    "href": "07-functions.html#scope",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.5 Scope",
    "text": "7.5 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\n\n\n\n\n\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\n\n7.5.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\nWhat does this function return, 10 or 20?\n\n\nPseudocode\nSketch\nR Function\n\n\n\na <- 10\n\nmyfun <- function() {\n  a <- 20\n  a\n}\n\nmyfun()\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\n\n\n\n\na <- 10\n\nmyfun <- function() {\n  a <- 20\n  a\n}\n\nmyfun()\n\n[1] 20\n\na\n\n[1] 10\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces. Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n7.5.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\n\nPseudocode\nSketch\nR Function\n\n\n\nmyfun <- function() {\n  if aa is not defined\n    aa <- 1\n  else\n    aa <- aa + 1\n}\n\nmyfun()\nmyfun()\n\nWhat does this output?\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\n\nmyfun <- function() {\n  if (!exists(\"aa\")) {\n    aa <- 1\n  } else {\n    aa <- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n\n[1] 1\n\nmyfun()\n\n[1] 1\n\n\n\n\n\n\n\n7.5.3 Dynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\n\nPseudocode\nSketch\nR\n\n\n\nmyfun <- function(){\n  x + 1\n}\n\nx <- 14\n\nmyfun()\n\nx <- 20\n\nmyfun()\n\nWhat will the output be of this code?\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\n\nmyfun <- function() {\n  x + 1\n}\n\nx <- 14\n\nmyfun()\n\n[1] 15\n\nx <- 20\n\nmyfun()\n\n[1] 21\n\n\n\n\n\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. (Taken from (Wickham 2015, chap. 6))\n\n\nCode\nSolution\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n[1] 202\n\n\n\n\n\n\n\n\n7.5.4 Tutorial - Writing Functions\nVisitRStudio Primer - Writing Functions\nI highly recommend you work through every tutorial except Loops!"
  },
  {
    "objectID": "07-functions.html#debugging",
    "href": "07-functions.html#debugging",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.6 Debugging",
    "text": "7.6 Debugging\nNow that you’re writing functions, it’s time to talk a bit about debugging techniques. This is a lifelong topic - as you become a more advanced programmer, you will need to develop more advanced debugging skills as well (because you’ll become more adept at screwing things up).\n\n\nThe faces of debugging (by Allison Horst)\n\n\nLet’s start with the basics: print debugging.\n\n7.6.1 Print Debugging\nThis technique is basically exactly what it sounds like. You insert a ton of print statements to give you an idea of what is happening at each step of the function.\nLet’s try it out on the previous example (see what I did there?)\n\nNote that I’ve modified the code slightly so that we store the value into returnval and then return it later - this allows us to see the code execution without calling functions twice (which would make the print output a bit more confusing).\n\nf <- function(x) {\n  print (\"Entering Outer Function\")\n  print (paste(\"x =\", x))\n  f <- function(x) {\n    print (\"Entering Middle Function\")\n    print (paste(\"x = \", x))\n    f <- function() {\n      print (\"Entering Inner Function\")\n      print (paste(\"x = \", x))\n      print (paste(\"Inner Function: Returning\", x^2))\n      x ^ 2\n    }\n    returnval <- f() + 1\n    print (paste(\"Middle Function: Returning\", returnval))\n    returnval\n  }\n  returnval <- f(x) * 2\n  print (paste(\"Outer Function: Returning\", returnval))\n  returnval\n}\nf(10)\n\n[1] \"Entering Outer Function\"\n[1] \"x = 10\"\n[1] \"Entering Middle Function\"\n[1] \"x =  10\"\n[1] \"Entering Inner Function\"\n[1] \"x =  10\"\n[1] \"Inner Function: Returning 100\"\n[1] \"Middle Function: Returning 101\"\n[1] \"Outer Function: Returning 202\"\n\n\n[1] 202\n\n\n\n\n7.6.2 General Debugging Strategies\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nThe overall process is well described in Advanced R by H. Wickham1; I’ve copied it here because it’s such a succinct distillation of the process, but I’ve adapted some of the explanations to this class rather than the original context of package development.\n\nRealize that you have a bug\nGoogle! In R you can automate this with the errorist and searcher packages, but general Googling the error + the programming language + any packages you think are causing the issue is a good strategy.\n\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while.\nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation.\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed <-- instead of <- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck2. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. A more thorough explanation can be found at gitduck.com.\n\n\n\nYou may find it helpful to procure a rubber duck expert for each language you work in. I use color-your-own rubber ducks to endow my ducks with certain language expertise. Other people use plain rubber ducks and give them capes.\n\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known it has an xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea.\n\nThese next two sections are included as FYI, but you don’t have to read them just now. They’re important, but not urgent, if that makes sense.\n\nDividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solveable.\nWe’ll start with a non-programming example:\n\n\nGeneral problem statement : “I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\nSpecific problem statement: “I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\n\nSeparating things into solvable problems:\nMoving through the list above, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\n\n\nBrainstorm Solutions:\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\n\n\nApproach each sub-problem separately\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfulness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work.\n\n\nHere’s another example of how to break down a real-world personal problem in programming/debugging style.\nMinimal Working (or Reproducible) Examples\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\n\nSo, with that said, there are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\n\nminimal - as little code as possible to still reproduce the problem\n\ncomplete - everything necessary to reproduce the issue is contained in the description/question\n\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into an R script or notebook (e.g. Quarto document code chunk). Describe what you see and what you’d hope to see if the code were working.\nOther resources:\n\nreprex package: Do’s and Don’ts\n\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney"
  },
  {
    "objectID": "07-functions.html#styling-functions",
    "href": "07-functions.html#styling-functions",
    "title": "\n7  Writing Functions\n",
    "section": "\n7.7 Styling Functions",
    "text": "7.7 Styling Functions\nPart of writing reproducible and shareable code is following good style guidelines. Mostly, this means choosing good object names and using white space in a consistent and clear way.\nYou should have already seen the sections of the Tidyverse Style Guide relevant to piping, plotting, and naming objects. This week we are extending these style guides to functions.\n\nI would highly recommend reading through the style guide for naming functions, what to do with long lines, and the use of comments.\nRead the tidyverse style guide for functions.\n\nIn summary, designing functions is somewhat subjective, but there are a few principles that apply:\n\nChoose a good, descriptive names\n\n\n\nYour function name should describe what it does, and usually involves a verb.\nYour argument names should be simple and / or descriptive.\nNames of variables in the body of the function should be descriptive.\n\n\nOutput should be very predictable\n\n\n\nYour function should always return the same object type, no matter what input it gets.\nYour function should expect certain objects or object types as input, and give errors when it does not get them.\nYour function should give errors or warnings for common mistakes.\nDefault values of arguments should only be used when there is a clear common choice.\n\n\nThe body of the function should be easy to read.\n\n\nCode should use good style principles.\nThere should be occasional comments to explain the purpose of the steps.\nComplicated steps, or steps that are repeated many times, should be written into separate functions (sometimes called helper functions).\n\n\nFunctions should be self-contained.\n\n\nThey should not rely on any information besides what is given as input.\n(Relying on other functions is fine, though)\nThey should not alter the Global Environment\nFunctions should never load or install packages!"
  },
  {
    "objectID": "07-functions.html#checkin7-1",
    "href": "07-functions.html#checkin7-1",
    "title": "\n7  Writing Functions\n",
    "section": "Check-in 7.1: Writing Functions",
    "text": "Check-in 7.1: Writing Functions\nSimple Functions\n\nFill in the necessary code to write a function called times_seven(). The function should take a single argument (x) and multiply the input by 7.\n\n\nThis function should check that the argument is numeric.\nThis function should also excitedly announce (print) “I love sevens!” if the argument to the function is a 7.\n\n\ntimes_seven <- function(x){\n  stopifnot(___)\n\n  if(___){\n    print(___)\n  }\n  return(___)\n}\n\nPredicting Function Behavior\n\nFor the following function, add_or_subtract():\n\n\nadd_or_subtract <- function(first_num,\n                            second_num = 2,\n                            type = \"add\") {\n\n  if (type == \"add\") {\n    first_num + second_num\n  } else if (type == \"subtract\") {\n    first_num - second_num\n  } else {\n    stop(\"Please choose `add` or `subtract` as the type.\")\n  }\n\n}\n\nWhat will be the output for the following three calls to the add_or_subtract() function?\n\nadd_or_subtract(5, 6, type = \"subtract\")\n\nadd_or_subtract(\"orange\")\n\nadd_or_subtract(5, 6, type = \"multiply\")\n\n\n1\n-1\n30\nAn error defined by the function add_or_subtract()\n\nAn error defined in a different function, which is called inside the add_or_subtract() function\n\n\nConsider the following code:\n\n\nfirst_num  <- 5\nsecond_num <- 3\n\nresult <- 8\n\nresult <- add_or_subtract(first_num, second_num = 4)\n\nresult_2 <- add_or_subtract(first_num)\n\nIn your Global Environment, what is the value of…\n\nfirst_num\nsecond_num\nresult\nresult_2"
  },
  {
    "objectID": "08-functional-programming.html",
    "href": "08-functional-programming.html",
    "title": "\n8  Functional Programming\n",
    "section": "",
    "text": "Reading: 18 minute(s) at 200 WPM\nVideos: 29 minute(s)"
  },
  {
    "objectID": "08-functional-programming.html#ch8-objectives",
    "href": "08-functional-programming.html#ch8-objectives",
    "title": "\n8  Functional Programming\n",
    "section": "Objectives",
    "text": "Objectives\n\nUse functional programming techniques to create code which is well organized and easier to understand and maintain"
  },
  {
    "objectID": "08-functional-programming.html#ch8-checkins",
    "href": "08-functional-programming.html#ch8-checkins",
    "title": "\n8  Functional Programming\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere is one check-in for this week:\n\nCheck-in 8.1: Functional Programming with the map() family"
  },
  {
    "objectID": "08-functional-programming.html#introduction-to-iteration",
    "href": "08-functional-programming.html#introduction-to-iteration",
    "title": "\n8  Functional Programming\n",
    "section": "\n8.1 Introduction to Iteration",
    "text": "8.1 Introduction to Iteration\nWe just learned the rule of “don’t repeat yourself more than two times” and to instead automate our procedures with functions in order to remove duplication of code. We have used tools such as across() to help eliminate this copy-paste procedure even further. This is a form of iteration in programming as across() “iterates” over variables, applying a function to manipulate each variable and then doing the same for the next variable.\n\n\n\n\n\n\nwhile() and for() loops are a common form of iteration that can be extremely useful when logically thinking through a problem, however are extremely computationally intensive. Therefore, loops will not be the focus of this chapter. If you are interested, you can go read about loops in the pre-reading material of this text.\nRead more\nYou can read all about iteration in the previous version of R4DS."
  },
  {
    "objectID": "08-functional-programming.html#review-of-lists-and-vectors",
    "href": "08-functional-programming.html#review-of-lists-and-vectors",
    "title": "\n8  Functional Programming\n",
    "section": "\n8.2 Review of Lists and Vectors",
    "text": "8.2 Review of Lists and Vectors\nIn the pre-reading, we introduce the different data structures we have worked with in R. We are going to do a review of some of the important data structures for this chapter.\nA vector is a 1-dimensional data structure that contains items of the same simple (‘atomic’) type (character, logical, integer, factor).\n\n(logical_vec <- c(T, F, T, T))\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n(numeric_vec <- c(3, 1, 4, 5))\n\n[1] 3 1 4 5\n\n(char_vec <- c(\"A\", \"AB\", \"ABC\", \"ABCD\"))\n\n[1] \"A\"    \"AB\"   \"ABC\"  \"ABCD\"\n\n\nYou index a vector using brackets: to get the \\(i\\)th element of the vector x, you would use x[i] in R or x[i-1] in python (Remember, python is 0-indexed, so the first element of the vector is at location 0).\n\nlogical_vec[3]\n\n[1] TRUE\n\nnumeric_vec[3]\n\n[1] 4\n\nchar_vec[3]\n\n[1] \"ABC\"\n\n\nYou can also index a vector using a logical vector:\n\nnumeric_vec[logical_vec]\n\n[1] 3 4 5\n\nchar_vec[logical_vec]\n\n[1] \"A\"    \"ABC\"  \"ABCD\"\n\nlogical_vec[logical_vec]\n\n[1] TRUE TRUE TRUE\n\n\nA list is a 1-dimensional data structure that has no restrictions on what type of content is stored within it. A list is a “vector”, but it is not an atomic vector - that is, it does not necessarily contain things that are all the same type.\n\n(\n  mylist <- list(\n    logical_vec, \n    numeric_vec, \n    third_thing = char_vec[1:2]\n  )\n)\n\n[[1]]\n[1]  TRUE FALSE  TRUE  TRUE\n\n[[2]]\n[1] 3 1 4 5\n\n$third_thing\n[1] \"A\"  \"AB\"\n\n\nList components may have names (or not), be homogeneous (or not), have the same length (or not).\n\n8.2.1 Indexing\nIndexing necessarily differs between R and python, and since the list types are also somewhat different (e.g. lists cannot be named in python), we will treat list indexing in the two languages separately.\n\n\n\n\n\nAn unusual pepper shaker which we’ll call pepper\n\n\n\n\n\n\nWhen a list is indexed with single brackets, pepper[1], the return value is always a list containing the selected element(s).\n\n\n\n\n\n\nWhen a list is indexed with double brackets, pepper[[1]], the return value is the selected element.\n\n\n\n\n\n\nTo actually access the pepper, we have to use double indexing and index both the list object and the sub-object, as in pepper[[1]][[1]].\n\n\n\n\nFigure 8.1: The types of indexing are made most memorable with a fantastic visual example from Grolemund and Wickham (2017), which I have repeated here.\n\n\nThere are 3 ways to index a list:\n\nWith single square brackets, just like we index atomic vectors. In this case, the return value is always a list.\n\n\nmylist[1]\n\n[[1]]\n[1]  TRUE FALSE  TRUE  TRUE\n\nmylist[2]\n\n[[1]]\n[1] 3 1 4 5\n\nmylist[c(T, F, T)]\n\n[[1]]\n[1]  TRUE FALSE  TRUE  TRUE\n\n$third_thing\n[1] \"A\"  \"AB\"\n\n\n\nWith double square brackets. In this case, the return value is the thing inside the specified position in the list, but you also can only get one entry in the main list at a time. You can also get things by name.\n\n\nmylist[[1]]\n\n[1]  TRUE FALSE  TRUE  TRUE\n\nmylist[[\"third_thing\"]]\n\n[1] \"A\"  \"AB\"\n\n\n\nUsing x$name. This is equivalent to using x[[\"name\"]]. Note that this does not work on unnamed entries in the list.\n\n\nmylist$third_thing\n\n[1] \"A\"  \"AB\"\n\n\nTo access the contents of a list object, we have to use double-indexing:\n\nmylist[[\"third_thing\"]][[1]]\n\n[1] \"A\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can get a more thorough review of vectors and lists from Jenny Bryan’s purrr tutorial introduction (Bryan n.d.)."
  },
  {
    "objectID": "08-functional-programming.html#vectorized-operations",
    "href": "08-functional-programming.html#vectorized-operations",
    "title": "\n8  Functional Programming\n",
    "section": "\n8.3 Vectorized Operations",
    "text": "8.3 Vectorized Operations\nOperations in R are (usually) vectorized - that is, by default, they operate on vectors. This is primarily a feature that applies to atomic vectors (and we don’t even think about it):\n\n\n\n\n\n\n\n(rnorm(10) + rnorm(10, mean = 3))\n\n [1] 3.7285369 0.7166789 3.8175248 2.4609676 4.7094032 1.6242776 3.9324809\n [8] 5.8978374 2.7208493 4.8787993\n\n\nWith vectorized functions, we don’t have to use a for loop to add these two vectors with 10 entries each together. In languages which don’t have implicit support for vectorized computations, this might instead look like:\n\na <- rnorm(10)\nb <- rnorm(10, mean = 3)\n\nresult <- rep(0, 10)\nfor (i in 1:10) {\n  result[i] <- a[i] + b[i]\n}\n\nresult\n\n [1]  5.1699090  3.1676718  2.1741276  2.1699792  5.3172499 -0.5303277\n [7]  4.9794343  5.3602351  3.6929630  5.0583391\n\n\nThat is, we would apply or map the + function to each entry of a and b. For atomic vectors, it’s easy to do this by default; with a list, however, we need to be a bit more explicit (because everything that’s passed into the function may not be the same type).\n\n\nI find the purrr package easier to work with, so we won’t be working with the base functions in this course. You can find a side-by-side comparison in the purrr tutorial.\n\nYou can also watch Dr. Theobold’s video to learn more:\n\n\n\n\nThe R package purrr (and similar base functions apply, lapply, sapply, tapply, and mapply) are based on extending “vectorized” functions to a wider variety of vector-like structures."
  },
  {
    "objectID": "08-functional-programming.html#functional-programming",
    "href": "08-functional-programming.html#functional-programming",
    "title": "\n8  Functional Programming\n",
    "section": "\n8.4 Functional Programming",
    "text": "8.4 Functional Programming\nThe concept of functional programming is a bit hard to define rigorously at the level we’re working at, but generally, functional programming is concerned with pure functions: functions that have an input value that determines the output value and create no other side effects.\nWhat this means is that you describe every step of the computation using a function, and chain the functions together. At the end of the computations, you might save the program’s results to an object, but (in general), the goal is to not change things outside of the “pipeline” along the way.\nThis has some advantages:\n\nEasier parallelization\n\n“Side effects” generally make it hard to parallelize code because e.g. you have to update stored objects in memory, which is hard to do with multiple threads accessing the same memory.\n\n\nFunctional programming tends to be easier to read\n\nYou can see output and input and don’t have to work as hard to keep track of what is stored where .\n\n\nEasier Debugging\n\nYou can examine the input and output at each stage to isolate which function is introducing the problem.\n\n\n\nThe introduction of the pipe in R has made chaining functions together in a functional programming-style pipeline much easier. purrr is just another step in this process: by making it easy to apply functions to lists of things (or to use multiple lists of things in a single function), purrr makes it easier to write clean, understandable, debuggable code.\n\nFunctional Programming Example\nThis example is modified from the motivation section of the Functional Programming chapter in Advanced R (Wickham 2019).\n\n\nProblem\nNaive Approach\nWriting a function\nMapping a function\n\n\n\nSuppose we want to replace every -99 in the following sample dataset with an NA. (-99 is sometimes used to indicate missingness in datasets).\n\n# Generate a sample dataset\nset.seed(1014)\ndf <- data.frame(replicate(6, sample(c(1:10, -99), 6, rep = TRUE)))\nnames(df) <- letters[1:6]\ndf\n\n  a   b   c   d  e f\n1 7   5 -99   2  5 2\n2 5   5   5   3  6 1\n3 6   8   5   9  9 4\n4 4   2   2   6  6 8\n5 6   7   6 -99 10 6\n6 9 -99   4   7  5 1\n\n\n\n\nThe “beginner” approach is to just replace each individual -99 with an NA:\n\ndf1 <- df\ndf1[6,2] <- NA\ndf1[1,3] <- NA\ndf1[5,4] <- NA\n\ndf1\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis is tedious, and painful, and won’t work if we have a slightly different dataset where the -99s are in different places. So instead, we might consider being a bit more general:\n\ndf2 <- df\ndf2$a[df2$a == -99] <- NA\ndf2$b[df2$b == -99] <- NA\ndf2$c[df2$c == -99] <- NA\ndf2$d[df2$d == -99] <- NA\ndf2$e[df2$e == -99] <- NA\ndf2$f[df2$f == -99] <- NA\ndf2\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis requires a few more lines of code, but is able to handle any data frame with 6 columns a - f. It also requires a lot of copy-paste and can leave you vulnerable to making mistakes.\n\n\nThe standard rule is that if you copy-paste the same code 3x, then you should write a function, so let’s try that instead:\n\nfix_missing <- function(x, missing = -99){\n  x[x == missing] <- NA\n  x\n}\n\ndf3 <- df\ndf3$a <- fix_missing(df$a)\ndf3$b <- fix_missing(df$b)\ndf3$c <- fix_missing(df$c)\ndf3$d <- fix_missing(df$d)\ndf3$e <- fix_missing(df$e)\ndf3$f <- fix_missing(df$f)\ndf3\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis still requires a lot of copy-paste, and doesn’t actually make the code more readable. We can more easily change the missing value, though, which is a bonus.\n\n\nWe have a function that we want to apply or map to every column in our data frame. We could use a for() loop (doing this for demonstrative purposes only, I expect you to use more efficient tools in class):\n\nfix_missing <- function(x, missing = -99){\n  x[x == missing] <- NA\n  x\n}\n\ndf4 <- df\nfor (i in 1:ncol(df)) {\n  df4[,i] <- fix_missing(df4[,i])\n}\ndf4\n\n  a  b  c  d  e f\n1 7  5 NA  2  5 2\n2 5  5  5  3  6 1\n3 6  8  5  9  9 4\n4 4  2  2  6  6 8\n5 6  7  6 NA 10 6\n6 9 NA  4  7  5 1\n\n\nThis is more understandable and flexible than the previous function approach as well as the naive approach - we don’t need to know the names of the columns in our data frame, or even how many there are. It is still quite a few lines of code, though.\nIterating through a list (or columns of a data frame) is a very common task, so R has a shorthand function for it. You could us lapply from base R, but we will be learning the map family of functions from the purrr package.\n\nfix_missing <- function(x, missing = -99){\n  x[x == missing] <- NA\n  x\n}\n\ndf5 <- df\ndf5 <- map_dfc(df5, fix_missing)\ndf5\n\n# A tibble: 6 × 6\n      a     b     c     d     e     f\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     7     5    NA     2     5     2\n2     5     5     5     3     6     1\n3     6     8     5     9     9     4\n4     4     2     2     6     6     8\n5     6     7     6    NA    10     6\n6     9    NA     4     7     5     1\n\n\nBy default, map returns a list (see below), but we can use map_dfc to return a data frame created by binding the columns together.\nmap() - returns a list\n\ndf6 <- df\nmap(df6, fix_missing)\n\n$a\n[1] 7 5 6 4 6 9\n\n$b\n[1]  5  5  8  2  7 NA\n\n$c\n[1] NA  5  5  2  6  4\n\n$d\n[1]  2  3  9  6 NA  7\n\n$e\n[1]  5  6  9  6 10  5\n\n$f\n[1] 2 1 4 8 6 1\n\n\nWe’ve replaced 6 lines of code that only worked for 6 columns named a - f with a single line of code that works for any data frame with any number of rows and columns, so long as -99 indicates missing data. In addition to being shorter, this code is also somewhat easier to read and much less vulnerable to typos."
  },
  {
    "objectID": "08-functional-programming.html#introduction-to-map",
    "href": "08-functional-programming.html#introduction-to-map",
    "title": "\n8  Functional Programming\n",
    "section": "\n8.5 Introduction to map()\n",
    "text": "8.5 Introduction to map()\n\n\npurrr is a part of the tidyverse, so you should already have the package installed. When you load the tidyverse with library(), this also loads purrr.\n\ninstall.packages(\"purrr\")\nlibrary(purrr)\n\nDownload the purrr cheatsheet.\n\n\n\n\n\n\n\n\n8.5.1 Data Setup\nWe’ll use one of the datasets in the repurrsive package, got_chars, to start playing with the map_ series of functions. First, let’s export the data from repurrrsive to a JSON file that we can read into R.\n\n\nThe source data for this example comes from An API of Ice and Fire and is fairly typical for API (automatic programming interface) data in both cleanliness and complexity.\n\nlibrary(repurrrsive) # example data\ndata(got_chars)\n\nlibrary(rjson)\nwrite(toJSON(got_chars), \"data/got_chars.json\")\n\n\nlibrary(purrr) # functions for working with lists\ngot_chars <- fromJSON(file = \"data/got_chars.json\")\n\n\nlength(got_chars)\n\n[1] 30\n\ngot_chars[[1]][1:6] # Only show the first 6 fields\n\n$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1022\"\n\n$id\n[1] 1022\n\n$name\n[1] \"Theon Greyjoy\"\n\n$gender\n[1] \"Male\"\n\n$culture\n[1] \"Ironborn\"\n\n$born\n[1] \"In 278 AC or 279 AC, at Pyke\"\n\nnames(got_chars[[1]]) # How many total fields? names?\n\n [1] \"url\"         \"id\"          \"name\"        \"gender\"      \"culture\"    \n [6] \"born\"        \"died\"        \"alive\"       \"titles\"      \"aliases\"    \n[11] \"father\"      \"mother\"      \"spouse\"      \"allegiances\" \"books\"      \n[16] \"povBooks\"    \"tvSeries\"    \"playedBy\"   \n\n\nIt appears that each entry in this 30-item list is a character from Game of Thrones, and there are several sub-fields for each character.\n\n8.5.2 Exploring the Data\nWhat characters do we have? How is the data structured?\nList data can be incredibly hard to work with because the structure is so flexible. It’s important to have a way to visualize the structure of a complex list object: the View() command in RStudio is one good way to explore and poke around a list.\n\nWe can use purrr::map(x, \"name\") to get a list of all characters’ names. Since they are all the same type, we could also use an extension of map(), map_chr(), which will coerce the returned list into a character vector (which may be simpler to operate on).\n\nThere are several packages with map() functions including functions that are meant to actually plot maps; it generally saves time and effort to just type the function name with the package you want in package::function notation. You don’t have to do so, but if you have a lot of other (non tidyverse, in particular) packages loaded, it will save you a lot of grief.\n\n\npurrr::map(got_chars, \"name\")[1:5]\n\n[[1]]\n[1] \"Theon Greyjoy\"\n\n[[2]]\n[1] \"Tyrion Lannister\"\n\n[[3]]\n[1] \"Victarion Greyjoy\"\n\n[[4]]\n[1] \"Will\"\n\n[[5]]\n[1] \"Areo Hotah\"\n\npurrr::map_chr(got_chars, \"name\")[1:5]\n\n[1] \"Theon Greyjoy\"     \"Tyrion Lannister\"  \"Victarion Greyjoy\"\n[4] \"Will\"              \"Areo Hotah\"       \n\n\nSimilar shortcuts work to get the nth item in each sub list:\n\npurrr::map_chr(got_chars, 4)\n\n [1] \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Female\"\n [9] \"Female\" \"Male\"   \"Female\" \"Male\"   \"Female\" \"Male\"   \"Male\"   \"Male\"  \n[17] \"Female\" \"Female\" \"Female\" \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"  \n[25] \"Male\"   \"Female\" \"Male\"   \"Male\"   \"Male\"   \"Female\"\n\n\nSpecifying the output type using e.g. map_chr() works if each item in the list is an atomic vector of length 1. If the list is more complicated, though, these shortcuts will issue an error:\n\npurrr::map(got_chars, \"books\")[1:5]\n\n[[1]]\n[1] \"A Game of Thrones\" \"A Storm of Swords\" \"A Feast for Crows\"\n\n[[2]]\n[1] \"A Feast for Crows\"         \"The World of Ice and Fire\"\n\n[[3]]\n[1] \"A Game of Thrones\" \"A Clash of Kings\"  \"A Storm of Swords\"\n\n[[4]]\n[1] \"A Clash of Kings\"\n\n[[5]]\n[1] \"A Game of Thrones\" \"A Clash of Kings\"  \"A Storm of Swords\"\n\npurrr::map_chr(got_chars, \"books\")[1:5]\n\nError in `purrr::map_chr()`:\nℹ In index: 1.\nCaused by error:\n! Result must be length 1, not 3.\n\n\nWhat if we want to extract several things? This trick works off of the idea that [ is a function: that is, the single brackets we used before are actually a special type of function. In R functions, there is often the argument ..., which is a convention that allows us to pass arguments to other functions that are called within the main function we are using (you’ll see … used in plotting and regression functions frequently as well).\nHere, we use ... to pass in our list of 3 things we want to pull from each item in the list.\n\npurrr::map(got_chars, `[`, c(\"name\", \"gender\", \"born\"))[1:5]\n\n[[1]]\n[[1]]$name\n[1] \"Theon Greyjoy\"\n\n[[1]]$gender\n[1] \"Male\"\n\n[[1]]$born\n[1] \"In 278 AC or 279 AC, at Pyke\"\n\n\n[[2]]\n[[2]]$name\n[1] \"Tyrion Lannister\"\n\n[[2]]$gender\n[1] \"Male\"\n\n[[2]]$born\n[1] \"In 273 AC, at Casterly Rock\"\n\n\n[[3]]\n[[3]]$name\n[1] \"Victarion Greyjoy\"\n\n[[3]]$gender\n[1] \"Male\"\n\n[[3]]$born\n[1] \"In 268 AC or before, at Pyke\"\n\n\n[[4]]\n[[4]]$name\n[1] \"Will\"\n\n[[4]]$gender\n[1] \"Male\"\n\n[[4]]$born\n[1] \"\"\n\n\n[[5]]\n[[5]]$name\n[1] \"Areo Hotah\"\n\n[[5]]$gender\n[1] \"Male\"\n\n[[5]]$born\n[1] \"In 257 AC or before, at Norvos\"\n\n\nWhat if we want this to be a data frame instead? We can use map_dfr() to get a data frame that is formed by row-binding each element in the list.\n\npurrr::map_dfr(got_chars, `[`, c(\"name\", \"gender\", \"born\")) \n\n# A tibble: 30 × 3\n   name               gender born                                    \n   <chr>              <chr>  <chr>                                   \n 1 Theon Greyjoy      Male   \"In 278 AC or 279 AC, at Pyke\"          \n 2 Tyrion Lannister   Male   \"In 273 AC, at Casterly Rock\"           \n 3 Victarion Greyjoy  Male   \"In 268 AC or before, at Pyke\"          \n 4 Will               Male   \"\"                                      \n 5 Areo Hotah         Male   \"In 257 AC or before, at Norvos\"        \n 6 Chett              Male   \"At Hag's Mire\"                         \n 7 Cressen            Male   \"In 219 AC or 220 AC\"                   \n 8 Arianne Martell    Female \"In 276 AC, at Sunspear\"                \n 9 Daenerys Targaryen Female \"In 284 AC, at Dragonstone\"             \n10 Davos Seaworth     Male   \"In 260 AC or before, at King's Landing\"\n# ℹ 20 more rows\n\n# Equivalent to\npurrr::map(got_chars, `[`, c(\"name\", \"gender\", \"born\")) |>\n  dplyr::bind_rows()\n\n# A tibble: 30 × 3\n   name               gender born                                    \n   <chr>              <chr>  <chr>                                   \n 1 Theon Greyjoy      Male   \"In 278 AC or 279 AC, at Pyke\"          \n 2 Tyrion Lannister   Male   \"In 273 AC, at Casterly Rock\"           \n 3 Victarion Greyjoy  Male   \"In 268 AC or before, at Pyke\"          \n 4 Will               Male   \"\"                                      \n 5 Areo Hotah         Male   \"In 257 AC or before, at Norvos\"        \n 6 Chett              Male   \"At Hag's Mire\"                         \n 7 Cressen            Male   \"In 219 AC or 220 AC\"                   \n 8 Arianne Martell    Female \"In 276 AC, at Sunspear\"                \n 9 Daenerys Targaryen Female \"In 284 AC, at Dragonstone\"             \n10 Davos Seaworth     Male   \"In 260 AC or before, at King's Landing\"\n# ℹ 20 more rows\n\n\nIf we want to more generally convert the entire data set to a data frame, we can use a couple of handy functions to do that:\n\n\npurrr::transpose transposes a list, so that x[[1]][[2]] becomes x[[2]][[1]]. This turns the list into a set of columns.\n\ntibble::as_tibble turns an object into a tibble. This creates a rectangular, data frame like structure\n\npurrr::unnest takes columns and “ungroups” them, so that each entry in the sub-lists of the column gets a row in the data frame. Here, I’ve used this to unwrap lists that are all single items so that we can see some of the data.\n\n\ngot_df <- got_chars |>\n  transpose() |>\n  as_tibble() |>\n  unnest(c(\"url\", \"id\", \"name\", \"gender\", \n           \"culture\", \"born\", \"died\", \"alive\", \n           \"father\", \"mother\", \"spouse\"))\n\n\n\n\n\n\n url \n    id \n    name \n    gender \n    culture \n    born \n    died \n    alive \n    titles \n    aliases \n    father \n    mother \n    spouse \n    allegiances \n    books \n    povBooks \n    tvSeries \n    playedBy \n  \n\n\n https://www.anapioficeandfire.com/api/characters/1022 \n    1022 \n    Theon Greyjoy \n    Male \n    Ironborn \n    In 278 AC or 279 AC, at Pyke \n     \n    TRUE \n    Prince of Winterfell                                , Lord of the Iron Islands (by law of the green lands) \n    Prince of Fools, Theon Turncloak, Reek           , Theon Kinslayer \n     \n     \n     \n    House Greyjoy of Pyke \n    A Game of Thrones, A Storm of Swords, A Feast for Crows \n    A Clash of Kings    , A Dance with Dragons \n    Season 1, Season 2, Season 3, Season 4, Season 5, Season 6 \n    Alfie Allen \n  \n\n https://www.anapioficeandfire.com/api/characters/1052 \n    1052 \n    Tyrion Lannister \n    Male \n     \n    In 273 AC, at Casterly Rock \n     \n    TRUE \n    Acting Hand of the King (former), Master of Coin (former) \n    The Imp           , Halfman           , The boyman        , Giant of Lannister, Lord Tywin's Doom , Lord Tywin's Bane , Yollo             , Hugor Hill        , No-Nose           , Freak             , Dwarf \n     \n     \n    https://www.anapioficeandfire.com/api/characters/2044 \n    House Lannister of Casterly Rock \n    A Feast for Crows        , The World of Ice and Fire \n    A Game of Thrones   , A Clash of Kings    , A Storm of Swords   , A Dance with Dragons \n    Season 1, Season 2, Season 3, Season 4, Season 5, Season 6 \n    Peter Dinklage \n  \n\n https://www.anapioficeandfire.com/api/characters/1074 \n    1074 \n    Victarion Greyjoy \n    Male \n    Ironborn \n    In 268 AC or before, at Pyke \n     \n    TRUE \n    Lord Captain of the Iron Fleet, Master of the Iron Victory \n    The Iron Captain \n     \n     \n     \n    House Greyjoy of Pyke \n    A Game of Thrones, A Clash of Kings , A Storm of Swords \n    A Feast for Crows   , A Dance with Dragons \n     \n     \n  \n\n https://www.anapioficeandfire.com/api/characters/1109 \n    1109 \n    Will \n    Male \n     \n     \n    In 297 AC, at Haunted Forest \n    FALSE \n     \n     \n     \n     \n     \n    NULL \n    A Clash of Kings \n    A Game of Thrones \n     \n    Bronson Webb \n  \n\n https://www.anapioficeandfire.com/api/characters/1166 \n    1166 \n    Areo Hotah \n    Male \n    Norvoshi \n    In 257 AC or before, at Norvos \n     \n    TRUE \n    Captain of the Guard at Sunspear \n     \n     \n     \n     \n    House Nymeros Martell of Sunspear \n    A Game of Thrones, A Clash of Kings , A Storm of Swords \n    A Feast for Crows   , A Dance with Dragons \n    Season 5, Season 6 \n    DeObia Oparei \n  \n\n https://www.anapioficeandfire.com/api/characters/1267 \n    1267 \n    Chett \n    Male \n     \n    At Hag's Mire \n    In 299 AC, at Fist of the First Men \n    FALSE \n     \n     \n     \n     \n     \n    NULL \n    A Game of Thrones, A Clash of Kings \n    A Storm of Swords \n     \n     \n  \n\n\n\n\n\n\n\n8.5.3 Map inside Mutate\nA very powerful way to work with data is to use a map function inside of a mutate statement: to simplify data and create a new column all in one go. Let’s use this to create a more human-readable (though somewhat less “clean”) data frame:\n\nfunction to simplify a character list-column,\n\nreplace any 0-length/NULL entries with an empty string\npaste all of the entries together, separated by “,”\nensure that the resulting list is coerced to a character vector\n\n\nThen, we can apply the above function to each list column in our data frame.\n\n\npaste_entries <- function(x) {\n  # Replace any null entries of x with \"\"\n  x[map_int(x, length) == 0] <- \"\"\n  \n  map_chr(x, ~paste(., collapse = \", \"))\n}\n\ngot_df <- got_df |>\n  mutate(across(where(is.list), paste_entries))\n\n\n8.5.4 Creating (and Using) List-columns\nData structures in R are typically list-based in one way or another. Sometimes, more complicated data structures are actually lists of lists, or tibbles with a list-column, or other variations on “list within a ____”. In combination with purrr, this is an incredibly powerful setup that can make working with simulations and data very easy.\n\nExample: Benefits of List columns\nSuppose, for instance, I want to simulate some data for modeling purposes, where I can control the number of outliers in the dataset:\n\ndata_sim <- function(n_outliers = 0) {\n  tmp <- tibble(x = seq(-10, 10, .1),\n                y = rnorm(length(x), mean = x, sd = 1)\n                )\n  \n  \n  outlier_sample <- c(NULL, sample(tmp$x, n_outliers))\n  \n  # Create outliers\n  tmp |> \n    mutate(\n      is_outlier = x %in% outlier_sample,\n      y = y + is_outlier * sample(c(-1, 1), n(), replace = T) * runif(n(), 5, 10)\n    )\n}\ndata_sim()\n\n# A tibble: 201 × 3\n       x      y is_outlier\n   <dbl>  <dbl> <lgl>     \n 1 -10   -10.8  FALSE     \n 2  -9.9 -11.4  FALSE     \n 3  -9.8  -8.86 FALSE     \n 4  -9.7  -9.52 FALSE     \n 5  -9.6  -9.36 FALSE     \n 6  -9.5  -7.88 FALSE     \n 7  -9.4  -9.29 FALSE     \n 8  -9.3  -9.43 FALSE     \n 9  -9.2 -11.1  FALSE     \n10  -9.1  -9.38 FALSE     \n# ℹ 191 more rows\n\n\nNow, lets suppose that I want 100 replicates of each of 0, 5, 10, and 20 outliers.\n\nsim <- crossing(rep = 1:100, n_outliers = c(0, 5, 10, 20)) |>\n  mutate(sim_data = purrr::map(n_outliers, data_sim))\n\nI could use unnest(sim_data) if I wanted to expand my data a bit to see what I have, but in this case, it’s more useful to leave it in its current, compact form. Instead, suppose I fit a linear regression to each of the simulated data sets, and store the fitted linear regression object in a new list-column?\n\nsim <- sim |>\n  mutate(reg = purrr::map(sim_data, \n                          ~lm(data = ., y ~ x)\n                          )\n         )\n\nHere, we use an anonymous function in purrr: by using ~{expression}, we have defined a function that takes the argument . (which is just a placeholder). So in our case, we’re saying “use the data that I pass in to fit a linear regression of y using x as a predictor”.\nLet’s play around a bit with this: We might want to look at our regression coefficients or standard errors to see how much the additional outliers affect us. We could use a fancy package for tidy modeling, such as broom, but for now, lets do something a bit simpler and apply the purrr name extraction functions we used earlier.\nIt can be helpful to examine one of the objects just to see what you’re dealing with:\n\nstr(sim$reg[[1]])\n\nList of 12\n $ coefficients : Named num [1:2] -0.117 1.001\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"x\"\n $ residuals    : Named num [1:201] 0.584 -0.721 0.696 1.282 -0.627 ...\n  ..- attr(*, \"names\")= chr [1:201] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:201] 1.662 82.367 0.733 1.318 -0.591 ...\n  ..- attr(*, \"names\")= chr [1:201] \"(Intercept)\" \"x\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:201] -10.13 -10.03 -9.93 -9.83 -9.73 ...\n  ..- attr(*, \"names\")= chr [1:201] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:201, 1:2] -14.1774 0.0705 0.0705 0.0705 0.0705 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:201] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"x\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.11\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 199\n $ xlevels      : Named list()\n $ call         : language lm(formula = y ~ x, data = .)\n $ terms        :Classes 'terms', 'formula'  language y ~ x\n  .. ..- attr(*, \"variables\")= language list(y, x)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"y\" \"x\"\n  .. .. .. ..$ : chr \"x\"\n  .. ..- attr(*, \"term.labels\")= chr \"x\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: 0x00000170c3663b00> \n  .. ..- attr(*, \"predvars\")= language list(y, x)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"y\" \"x\"\n $ model        :'data.frame':  201 obs. of  2 variables:\n  ..$ y: num [1:201] -9.55 -10.75 -9.23 -8.55 -10.36 ...\n  ..$ x: num [1:201] -10 -9.9 -9.8 -9.7 -9.6 -9.5 -9.4 -9.3 -9.2 -9.1 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language y ~ x\n  .. .. ..- attr(*, \"variables\")= language list(y, x)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"y\" \"x\"\n  .. .. .. .. ..$ : chr \"x\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"x\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: 0x00000170c3663b00> \n  .. .. ..- attr(*, \"predvars\")= language list(y, x)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"y\" \"x\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nIf we pull out the coefficients by name we get a vector of length two. So before we unnest, we need to change that so that R formats it as a row of a data frame.\n\nsim$reg[[1]]$coefficients |> \n  as_tibble_row()\n\n# A tibble: 1 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1        -0.117  1.00\n\n\nThis will make our formatting a lot easier and prevent any duplication that might occur if we unnest a vector that has length > 1.\n\nsim <- sim |>\n  mutate(coefs = purrr::map(reg, \"coefficients\") |>\n           purrr::map(as_tibble_row))\n\nsim$coefs[1:5]\n\n[[1]]\n# A tibble: 1 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1        -0.117  1.00\n\n[[2]]\n# A tibble: 1 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1        -0.105 0.977\n\n[[3]]\n# A tibble: 1 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1        -0.125 0.961\n\n[[4]]\n# A tibble: 1 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1         0.211 0.966\n\n[[5]]\n# A tibble: 1 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1         0.108  1.01\n\n\nThen, we can plot our results:\n\nsim |>\n  unnest(coefs) |>\n  select(rep, n_outliers, `(Intercept)`, x) |>\n  pivot_longer(-c(rep, n_outliers), \n               names_to = \"coef\", \n               values_to = \"value\"\n               ) |>\n  ggplot(aes(x = value, \n             color = factor(n_outliers))\n         ) + \n  geom_density() + \n  facet_wrap(~coef, scales = \"free_x\")\n\n\n\n\nSo as there are more and more outliers, the coefficient estimates get a wider distribution, but remain (relatively) centered on the “true” values of 0 and 1, respectively.\nNotice that we keep our data in list column form right up until it is time to actually unnest it - which means that we have at the ready the simulated data, the simulated model, and the conditions under which it was simulated, all in the same data structure. It’s a really nice, organized system.\n\n8.5.5 Ways to use map\n\nThere are 3 main use cases for map (and its cousins pmap, map2, etc.):\n\nUse with an existing function\nUse with an anonymous function, defined on the fly\nUse with a formula (which is just a concise way to define an anonymous function)\n\nI’ll use a trivial example to show the difference between these options:\n\n# An existing function\nres <- tibble(x = 1:10, \n              y1 = map_dbl(x, log10)\n              )\n\n# An anonymous function\nres <- res |> \n  mutate(\n    y2 = map_dbl(x, function(z) z^2/10)\n    )\n\n# A formula equivalent to function(z) z^5/(z + 10)\n# the . is the variable you're manipulating\nres <- res |> \n  mutate(\n    y3 = map_dbl(x, ~.x^5/(.x+10))\n    )\n\nIt can be a bit tricky to differentiate between options 2 and 3 in practice - the biggest difference is that you’re not using the keyword function and your variable is the default placeholder variable .x used in the tidyverse.\nExample\n\n\nProblem\nSolution\n\n\n\nCreate a new column containing a single string of all of the books each character was in.\nTo do this, you’ll need to collapse the list of books for each character into a single string, which you can do with the str_flatten() function and the collapse argument. I’ve copied here for convenience. (The function won’t work out of the box, because it was designed to work on each column of a dataframe, and here we’d be applying it to each row.)\n\nletters[1:10] |> str_flatten(collapse = \"|\")\n\n[1] \"a|b|c|d|e|f|g|h|i|j\"\n\n\nStart with this data frame of character names and book list-columns:\n\ndata(got_chars)\n\ngot_df <- tibble(name = map_chr(got_chars, \"name\"),\n                 id = map_int(got_chars, \"id\"),\n                 books = map(got_chars, \"books\")\n                 )\n\n\n\n\n# Define a function\nmy_collapse <- function(x){\n  \n  str_flatten(x, collapse = \" | \")\n  \n}\n\ndata(got_chars)\n\ngot_df <- tibble(name = map_chr(got_chars, \"name\"),\n                 id = map_int(got_chars, \"id\"),\n                 books = map(got_chars, \"books\")\n                 )\n\ngot_df <- got_df |>\n  mutate(\n    fun_def_res = map_chr(.x = books, \n                          .f = my_collapse\n                          ),\n    # Here, I don't have to define a function, I just pass my additional \n    # argument in after the fact (recall ...)\n    fun_base_res = map_chr(.x = books, \n                           .f = str_flatten, \n                           collapse = \" | \"\n                           ),\n    \n    # Here, I can just define a new function without a name and apply it to \n    # each entry\n    fun_anon_res = map_chr(.x = books, \n                           .f = function(x) str_flatten(x, collapse = \" | \")\n                           ),\n    \n    # And here, I don't even bother to specifically say that I'm defining a \n    # function, I just apply a formula to each entry\n    fun_formula_res = map_chr(.x = books, \n                              .f = ~ str_flatten(.x, collapse = \" | \")\n                              )\n  ) \n\n\n\n\n\n\n name \n    id \n    books \n    fun_def_res \n    fun_base_res \n    fun_anon_res \n    fun_formula_res \n  \n\n\n Theon Greyjoy \n    1022 \n    A Game of Thrones, A Storm of Swords, A Feast for Crows \n    A Game of Thrones | A Storm of Swords | A Feast for Crows \n    A Game of Thrones | A Storm of Swords | A Feast for Crows \n    A Game of Thrones | A Storm of Swords | A Feast for Crows \n    A Game of Thrones | A Storm of Swords | A Feast for Crows \n  \n\n Tyrion Lannister \n    1052 \n    A Feast for Crows        , The World of Ice and Fire \n    A Feast for Crows | The World of Ice and Fire \n    A Feast for Crows | The World of Ice and Fire \n    A Feast for Crows | The World of Ice and Fire \n    A Feast for Crows | The World of Ice and Fire \n  \n\n Victarion Greyjoy \n    1074 \n    A Game of Thrones, A Clash of Kings , A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n  \n\n Will \n    1109 \n    A Clash of Kings \n    A Clash of Kings \n    A Clash of Kings \n    A Clash of Kings \n    A Clash of Kings \n  \n\n Areo Hotah \n    1166 \n    A Game of Thrones, A Clash of Kings , A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n    A Game of Thrones | A Clash of Kings | A Storm of Swords \n  \n\n Chett \n    1267 \n    A Game of Thrones, A Clash of Kings \n    A Game of Thrones | A Clash of Kings \n    A Game of Thrones | A Clash of Kings \n    A Game of Thrones | A Clash of Kings \n    A Game of Thrones | A Clash of Kings \n  \n\n\n\n\n\n\n\n\n\n(Optional) Beyond map: Functions with multiple inputs\nSometimes, you might need to map a function over two vectors/lists in parallel. purrr has you covered with the map2 function. As with map, the syntax is map2(thing1, thing2, function, other.args); the big difference is that function takes two arguments.\nExample\nLet’s create a simple times-table:\n\ncrossing(x = 1:10, y = 1:10) |>\n  mutate(times = map2_int(x, y, `*`)) |>\n  pivot_wider(names_from = y, names_prefix = 'y=', values_from = times)\n\n# A tibble: 10 × 11\n       x `y=1` `y=2` `y=3` `y=4` `y=5` `y=6` `y=7` `y=8` `y=9` `y=10`\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>  <int>\n 1     1     1     2     3     4     5     6     7     8     9     10\n 2     2     2     4     6     8    10    12    14    16    18     20\n 3     3     3     6     9    12    15    18    21    24    27     30\n 4     4     4     8    12    16    20    24    28    32    36     40\n 5     5     5    10    15    20    25    30    35    40    45     50\n 6     6     6    12    18    24    30    36    42    48    54     60\n 7     7     7    14    21    28    35    42    49    56    63     70\n 8     8     8    16    24    32    40    48    56    64    72     80\n 9     9     9    18    27    36    45    54    63    72    81     90\n10    10    10    20    30    40    50    60    70    80    90    100\n\n# we could use `multiply_by` instead of `*` if we wanted to\n\nIf you are using formula notation to define functions with map2, you will need to refer to your two arguments as .x and .y. You can determine this from the Usage section when you run map2, which shows you map2(.x, .y, .f, ...) - that is, the first argument is .x, the second is .y, and the third is the function.\nLike map, you can specify the type of the output response using map2. This makes it very easy to format the output appropriately for your application.\nYou can use functions with many arguments with map by using the pmap variant; here, you pass in a list of functions, which are identified by position (..1, ..2, ..3, etc). Note the .. - you are referencing the list first, and the index within the list argument 2nd.\nExample\n\n\nProblem\nSolution\n\n\n\nDetermine if each Game of Thrones character has more titles than aliases. Start with this code:\n\nlibrary(repurrrsive)\nlibrary(tidyverse)\n\ndata(got_chars)\ngot_names <- tibble(name = purrr::map_chr(got_chars, \"name\"),\n                    titles = purrr::map(got_chars, \"titles\"),\n                    aliases = purrr::map(got_chars, \"aliases\"))\n\n\n\n\ngot_names |>\n  mutate(more_titles = map2_lgl(titles, aliases, ~length(.x) > length(.y)))\n\n# A tibble: 30 × 4\n   name               titles    aliases    more_titles\n   <chr>              <list>    <list>     <lgl>      \n 1 Theon Greyjoy      <chr [2]> <chr [4]>  FALSE      \n 2 Tyrion Lannister   <chr [2]> <chr [11]> FALSE      \n 3 Victarion Greyjoy  <chr [2]> <chr [1]>  TRUE       \n 4 Will               <chr [1]> <chr [1]>  FALSE      \n 5 Areo Hotah         <chr [1]> <chr [1]>  FALSE      \n 6 Chett              <chr [1]> <chr [1]>  FALSE      \n 7 Cressen            <chr [1]> <chr [1]>  FALSE      \n 8 Arianne Martell    <chr [1]> <chr [1]>  FALSE      \n 9 Daenerys Targaryen <chr [5]> <chr [11]> FALSE      \n10 Davos Seaworth     <chr [4]> <chr [5]>  FALSE      \n# ℹ 20 more rows\n\n\n\n\n\nTutorial\n(Required) Complete the tutorial on iteration and functional programming.\n\nLearn More About Purrr\n\nThe Joy of Functional Programming (for Data Science): Hadley Wickham’s talk on purrr and functional programming. ~1h video and slides.\n(The Joy of Cooking meets Data Science, with illustrations by Allison Horst)\nPirating Web Content Responsibly with R and purrr (a blog post in honor of international talk like a pirate day) (Rudis 2017)\nHappy R Development with purrr\nWeb mining with purrr\nText Wrangling with purrr\nSetting NAs with purrr (uses the naniar package)\nMappers with purrr - handy ways to make your code simpler if you’re reusing functions a lot.\nFunction factories - code optimization with purrr\nStats and Machine Learning examples with purrr\n\nCheck-in 8.1: Functional Programming with the map() family\n\nlibrary(palmerpenguins)\ndata(penguins)\n\n1. Suppose we would like to find the median of the measurement variables (bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) from the penguins dataset (from the palmerpenguins package). Which of the following will produce a numeric vector of the median of every measurement variable? Select all that apply!\n\n\n\n\npenguins |>\n  select(bill_length_mm:body_mass_g) |>\n  map(.f = median, na.rm = TRUE)\n\n\n\n\n\npenguins |>\n  select(bill_length_mm:body_mass_g) |>\n  map_dbl(.f = median, na.rm = TRUE)\n\n\n\n\n\npenguins |>\n  select(bill_length_mm:body_mass_g) |>\n  map_dfc(.f = median, na.rm = TRUE)\n\n\n\n\n\npenguins |>\n  select(bill_length_mm:body_mass_g) |>\n  map_chr(function(x) median(x, na.rm = TRUE))\n\n2. Recall that in the last unit, we discussed the challenge of standardizing many columns in a data frame. For example, If we wanted to standardize a numeric variable to be centered at the mean and scaled by the standard deviation, we could use the following function:\n\nstandardize <- function(vec) {\n  stopifnot(is.numeric(vec))\n\n  # Center with mean\n  deviations <- vec - mean(vec, na.rm = TRUE)\n  # Scale with standard deviation\n  newdata <- deviations / sd(vec, na.rm = TRUE)\n\n  return(newdata)\n}\n\nWhy does the following return a vector of NAs for the variable body_mass_g?\n\npenguins |>\n  mutate(\n    body_mass_g = map_dbl(.x = body_mass_g,\n                          .f = standardize\n                          )\n  )\n\n\nBecause body_mass_g needs to be passed to standardize() as an argument\nBecause mutate() operates on rows, so map_dbl() is supplying standardize() with one row of body_mass_g at a time\nBecause map_dbl() only takes one input, so you need to use map2_dbl() instead\nBecause there is no function named standardize(), so it cannot be applied to the body_mass_g column body_mass_g is not a data frame so it is not a valid argument for map_dbl()\n\n3. Which of the following returns a dataframe with the standardized version of the numerical variables (bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) from the penguins data set?\n\n\n\n\npenguins |>\n  select(bill_length_mm:body_mass_g) |>\n  map_df(standardize)\n\n\n\n\n\npenguins |>\n  map_at(bill_length_mm:body_mass_g, standardize)\n\n\n\n\n\npenguins |>\n  map_if(is.numeric, standardize)\n\n\n\n\n\npenguins |>\n  map_at(\n    c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"),\n    standardize\n    )"
  },
  {
    "objectID": "08-functional-programming.html#pa-8-the-12-days-of-christmas-starter-functions",
    "href": "08-functional-programming.html#pa-8-the-12-days-of-christmas-starter-functions",
    "title": "\n8  Functional Programming\n",
    "section": "PA 8: The 12 Days of Christmas Starter Functions",
    "text": "PA 8: The 12 Days of Christmas Starter Functions\nThe song “12 Days of Christmas”, written around 1780, tells the tale of many gifts a person receives in the days leading up to Christmas (link to lyrics). This week, you will be using your new R developer skills to write functions that automatically sing this very repetitive song.\nIn the practice activity, we will start by writing two helper functions which we will use in the lab to write a function to sing this entire song.\nVisit PA 8: The 12 Days of Christmas Starter Functions for instructions.\n\nYour Full.Phrase column is the answer to this week’s Practice Activity.\nCopy and paste your Full.Phrase column to show me the phrases you made!"
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html",
    "href": "09-statistical-modeling-and-simulation.html",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "",
    "text": "Reading: 7 minute(s) at 200 WPM\nVideos: 48 minutes"
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#ch9-objectives",
    "href": "09-statistical-modeling-and-simulation.html#ch9-objectives",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "Objectives",
    "text": "Objectives\nThis chapter is heavily from Dr. Theobold’s course-page material."
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#ch9-checkins",
    "href": "09-statistical-modeling-and-simulation.html#ch9-checkins",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere are two check-ins for this week:\n\nCheck-in 9.1: Linear Regression\nCheck-in 9.2: Distributions and Simulations"
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#linear-regression",
    "href": "09-statistical-modeling-and-simulation.html#linear-regression",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "\n9.1 Linear Regression",
    "text": "9.1 Linear Regression\nYou now have the skills to import, wrangle, and visualize data. All of these tools help us prepare our data for statistical modeling. In this chapter we will be learning how to implement linear regression models in R, but first let’s review simple linear regression. Linear regression models the linear relationship between two quantitative variables.\n\n\n\n\n\n\n\n\n\n\n\nReview of Simple Linear Regression and Conditions\nRecommended Reading – Modern Dive : Basic Regression\nHandy function shown in the reading! skim from the skimr package.\n\n9.1.1 Linear Regression in R\nTo demonstrate linear regression in R, we will be working with the penguins data set.\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins) |> \n  kable()\n\n\n\n\n species \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n\n\n Adelie \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    NA \n    NA \n    NA \n    NA \n    NA \n    2007 \n  \n\n Adelie \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007 \n  \n\n\n\n\nWhen conducting linear regression with tools in R, we often want to visualize the relationship between the two quantitative variables of interest with a scatterplot. We can then use either geom_smooth(method = \"lm\") (or equivalently stat_smooth(method = \"lm\") to add a line of best fit (“regression line”) based on the ordinary least squares (OLS) equation to our scatter plot. The regression line is shown in a default blue line with the standard error uncertainty displayed in a gray transparent band (use se = FALSE to hide the standard error uncertainty band). These visual aesthetics can be changed just as any other plot aesthetics.\n\npenguins |>\n  ggplot(aes(x = bill_depth_mm, \n             y = bill_length_mm\n             )\n         ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"Bill Depth (mm)\",\n       subtitle = \"Bill Length (mm)\",\n       title = \"Relationship between penguin bill length and depth\"\n       ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\n\nBe careful of “overplotting” and use geom_jitter() instead of geom_point() if your data set is dense. This is strictly a data visualization tool and will not alter the original values.\n\nIn simple linear regression, we can define the linear relationship with a mathematical equation given by:\n\\[y = a + b\\cdot x\\]\n\n\n\nRemember \\(y = m\\cdot x+b\\) from eighth grade?!\n\nwhere\n\n\n\\(y\\) are the values of the response variable,\n\n\\(x\\) are the values of the explanatory/predictor variable,\n\n\\(a\\) is the \\(y\\)-intercept (average value of \\(y\\) when \\(x = 0\\)), and\n\n\\(b\\) is the slope coefficient (for every 1 unit increase in \\(x\\), the average of \\(y\\) increases by b)\n\n\n\n\nRemember “rise over run”!\n\nIn statistics, we use slightly different notation to denote this relationship with the estimated linear regression equation:\n\\[\\hat y = b_0 + b_1\\cdot x.\\]\nNote that the “hat” symbol above our response variable indicates this is an “estimated” value (or our best guess).\nWe can “fit” the linear regression equation with the lm function in R. The formula argument is denoted as y ~ x where the left hand side (LHS) is our response variable and the right hand side (RHS) contains our explanatory/predictor variable(s). We indicate the data set with the data argument and therefore use the variable names (as opposed to vectors) when defining our formula. We name (my_model) and save our fitted model just as we would any other R object.\n\nmy_model <- lm(bill_length_mm ~ bill_depth_mm, \n               data = penguins\n               )\n\nNow that we have fit our linear regression, we might be wondering how we actually get the information out of our model. What are the y-intercept and slope coefficient estimates? What is my residual? How good was the fit? The code options below help us obtain this information.\n\n\nRaw Coefficients\nModel Summary\nTidy Model Summary\n\n\n\nThis is what is output when you just call the name of the linear model object you created (my_model). Notice, the output doesn’t give you much information and it looks kind of bad.\n\nmy_model\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = penguins)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\n\n\nThis is what is output when you use the summary() function on a linear model object. Notice, the output gives you a lot of information, some of which is really not that useful. And, the output is quite messy!\n\nsummary(my_model)\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8949  -3.9042  -0.3772   3.6800  15.5798 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    55.0674     2.5160  21.887  < 2e-16 ***\nbill_depth_mm  -0.6498     0.1457  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.314 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\n\n\nThe tidy() function from the {broom} package takes a linear model object and puts the “important” information into a tidy tibble output.\nAh! Just right!\n\nlibrary(broom)\ntidy(my_model) \n\n# A tibble: 2 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     55.1       2.52      21.9  6.91e-67\n2 bill_depth_mm   -0.650     0.146     -4.46 1.12e- 5\n\n\nIf you are sad that you no longer have the statistics about the model fit (e.g., R-squared, adjusted R-squared, \\(\\sigma\\)), you can use the glance() function from the broom package to grab those!\n\nbroom::glance(my_model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0552        0.0525  5.31      19.9 0.0000112     1 -1056. 2117. 2129.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\n\n\nCheck-in 9.1: Linear Regression\n1. True or False – If you switch the order of bill_length_mm and bill_depth_mm in the lm() formula nothing happens.\n2. What object type was returned by summary()?\n3. What object type was returned by tidy()?\n4. What is the equation for the estimated regression line for the relationship between bill length and depth (given above)?\n5. Penguin Mopsy has a bill that is 5mm deeper than Penguin Gidget. What will be the difference between their bill lengths?\n6. Run the following code, and explore the results:\n\nmy_model_2 <- lm(bill_length_mm ~ bill_depth_mm*species,\n                 data = penguins\n                 )\n\na. Make a plot illustrating my_model_2. Upload a picture of your plot! (Hint: Think about what needs to change in the aesthetic of the plot featured previously?)\nb. Which model of the two models explains the most variance in bill_length_mm?\nc. Do the three species of penguin have the same bill shape (i.e., the relationship between length and depth)?"
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#simulation",
    "href": "09-statistical-modeling-and-simulation.html#simulation",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "\n9.2 Simulation",
    "text": "9.2 Simulation\nIn statistics, we often want to simulate data (or create fake data) for a variety of purposes. For example, in your first statistics course, you may have flipped coins to “simulate” a 50-50 chance. In this section, we will learn how to simulate data from statistical distributions using R.\n\n\n\n\n\n\n\nRequired Reading – R Programming for Data Science : Simulation\n\n\n9.2.1 Setting a Random Number Seed\nFunctions like rnorm() rely on something called pseudo-randomness. Because computers can never be truly random, complicated processes are implemented to make “random” number generation be so unpredictable as to behave like true randomness.\nThis means that projects involving simulation are harder to make reproducible. For example, here are two identical lines of code that give different results!\n\nrnorm(1, mean = 0, sd = 1)\n\n[1] 0.6406349\n\n\n\nrnorm(1, mean = 0, sd = 1)\n\n[1] -0.5387425\n\n\nFortunately, pseudo-randomness depends on a seed, which is an arbitrary number where the randomizing process starts. Normally, R will choose the seed for you, from a pre-generated vector:\n\nhead(.Random.seed)\n\n[1]       10403           4  -895367753   393876062  -587533416 -1215751429\n\n\nHowever, you can also choose your own seed using the set.seed() function. This guarantees your results will be consistent across runs (and hopefully computers):\n\nset.seed(1234)\nrnorm(1, mean = 0, sd = 1)\n\n[1] -1.207066\n\n\n\nset.seed(1234)\nrnorm(1, mean = 0, sd = 1)\n\n[1] -1.207066\n\n\nOf course, it doesn’t mean the results will be the same in every subsequent run if you forget or reset the seed in between each line of code!\n\nset.seed(1234)\nrnorm(1, mean = 0, sd = 1)\n\n[1] -1.207066\n\n## Calling rnorm() again without a seed \"resets\" the seed! \nrnorm(1, mean = 0, sd = 1)\n\n[1] 0.2774292\n\n\nIt is very important to always set a seed at the beginning of a Quarto document that contains any random steps, so that your rendered results are consistent.\n\nNote, though, that this only guarantees your rendered results will be the same if the code has not changed.\nChanging up any part of the code will re-randomize everything that comes after it!\n\nWhen writing up a report which includes results from a random generation process, in order to ensure reproducibility in your document, use `r ` to include your output within your written description with inline code.\n\n\nReproducibility: inline code example\n\nmy_rand <- rnorm(1, mean = 0, sd = 1)\nmy_rand\n\n[1] 1.084441\n\n\nUsing r my_rand will display the result within my text:\nMy random number is 1.0844412.\nAlternatively, you could have put the rnorm code directly into the inline text r rnorm(1, mean = 0, sd = 1), but this can get messy if you have a result that requires a larger chunk of code.\n\n9.2.2 Plotting Density Distributions\nThe code below creates a tibble (read fancy data frame) of 100 heights randomly simulated (read drawn) from a normal distribution with a mean of 67 and standard deviation of 3.\n\nset.seed(93401)\nmy_samples <- tibble(height = rnorm(n    = 100, \n                                    mean = 67, \n                                    sd   = 3)\n                     )\nmy_samples |> \n  head()\n\n# A tibble: 6 × 1\n  height\n   <dbl>\n1   63.1\n2   66.7\n3   68.2\n4   63.4\n5   68.9\n6   71.4\n\n\nTo visualize the simulated heights, we can look at the density of the values. We plot the simulated values using geom_histogram() and define the local \\(y\\) aesthetic to plot calculate and plot the density of these values. We can then overlay the normal distribution curve (theoretical equation) with our specified mean and standard deviation using dnorm within stat_function()\n\nmy_samples |> \n  ggplot(aes(x = height)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 1.75, \n                 fill = \"grey\"\n                 ) +\n  stat_function(fun = ~ dnorm(.x, mean = 67, sd = 3), \n                col = \"cornflowerblue\", \n                lwd = 2\n                ) + \n  xlim(c(55, 80))\n\n\n\n\nCheck-in 9.2: Distributions and Simulations\n1. The r, p, q, and d functions: Try to predict what the following outputs will be WITHOUT running the code in R. Drawing pictures of the relevant distributions may help. Explain what each of the functions produces in the output.\nYes, it is very easy to “cheat” on this question. But this is for your practice, and I recommend you give it some thought.\n\n\n\n\npnorm(-4, mean = 2, sd = 4)\n\n\n\n\n\nqnorm(.975, mean = 2, sd = 4)\n\n\n\n\n\ndnorm(1.5, mean = 2, sd = 4)\n\n2. Why does rnorm(mean = 0, sd = 1) give an error?"
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#pa-9.1-mystery-animal",
    "href": "09-statistical-modeling-and-simulation.html#pa-9.1-mystery-animal",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "PA 9.1: Mystery Animal",
    "text": "PA 9.1: Mystery Animal\nYou will be fitting a linear regression model to a data set that contains the weights of a particular animal before and after a year of eating only roasted duck. Plotting the residuals will result in an image of a particular mystery animal.\nVisit PA 9.1 Mystery Animal for instructions."
  },
  {
    "objectID": "09-statistical-modeling-and-simulation.html#pa-9.2-instrument-con",
    "href": "09-statistical-modeling-and-simulation.html#pa-9.2-instrument-con",
    "title": "\n9  Regression & Simulating Distributions\n",
    "section": "PA 9.2: Instrument Con",
    "text": "PA 9.2: Instrument Con\nYou will be simulating data from statistical distributions to determine whether Professor Hill’s instruments are genuine or not.\nVisit PA 9.2 Instrument Con for instructions."
  },
  {
    "objectID": "10-predictive-checks.html",
    "href": "10-predictive-checks.html",
    "title": "\n10  Predictive Checks\n",
    "section": "",
    "text": "Reading: 11 minute(s) at 200 WPM\nVideos: NA"
  },
  {
    "objectID": "10-predictive-checks.html#ch10-objectives",
    "href": "10-predictive-checks.html#ch10-objectives",
    "title": "\n10  Predictive Checks\n",
    "section": "Objectives",
    "text": "Objectives\nThis chapter is heavily from Dr. Theobold’s course-page material.\n\nMake predictions from a linear model\nInclude variability into predictions\nUnderstand what is assumed about the data generating process in a linear regression\nAssess if the assumed linear model accurately describes the observed data"
  },
  {
    "objectID": "10-predictive-checks.html#ch10-checkins",
    "href": "10-predictive-checks.html#ch10-checkins",
    "title": "\n10  Predictive Checks\n",
    "section": "Check-ins",
    "text": "Check-ins\nThere is one check-in for this week:\n\nCheck-in 10.1: Linear Regression\n\n\nThere are 7 questions scattered throughout the reading this week."
  },
  {
    "objectID": "10-predictive-checks.html#model-checking",
    "href": "10-predictive-checks.html#model-checking",
    "title": "\n10  Predictive Checks\n",
    "section": "\n10.1 Model Checking",
    "text": "10.1 Model Checking\nIn explanatory modeling, it is incredibly important to check if you can make sound inferences from the model that was fit. Whether the model is a “simple” difference in means or a more “complex” multiple linear regression, the estimated coefficients obtained from the model should not be used for inference if the conditions of the model are violated. Typically, model checking for a linear regression is taught through a “residual analysis,” inspecting different visualizations of the residuals from the model. This week, however, we will learn a different way to assess our regression model.\n\n10.1.1 Predictive Checks, Bayesian Modeling, and Subjectivity\nThe idea of predictive checks for a statistical model are most often seen in the context of Bayesian modeling. In Bayesian modeling, rather than obtaining a single estimate for a parameter, you obtain a distribution of plausible values for that parameter. The distribution suggests which values of that parameter are more or less likely, given what was seen in the data and the prior knowledge incorporated into the model.\nThe prior knowledge about what values we expect for the parameter comes in the form of a prior distribution. Different statisticians may choose a different prior distributions, much like different statisticians might choose different variables to include in their regression. Although there are large similarities between the choice of a prior distribution and the choice of a statistical model, prior distributions receive a great deal of flack for how the interlace “subjectivity” into the statistical modeling process.\nDue in part to these criticisms regarding the choice of a prior distribution, it is common in Bayesian analyses to perform predictive checks to assess the fit of the model to the data at hand. While we are not fitting Bayesian models, I believe there is a great deal we can learn from the concept of using predictive checks for any statistical model. Any good analysis should include a check of the “adequacy of the fit of the model to the data and the plausibility of the model for the purposes for which the model will be used” (Gelman 2014).\n\n10.1.2 Predictive Checks in a Linear Regression\nUnder a simple linear regression model, we assume that the responses can be modeled as a function of the explanatory variable and some error:\n\\[y = \\beta_0 + \\beta_1 \\cdot x_1 + \\epsilon\\]\nIn a linear model we assume that the errors (\\(\\epsilon\\)) follow a Normal distribution with mean 0 and standard deviation (\\(\\sigma\\)) – \\(\\epsilon \\sim N(0,\\sigma)\\).\nThis implies that we can simulate data that we would expect to come from this model, by adding normally distributed errors to the values predicted from the linear regression. Moreover, when we randomly generate these errors, we better understand what the “normality,” “equal variance,” and “independence” conditions mean in the context of the linear regression model.\n\n10.1.3 Connection to Model Conditions\nA linear regression model has four conditions, which follow the LINE acronym:\nLinear relationship between \\(x\\) and \\(y\\) Independence of observations Normality of residuals Equal variance of residuals\nThe first condition relates to the relationship between the explanatory (\\(x\\)) and response (\\(y\\)) variable. As you might expect, using a line to model a non-linear relationship is a bad choice! Thus, we will only make predictions from a linear regression where there appears to be a linear relationship between the variables.\nThe process of adding normally distributed errors to the predictions from the linear regression assumes three things:\n\nthe errors follow a specific distribution, namely \\(N(0,\\sigma)\\)\n\nthe errors all have the same variance (\\(\\sigma\\))\nthe errors are not related to each other\n\nThe Normality and equal variance conditions are manifested in using the \\(N(0,\\sigma)\\) distribution to generate the errors. The independence condition is manifested in the assumption that we can draw random errors for each observation (prediction), because the errors are not related to each other.\nThis might feel like a bit of a stretch, especially if you haven’t seen linear regression in a bit, so let me break this down a bit more. When we assume that observations are independent, we are saying that we shouldn’t be able to know the \\(y\\) value for one observation just from knowing the \\(y\\) value of another observation. If I have repeated observations, like values for the same country across multiple years, then it’s pretty likely that I can guess the value of a variable in 1991 from knowing the same variable’s value in 1990. Since we are using linear regression to model the relationship between the explanatory variable(s) and the response, this assumption can be rephrased in terms of each observation’s residual. Specifically, knowing the residual for one observation shouldn’t give us perfect information about the residual for another observation. Because we are assuming that the observations / residuals are independent, we can randomly draw an error for each observation."
  },
  {
    "objectID": "10-predictive-checks.html#performing-a-predicitive-check-for-a-linear-regression",
    "href": "10-predictive-checks.html#performing-a-predicitive-check-for-a-linear-regression",
    "title": "\n10  Predictive Checks\n",
    "section": "\n10.2 Performing a Predicitive Check for a Linear Regression",
    "text": "10.2 Performing a Predicitive Check for a Linear Regression\nI will be walking through how to carry out the process of performing a predictive check for a simple linear regression model in the context of the penguins data set from the palmerpenguins package.\n\nlibrary(palmerpenguins)\ndata(penguins)\n\nWe will start by fitting a simple linear regression model, modeling a penguin’s body_mass_g as a function of their flipper_length_mm.\n\npenguins |> \n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"Flipper Length (mm)\", \n       y = \"\", \n       title = \"Relationship between Flipper Length (mm) and Body Mass (g)\", \n       subtitle = \"for Penguins on Palmer Archipelago\") +\n  theme_bw()\n\n\n\n\n\npenguin_lm <- lm(body_mass_g ~ flipper_length_mm, data  = penguins)\n\n\n10.2.1 Obtaining Predictions predict()\n\nThe next step is to obtain the predictions (.fitted values) from the regression model. There is a handy built-in function for extracting the predicted values from a lm object – the predict() function.\n\nRecall we also have the augment() function that will provide us with the .fitted values – this is analogous to the predictions from predict().\n\n\npenguin_predict <- predict(penguin_lm)\nhead(penguin_predict)\n\n       1        2        3        5        6        7 \n3212.256 3460.684 3907.854 3808.483 3659.426 3212.256 \n\n\nQ1: The predict() function\nWhat is the data structure of the predictions that are output by the predict() function?\n\nlist\nvector\nmatrix\ndataframe\n\n10.2.2 Extracting the Estimate of \\(\\sigma\\) sigma()\n\nThe residual standard error shown in the summary() output of a linear regression is the “best” estimate for the value of \\(\\sigma\\). We can extract this value using the built-in sigma() function and save it in an object for later use.\n\nsummary(penguin_lm)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1058.80  -259.27   -26.88   247.33  1288.69 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -5780.831    305.815  -18.90   <2e-16 ***\nflipper_length_mm    49.686      1.518   32.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.3 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.759, Adjusted R-squared:  0.7583 \nF-statistic:  1071 on 1 and 340 DF,  p-value: < 2.2e-16\n\npenguin_sigma <- sigma(penguin_lm)\npenguin_sigma\n\n[1] 394.2782\n\n\n\n10.2.3 Adding Errors / Noise to Predictions rnorm()\n\nAs stated before, under a linear regression model, we can simulate data that we would have expected to come from this model, by adding normally distributed errors to the values predicted from the linear regression.\nThus, it is useful for us to write a function that adds Normally distributed errors to a vector, x, given the value of sd input. I’ve written one such function here:\n\nnoise <- function(x, mean = 0, sd){\n  x + rnorm(length(x), \n            mean, \n            sd)\n}\n\nNext, we can use this function to generate a new “fake” dataset with observations we would have expected to come from our linear regression. Note I’m storing these predictions in a tibble(), so that I can easily plot them later!\n\nsim_response <- tibble(sim_body_mass_g = noise(penguin_predict, \n                                           sd = penguin_sigma)\n                   )\nhead(sim_response)\n\n# A tibble: 6 × 1\n  sim_body_mass_g\n            <dbl>\n1           2701.\n2           3422.\n3           4070.\n4           3339.\n5           3911.\n6           3792.\n\n\nQ2: Normal Errors\nWhat does a mean of 0 imply for the residuals?"
  },
  {
    "objectID": "10-predictive-checks.html#plotting-predictions",
    "href": "10-predictive-checks.html#plotting-predictions",
    "title": "\n10  Predictive Checks\n",
    "section": "\n10.3 Plotting Predictions",
    "text": "10.3 Plotting Predictions\nThe best way to assess if / where there are differences between the simulated data and the observed data is through visualizations. We can do this in two ways, (1) visualizing the distribution of the responses, and (2) visualizing the relationship between the responses and the explanatory variables.\n\n10.3.1 Distribution of Responses\n\nobs_p <- penguins |>\n  ggplot(aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 350) +\n  labs(x = \"Observed Body Mass (g)\",\n       y = \"\",\n       subtitle = \"Count\") +\n  xlim(2500, 6500) +\n  theme_bw()\n\nnew_p <- sim_response |>\n  ggplot(aes(x = sim_body_mass_g)) +\n  geom_histogram(binwidth = 350) +\n  labs(x = \"Simulated Body Mass (g)\",\n       y = \"\",\n       subtitle = \"Count\") +\n  xlim(2500, 6500) +\n  theme_bw()\n\nlibrary(patchwork)\nobs_p + new_p\n\n\n\n\n\n\n\nQ3: Simulated Data Distribution\nAre the simulated bill lengths similar or different from the observed bill lengths? Where do you see differences? Where do you see similarities? Would you conclude that both distributions could have come from the same data generating process?\n\n10.3.2 Adding in the Observed x’s\nIf we are interested in seeing if the observed relationships between flipper_length_mm and body_mass_g are radically different from the simulated data, we need to add these variables into our simulated data set.\nRemember, by default lm() throws out any observations with missing values (NA) for any of the variables included in the regression. Thus, we will have fewer predictions than observations in our data set. To make sure the predictions match up with their corresponding row, we need to filter the missing values out of the data set before we bind the columns together.\n\nsim_data <- penguins |> \n  filter(!is.na(body_mass_g), \n         !is.na(flipper_length_mm)\n         ) |> \n  select(body_mass_g, flipper_length_mm) |> \n  bind_cols(sim_response)\n\nhead(sim_data)\n\n# A tibble: 6 × 3\n  body_mass_g flipper_length_mm sim_body_mass_g\n        <int>             <int>           <dbl>\n1        3750               181           2701.\n2        3800               186           3422.\n3        3250               195           4070.\n4        3450               193           3339.\n5        3650               190           3911.\n6        3625               181           3792.\n\n\n\n10.3.3 Scatterplot of Relationships\nA scatterplot of the relationships modeled by the linear regression can give a more detailed idea for where the simulated data differ from the observed data. In the scatterplots below, we see that the data simulated from the regression more closely follows a linear relationship, but there are not dramatic differences between the two scatterplots.\n\nobs_reg_p <- penguins |>\n  ggplot(aes(y = body_mass_g,\n             x = flipper_length_mm)) +\n  geom_point() +\n  labs(title = \"Observed Body Mass\",\n       x = \"Flipper Length (mm)\",\n       y = \"\",\n       subtitle = \"Body Mass (g)\") +\n  theme_bw()\n\nsim_reg_p <-sim_data |>\n  ggplot(aes(y = sim_body_mass_g,\n             x = flipper_length_mm)\n         ) +\n  geom_point() +\n   labs(title = \"Simulated Body Mass \\nbased on Regression Model\",\n       x = \"Flipper Length (mm)\",\n       y = \"\",\n       subtitle = \"Body Mass (g)\") +\n  theme_bw()\n\nobs_reg_p + sim_reg_p"
  },
  {
    "objectID": "10-predictive-checks.html#assessing-predictions",
    "href": "10-predictive-checks.html#assessing-predictions",
    "title": "\n10  Predictive Checks\n",
    "section": "\n10.4 Assessing Predictions",
    "text": "10.4 Assessing Predictions\nWe would expect that if the regression model is a good model for penguin body mass, then the simulated data should look similar to what was observed. We can compare the simulated data with the observed data using a scatterplot.\nIf the simulated data were identical to the observed data, they would all fall on the \\(y = x\\) line (dashed blue). Values above the \\(y = x\\) line correspond to simulated body masses larger than the observed body masses, and values below the line correspond to simulated body masses smaller than the observed body masses. Overall, it appears that there are about as many over estimates as underestimates. It appears there is a “moderate” relationship between the observed values and simulated values, as the points are fairly close to the line, but not extremely close.\n\nsim_data |> \n  ggplot(aes(x = sim_body_mass_g, \n             y = body_mass_g)\n         ) + \n  geom_point() + \n   labs(x = \"Simulated Body Mass (g)\", \n        y = \"\",\n        subtitle = \"Observed Body Mass (g)\" ) + \n  geom_abline(slope = 1,\n              intercept = 0, \n              color = \"steelblue\",\n              linetype = \"dashed\",\n              lwd = 1.5) +\n  theme_bw()\n\n\n\n\nWe can use a statistic to summarize how “close” the simulated values and the observed values are. We can use any statistic that captures the residuals from this regression. I like \\(R^2,\\) since it has an easy reference value for what I would expect if my model did a good job (\\(R^2 \\approx 1\\)).\nI like the glance() function from the broom package, since it produces a nice table output of the summary measures from a linear regression.\n\nlm(body_mass_g ~ sim_body_mass_g, \n   data = sim_data\n   ) |> \n  glance() \n\n\n\n\n\n\n r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n\n 0.553 \n    0.552 \n    536.778 \n    421.14 \n    0 \n    1 \n    -2633.944 \n    5273.888 \n    5285.392 \n    97964349 \n    340 \n    342 \n  \n\n\n\nI can then use the column names of the table to select() the r.squared column\n\nThis would be a great place to use inline code to report the $R^2 value!\n\n\nsim_r2 <- lm(body_mass_g ~ sim_body_mass_g, \n             data = sim_data\n             ) |> \n  glance() |> \n  select(r.squared) |> \n  pull()\nsim_r2\n\n[1] 0.5533018\n\n\nQ4: Interpreting \\(R^2\\)\n\nWhat does the \\(R^2\\) value of 0.553 for the above regression mean? How would you interpret it?"
  },
  {
    "objectID": "10-predictive-checks.html#iterating",
    "href": "10-predictive-checks.html#iterating",
    "title": "\n10  Predictive Checks\n",
    "section": "\n10.5 Iterating!",
    "text": "10.5 Iterating!\nWe are interested in seeing how our model performs for more than one simulated data set. Thus, we need to iterate through the process outlined above. Specifically, at each step we will need to:\n\nSimulate new data\nRegress the new data on the observed data\nSave the \\(R^2\\) from the regression\n\n\n10.5.1 Lots of Simulated Observations\nLuckily, we have already written the noise() function that helps us simulate new observations that could have occurred under our model. Since we are not changing any input to noise(), we can choose between passing an “arbitrary” input to map() (e.g., 1:100). This essentially rerun the same process multiple times. Since we want for every simulated data set to be bound together as columns, we can use the map_dfc() function!\n\nnsims <- 100\nsims <- map_dfc(.x = 1:nsims,\n                .f = ~ tibble(sim = noise(penguin_predict, \n                                          sd = penguin_sigma)\n                              )\n                )\n\nhead(sims)\n\n# A tibble: 6 × 100\n  sim...1 sim...2 sim...3 sim...4 sim...5 sim...6 sim...7 sim...8 sim...9\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   3330.   2653.   3087.   3150.   3129.   3859.   3454.   3088.   3262.\n2   3446.   3754.   3335.   2994.   2989.   3979.   3456.   3929.   3401.\n3   3904.   4197.   3450.   3582.   4103.   4317.   3666.   4069.   3626.\n4   4445.   3608.   3342.   3672.   4412.   3510.   3543.   4393.   4261.\n5   3356.   3924.   3464.   3581.   3304.   4460.   3600.   3877.   3568.\n6   3100.   3072.   3534.   3054.   3006.   3769.   2769.   2546.   3224.\n# ℹ 91 more variables: sim...10 <dbl>, sim...11 <dbl>, sim...12 <dbl>,\n#   sim...13 <dbl>, sim...14 <dbl>, sim...15 <dbl>, sim...16 <dbl>,\n#   sim...17 <dbl>, sim...18 <dbl>, sim...19 <dbl>, sim...20 <dbl>,\n#   sim...21 <dbl>, sim...22 <dbl>, sim...23 <dbl>, sim...24 <dbl>,\n#   sim...25 <dbl>, sim...26 <dbl>, sim...27 <dbl>, sim...28 <dbl>,\n#   sim...29 <dbl>, sim...30 <dbl>, sim...31 <dbl>, sim...32 <dbl>,\n#   sim...33 <dbl>, sim...34 <dbl>, sim...35 <dbl>, sim...36 <dbl>, …\n\n\nSince all of the columns have the same name, dplyr automatically adds ... and a number after each column. So, our column names look like sim...1, sim...2, etc. It would be nice to replace the ...s with a _, which our friend str_replace_all() can help us do! Remember, the . is a special character and needs to be escaped!\n\ncolnames(sims) <- colnames(sims) |> \n  str_replace(pattern = \"\\\\.\\\\.\\\\.\",\n                  replace = \"_\")\n\nFinally, we can add the observed response (body_mass_g) into our simulated data set, for ease of modeling and visualizing.\n\nsims <- penguins |> \n  filter(!is.na(body_mass_g), \n         !is.na(flipper_length_mm)) |> \n  select(body_mass_g) |> \n  bind_cols(sims)\n\nhead(sims)\n\n# A tibble: 6 × 101\n  body_mass_g sim_1 sim_2 sim_3 sim_4 sim_5 sim_6 sim_7 sim_8 sim_9 sim_10\n        <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1        3750 3330. 2653. 3087. 3150. 3129. 3859. 3454. 3088. 3262.  3379.\n2        3800 3446. 3754. 3335. 2994. 2989. 3979. 3456. 3929. 3401.  3845.\n3        3250 3904. 4197. 3450. 3582. 4103. 4317. 3666. 4069. 3626.  3981.\n4        3450 4445. 3608. 3342. 3672. 4412. 3510. 3543. 4393. 4261.  4222.\n5        3650 3356. 3924. 3464. 3581. 3304. 4460. 3600. 3877. 3568.  4258.\n6        3625 3100. 3072. 3534. 3054. 3006. 3769. 2769. 2546. 3224.  2885.\n# ℹ 90 more variables: sim_11 <dbl>, sim_12 <dbl>, sim_13 <dbl>, sim_14 <dbl>,\n#   sim_15 <dbl>, sim_16 <dbl>, sim_17 <dbl>, sim_18 <dbl>, sim_19 <dbl>,\n#   sim_20 <dbl>, sim_21 <dbl>, sim_22 <dbl>, sim_23 <dbl>, sim_24 <dbl>,\n#   sim_25 <dbl>, sim_26 <dbl>, sim_27 <dbl>, sim_28 <dbl>, sim_29 <dbl>,\n#   sim_30 <dbl>, sim_31 <dbl>, sim_32 <dbl>, sim_33 <dbl>, sim_34 <dbl>,\n#   sim_35 <dbl>, sim_36 <dbl>, sim_37 <dbl>, sim_38 <dbl>, sim_39 <dbl>,\n#   sim_40 <dbl>, sim_41 <dbl>, sim_42 <dbl>, sim_43 <dbl>, sim_44 <dbl>, …\n\n\n\n10.5.2 Lots of Regressions & \\(R^2\\) Values\nNow, we want to regress each of these simulated body_mass_gs on the original, observed body_mass_g. Again, we will need to iterate through this process.\nBefore, I fit a linear regression between body_mass_g and the simulated data. Now, I have 100 different datasets I need to regress on body_mass_g. I can use map() to define a function that will be applied to each column of sims. This function regresses body_mass_g on each column (.x) from the sims dataset.\n\nsims |> \n  map(~ lm(body_mass_g ~ .x, data = sims))\n\nQ5: Mapping Linear Regression\nWhat is the data structure of the values output after the map(~lm(body_mass_g ~ .x, data = sims)) step?\n\nlist\nvector\nmatrix\ndataframe\nNext, I map() the glance() function onto each of these 100 regressions.\n\nsims |> \n  map(~ lm(body_mass_g ~ .x, data = sims)) |> \n  map(glance)\n\nQ6: Mapping Linear Regression Summaries\nWhat is the data structure of the values output after the map(glance) step?\n\nlist\nvector\nmatrix\ndataframe\nFinally, I use map_dbl() to extract the r.squared from each of the model fit summaries.\n\nsims |> \n  map(~ lm(body_mass_g ~ .x, data = sims)) |> \n  map(glance) |> \n  map_dbl(~ .x$r.squared)\n\nQ7: Mapping Regression Summaries\nWhat is the data structure of the values output after the map_dbl(~.$r.squared) step?\n\nlist\nvector\nmatrix\ndataframe\n\n10.5.3 Inspecting the \\(R^2\\) Values\n\nsim_r_sq <- sims |> \n  map(~ lm(body_mass_g ~ .x, data = sims)) |> \n  map(glance) |> \n  map_dbl(~ .x$r.squared)\n\nIf I look at the first six \\(R^2\\) values, I see that the first value corresponds to the the following regression: lm(body_mass_g ~ body_mass_g)\nAll of the values thereafter are the \\(R^2\\) values from the simulated data.\n\nhead(sim_r_sq)\n\nbody_mass_g       sim_1       sim_2       sim_3       sim_4       sim_5 \n  1.0000000   0.5896863   0.5753450   0.5819932   0.5725102   0.5705712 \n\n\nI am interested in looking at the distribution of \\(R^2\\) values from the simulated data, so I’m going to remove this unwanted entry of the sim_r_sq vector.\n\nsim_r_sq <- sim_r_sq[names(sim_r_sq) != \"body_mass_g\"]\n\n\n10.5.4 Plotting the Simulated \\(R^2\\) Values versus the Observed \\(R^2\\)\n\nThe final stage is to plot the statistics from the simulated data. The distribution of these \\(R^2\\) values will tell if our assumed model does a good job of producing data similar to what was observed. If the model produces data similar to what was observed, we would expect \\(R^2\\) values near 1.\n\ntibble(sims = sim_r_sq) |> \n  ggplot(aes(x = sims)) + \n  geom_histogram(binwidth = 0.025) +\n  labs(x = expression(\"Simulated\"~ R^2),\n       y = \"\",\n       subtitle = \"Number of Simulated Models\") +\n  theme_bw()\n\n\n\n\nIn this plot, we see that the simulated datasets have \\(R^2\\) vales between 0.5 and 0.65. This indicates the data simulated under this statistical model are moderately similar to what was observed. On average, our simulated data account for about 57.7% of the variability in the observed penguin body mass."
  },
  {
    "objectID": "10-predictive-checks.html#references",
    "href": "10-predictive-checks.html#references",
    "title": "\n10  Predictive Checks\n",
    "section": "References",
    "text": "References\n\n\n\n\nGelman, Andrew. 2014. “Model Checking.” In Bayesian Data Analysis. CRC Press."
  }
]